---
title: Interactions II 
format: revealjs
slide-number: true
editor: visual
execute:
  echo: true
html:
  code-fold: true
  code-summary: Show the code
  scrollable: true
---

## Mixing categorical and continuous

```{r, echo = FALSE, warning=FALSE, message=FALSE, include=FALSE}
library(sjPlot)
library(tidyverse)
```

Consider the case where D is a variable representing two groups. In a univariate regression, how do we interpret the coefficient for D?

$$\hat{Y} = b_{0} + b_{1}D$$

## Interpreting slopes

Extending this to the multivariate case, where X is continuous and D is a dummy code representing two groups.

$$\hat{Y} = b_{0} + b_{1}D + b_2X$$

How do we interpret $b_1?$

## Visualizing

```{r, echo = F, message=F, warning = F}
library(tidyverse)
set.seed(022520)
D = rep(c(0,1), each = 10)
X = rnorm(20) + D
Y = 2*D + X + rnorm(20)

df = data.frame(X,Y,D)

means = df %>%
  group_by(D) %>%
  summarize(M = mean(Y))

mod1 = lm(Y ~ D, data = df)
predict.1 = data.frame(X = rep(mean(X),2), D = c(0,1))
predict.1$Y = predict(mod1, newdata = predict.1) 
predict.1 = cbind(predict.1[1,], predict.1[2,])
names(predict.1) = c("x1", "d1", "y1", "x2", "d2", "y2")

```

```{r}
#| code-fold: true

set.seed(022520)
D = rep(c(0,1), each = 10)
X = rnorm(20) + D
Y = 2*D + X + rnorm(20)

df = data.frame(X,Y,D)

means = df %>%
  group_by(D) %>%
  summarize(M = mean(Y))
mod = lm(Y ~ X + D, data = df)
df$pmod = predict(mod)

predict.2 = data.frame(X = rep(mean(X)+.1,2), D = c(0,1))
predict.2$Y = predict(mod, newdata = predict.2) 
predict.2 = cbind(predict.2[1,], predict.2[2,])
names(predict.2) = c("x1", "d1", "y1", "x2", "d2", "y2")

ggplot(df, aes(X,Y, color = as.factor(D))) +
  geom_point(size = 3) +
  geom_smooth(aes(y = pmod), method = "lm", se = F)+
  labs(color = "D") +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = predict.2, 
               inherit.aes = F, size = 1.5)+
  cowplot::theme_cowplot()
```

## Visualizing

```{r, echo = F, warning=FALSE, message=FALSE}
#| code-fold: true
#| 
mod = lm(Y ~ X + D, data = df)
df$pmod = predict(mod)

predict.2 = data.frame(X = rep(mean(X)+.1,2), D = c(0,1))
predict.2$Y = predict(mod, newdata = predict.2) 
predict.2 = cbind(predict.2[1,], predict.2[2,])
names(predict.2) = c("x1", "d1", "y1", "x2", "d2", "y2")

ggplot(df, aes(X,Y, color = as.factor(D))) +
  geom_point(size = 3) +
  geom_smooth(aes(y = pmod), method = "lm", se = F)+
  labs(color = "D") +
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = predict.1, 
               inherit.aes = F, size = 1.5)+
  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = predict.2, 
               inherit.aes = F, size = 1.5)+
  cowplot::theme_cowplot()
```

## Interactions

Now extend this example to include joint effects, not just additive effects:

$$\hat{Y} = b_{0} + b_{1}D + b_2X + b_3DX$$

How do we interpret $b_1?$, $b_2$, $b_3?$?

## Visualizing

```{r}
#| code-fold: true
ggplot(df, aes(X,Y, color = as.factor(D))) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = F)+
  labs(color = "D") +
  cowplot::theme_cowplot()
```

------------------------------------------------------------------------

Where should we draw the segment to compare means?

???

Where you draw the segment changes the difference in means. That's why $b_1$ can only be interpreted as the difference in means when X = 0.

## Example

```{r, echo = F}
#| code-fold: true
set.seed(022520)
x = rnorm(n = 50)
econ_income = 120 + 20*x + rnorm(n = 50, sd = 15)
psych_income = 80 + 30*x+ rnorm(n = 50, sd = 15)
english_income = 60 + 20*x+ rnorm(n = 50, sd = 15)
econ_gpa = .3*x + 3.0 + rnorm(n = 50, sd = .05)
psych_gpa = .3*x + 3.5 + rnorm(n = 50, sd = .05)
english_gpa = .3*x + 3.7 + rnorm(n = 50, sd = .05)

inc_data = data.frame(major = rep(c("Econ", "Psych", "English"), each = 50),
                      gpa = c(econ_gpa, psych_gpa, english_gpa),
                      income = c(econ_income, psych_income, english_income))
inc_data$income[inc_data$income < 21] = rnorm(n = length(which(inc_data$income < 21)),
                                              m = 55, sd = 10)
```

Wash U is interested in understanding how undergraduates' academic performance and choice of major impacts their career success. They contact 150 alumni between the ages of 25 and 35 and collect their current salary (in thousands of dollars), their primary undergraduate major, and their GPA upon graduating.

```{r}
#| code-fold: true
library(psych)
table(inc_data$major)
describe(inc_data[,c("gpa", "income")], fast = T)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
career.mod = lm(income ~ gpa*major, data = inc_data)
summary(career.mod)
```

## Pop Quiz

For the model just defined in the previous slide...

-   Write out the regression equation
-   Interpret each term
-   Where is Econ?

## Model summary: centering predictors

```{r}
inc_data$gpa_c = inc_data$gpa - mean(inc_data$gpa)
career.mod_c = lm(income ~ gpa_c*major, data = inc_data)
summary(career.mod_c)
```

## Plotting results

```{r}
#| code-fold: true
#| 
library(ggeffects)
predictedvals = ggpredict(model = career.mod_c, terms = c("gpa_c", "major"))

ggplot(data = predictedvals, aes(x = x, y = predicted, group = group)) +
  geom_smooth(aes(ymin = conf.low,
                  ymax = conf.high,
                  color = group,
                  fill = group),
              stat = "identity",
              alpha = .2) +
  labs(x = "GPA",
       y = "Income (in thousands of dollars)",
       title = "I am in the wrong profession",
       color = "Undergraduate Major",
       fill = "Undergraduate Major",
       group = "Undergraduate Major")

```

## Uncentered Plot

```{r}
#| code-fold: true
#| 
library(ggeffects)
predictedvals = ggpredict(model = career.mod, terms = c("gpa", "major"))

ggplot(data = predictedvals, aes(x = x, y = predicted, group = group)) +
  geom_smooth(aes(ymin = conf.low,
                  ymax = conf.high,
                  color = group,
                  fill = group),
              stat = "identity",
              alpha = .2) +
  labs(x = "GPA",
       y = "Income (in thousands of dollars)",
       title = "I am in the wrong profession",
       color = "Undergraduate Major",
       fill = "Undergraduate Major",
       group = "Undergraduate Major")

```

## Factorial ANOVA

The interaction of two or more categorical variables in a general linear model is formally known as **Factorial ANOVA**.

A factorial design is used when there is an interest in how two or more variables (or factors) affect the outcome.

-   Rather than conduct separate one-way ANOVAs for each factor, they are all included in one analysis.

-   The unique and important advantage to a factorial ANOVA over separate one-way ANOVAs is the ability to examine interactions.

------------------------------------------------------------------------

```{r}
#| code-fold: true
set.seed(23)
SD=120

DV_Slow_N <- rnorm(20,mean=600,sd=SD) # draw 20 from normal distribution
DV_Slow_C <- rnorm(20,mean=590,sd=SD) # draw 20 from normal distribution
DV_Slow_U <- rnorm(20,mean=585,sd=SD) # draw 20 from normal distribution

DV_Med_N <- rnorm(20,mean=550,sd=SD) # draw 20 from normal
DV_Med_C <- rnorm(20,mean=450,sd=SD) # draw 20 from normal
DV_Med_U <- rnorm(20,mean=300,sd=SD) # draw 20 from normal

DV_Fast_N <- rnorm(20,mean=310,sd=SD) # draw 20 from normal
DV_Fast_C <- rnorm(20,mean=305,sd=SD) # draw 20 from normal
DV_Fast_U <- rnorm(20,mean=290,sd=SD) # draw 20 from normal

# put DVs together in a data frame; specify Speed and Noise Values
Data = data.frame(Time = c(DV_Slow_N,
                           DV_Slow_C,
                           DV_Slow_U,
                           DV_Med_N,
                           DV_Med_C,
                           DV_Med_U,
                           DV_Fast_N,
                           DV_Fast_C,
                           DV_Fast_U),
                  Speed = rep(c("Slow", "Medium", "Fast"), each = 60),
                  Noise = rep(rep(c("None", "Controllable", "Uncontrollable"), 
                                  each = 20), 3)) #repeat each label 20 times, then repeat that whole sequence 3 times
Data$Speed = factor(Data$Speed, levels = c("Slow", "Medium", "Fast")) # set order of levels as I want them presented
Data$Noise = factor(Data$Noise, levels = c("None", "Controllable", "Uncontrollable")) # set order of levels as I want them presented
```

-   The example data are from a simulated study in which 180 participants performed an eye-hand coordination task in which they were required to keep a mouse pointer on a red dot that moved in a circular motion.

![](dot.jpg)

-   The outcome was the time of the 10th failure. The experiment used a completely crossed, 3 x 3 factorial design. One factor was dot speed: .5, 1, or 1.5 revolutions per second. The second factor was noise condition. Some participants performed the task without any noise; others were subjected to periodic and unpredictable 3-second bursts of 85 dB white noise played over earphones. Of those subjected to noise, half could do nothing to stop the noise (uncontrollable noise); half believed they could stop the noise by pressing a button (controllable noise).

## Terminology

In a **completely crossed** factorial design, each level of one factor occurs in combination with each level of the other factor.

If equal numbers of participants occur in each combination, the design is **balanced**. This has some distinct advantages (described later).

|                      | Slow | Medium | Fast |
|:---------------------|:----:|:------:|:----:|
| No Noise             |  X   |   X    |  X   |
| Controllable Noise   |  X   |   X    |  X   |
| Uncontrollable Noise |  X   |   X    |  X   |

## Terminology

We describe the factorial ANOVA design by the number of **levels** of each **factor.**

-   Factor: a variable that is being manipulated or in which there are two or more groups

-   Level: the different groups within a factor

In this case, we have a 3 x 3 ANOVA ("three by three"), because our first factor (speed) has three levels (slow, medium, and fast) and our second factor (noise) also has three levels (none, controllable, and uncontrollable)

## Questions

```{r}
#| code-fold: true
mean.summary = Data %>%
  group_by(Noise, Speed) %>%
  summarize(Time = mean(Time)) %>%
  spread("Speed", "Time")
mean.summary$Noise = as.character(mean.summary$Noise)
mean.summary$Marginal = rowMeans(mean.summary[2:4])
mean.summary[4,1] = "Marginal"
mean.summary[4,2] = colMeans(mean.summary[2:5], na.rm=T)[1]
mean.summary[4,3] = colMeans(mean.summary[2:5], na.rm=T)[2]
mean.summary[4,4] = colMeans(mean.summary[2:5], na.rm=T)[3]
mean.summary[4,5] = colMeans(mean.summary[2:5], na.rm=T)[4]
library(knitr)
library(kableExtra)
kable(mean.summary, digits = 2) %>% kable_styling() %>% group_rows(start_row = 1, end_row = 3)
```

------------------------------------------------------------------------

```{r}
#| code-fold: true
library(ggpubr)
ggbarplot(data = Data, x = "Speed", y = "Time", add = c("mean_ci"), fill = "#562457", xlab = "Speed Condition", ylab = "Mean Seconds (95% CI)", title = "Failure time as a function of\nspeed condition") 
```

Looks like the mean differences are substantial. The ANOVA will be able to tell us if the means are significantly different and the magnitude of those differences in terms of variance accounted for.

## Marginal means

```{r, echo = F, results = 'asis', message = F, warning = F}
kable(mean.summary, digits = 2) %>% 
  kable_styling() %>% 
  group_rows(start_row = 1, end_row = 3) %>% 
  column_spec(5, bold = T, color = "white", background = "#562457")
```

Regardless of dot speed, does noise condition affect performance? Performance declines in the presence of noise, especially if the noise is uncontrollable.

Here, too adding information about variability allows us a sense of whether these are significant and meaningful differences...

------------------------------------------------------------------------

```{r}
#| code-fold: true
ggbarplot(data = Data, x = "Noise", y = "Time", add = c("mean_ci"), fill = "#562457", xlab = "Noise Condition", ylab = "Mean Seconds (95% CI)", title = "Failure time as a function of\nnoise condition") + cowplot::theme_cowplot(font_size = 20)
```

The mean differences are not as apparent for this factor

## Marginal means

```{r, echo = F, results = 'asis', message = F, warning = F}
kable(mean.summary, digits = 2) %>% 
  kable_styling() %>% 
  group_rows(start_row = 1, end_row = 3) 
```

The **marginal mean differences** correspond to main effects. They tell us what impact a particular factor has, ignoring the impact of the other factor.

The remaining effect in a factorial design, and it primary advantage over separate one-way ANOVAs, is the ability to examine **conditional mean differences**.

## The Linear Model Way

```{r}
summary(lm(Time ~ Noise*Speed, data = Data))
```

## Mean differences

```{r, echo = F, results = 'asis', message = F, warning = F}
kable(mean.summary, digits = 2) %>% 
  kable_styling() %>% 
  group_rows(start_row = 1, end_row = 3) %>%
  column_spec(2, background = "#EECACA") %>%
  column_spec(3, background = "#B2D4EB") %>%
  column_spec(4, background = "#FFFFC5") %>%
  column_spec(5, background = "grey", color = "white") %>%
  row_spec(4, background = "white")
```

Are the marginal mean differences for noise condition a good representation of what is happening within each of the dot speed conditions?

If not, then we would need to say that the noise condition effect depends upon (is conditional on) dot speed. We would have an interaction between noise condition and dot speed condition.

------------------------------------------------------------------------

```{r}
#| code-fold: true
library(sjPlot)
library(tidyverse)
model = lm(Time ~Speed*Noise, data = Data)
model.data = plot_model(model, type = "int")$data
model.data %>% ggplot(aes(x = x, y = predicted, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(width = .8), color = "black", width = .7) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .3, position = position_dodge(width = .8)) +
  scale_x_continuous("Speed Condition", breaks = c(1, 2,3), labels = c("Slow", "Medium", "Fast")) +
  labs(y = "Mean Seconds (95% CI)", fill = "Noise Condition", title = "Failure time as a function of\nnoise condition and speed condition") +
cowplot::theme_cowplot(font_size = 20)
```

The noise condition means are most distinctly different in the medium speed condition. The noise condition means are clearly not different in the fast speed condition.

## Interpretation of significance tests

```{r}
#| code-fold: true
fit = lm(Time ~ Speed*Noise, data = Data)
anova(fit)
```

All three null hypotheses are rejected. This only tells us that systemic differences among the means are present; follow-up comparisons are necessary to determine the nature of the differences.

------------------------------------------------------------------------

```{r}
summary(fit)
```

------------------------------------------------------------------------

```{r}
library(emmeans)
em <- emmeans(fit, ~ Speed : Noise )
em 
```

## Cell contrasts

```{r}
pairs(em)
```

## Marginal contrasts

```{r}
emmeans(fit, pairwise ~ Speed)
```


## effect sizes

```{r}
em.ef <- emmeans(fit, pairwise ~ Speed)
eff_size(em.ef, sigma = sigma(fit), edf = 171)
```

