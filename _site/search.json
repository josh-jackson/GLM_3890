[
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "Untitled",
    "section": "",
    "text": "General(ized) Linear Model\nA workhorse that is responsible for &gt;99% of statistical tests in psychology, as well as the building block of many machine learning models"
  },
  {
    "objectID": "1-intro.html#glm",
    "href": "1-intro.html#glm",
    "title": "What are models?",
    "section": "GLM",
    "text": "GLM\n\nGeneral(ized) Linear Model\nA workhorse that is responsible for &gt;99% of statistical tests in psychology, as well as the building block of many machine learning models"
  },
  {
    "objectID": "1-intro.html#what-do-we-mean-by-generalized",
    "href": "1-intro.html#what-do-we-mean-by-generalized",
    "title": "What are models?",
    "section": "What do we mean by General(ized)?",
    "text": "What do we mean by General(ized)?\n\nIt is general in that it refers to a broad set of similar models that can applied to almost any context"
  },
  {
    "objectID": "1-intro.html#what-do-we-mean-by-linear",
    "href": "1-intro.html#what-do-we-mean-by-linear",
    "title": "What are models?",
    "section": "What do we mean by linear?",
    "text": "What do we mean by linear?\n\nWe try to understand our dependent variable (DV) via a linear combination predictor variables.\nA linear combination a way of combining things (variables) using scalar multiplication and addition"
  },
  {
    "objectID": "1-intro.html#what-is-a-model",
    "href": "1-intro.html#what-is-a-model",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?"
  },
  {
    "objectID": "1-intro.html#what-is-a-model-1",
    "href": "1-intro.html#what-is-a-model-1",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?\n\na representation of the world\na statistical model uses math to make predictions about the world"
  },
  {
    "objectID": "1-intro.html#middle-school-math",
    "href": "1-intro.html#middle-school-math",
    "title": "What are models?",
    "section": "Middle School Math",
    "text": "Middle School Math\n\\[ y = mx + b \\] - what is \\(y\\)?\n\nwhat is \\(m\\)?\nwhat is \\(x\\)?\nwhat is \\(b\\)?"
  },
  {
    "objectID": "1-intro.html#lets-rewrite-this",
    "href": "1-intro.html#lets-rewrite-this",
    "title": "What are models?",
    "section": "Let’s rewrite this",
    "text": "Let’s rewrite this\n\\[y = b_0 + b_{1}X\\]\n\nwhat is \\(y\\)?\nwhat is \\(b_0\\)?\nwhat is \\(b_1\\)?\nwhat is \\(X\\)?"
  },
  {
    "objectID": "1-intro.html#are-models-always-right",
    "href": "1-intro.html#are-models-always-right",
    "title": "What are models?",
    "section": "Are models always right?",
    "text": "Are models always right?"
  },
  {
    "objectID": "1-intro.html#models-are-flawed",
    "href": "1-intro.html#models-are-flawed",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?"
  },
  {
    "objectID": "1-intro.html#models-are-flawed-1",
    "href": "1-intro.html#models-are-flawed-1",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "1-intro.html#models",
    "href": "1-intro.html#models",
    "title": "What are models?",
    "section": "Models",
    "text": "Models\n\nWhat are the goals of modeling?\nWhat do you need in order to develop a model?"
  },
  {
    "objectID": "1-intro.html#how-do-we-know-if-a-model-is-good-what-makes-it-good",
    "href": "1-intro.html#how-do-we-know-if-a-model-is-good-what-makes-it-good",
    "title": "What are models?",
    "section": "How do we know if a model is good? What makes it good?",
    "text": "How do we know if a model is good? What makes it good?"
  },
  {
    "objectID": "1-intro.html#how-will-we-use-models",
    "href": "1-intro.html#how-will-we-use-models",
    "title": "What are models?",
    "section": "How will we use models?",
    "text": "How will we use models?\n\nThis semester, we will mainly focus on classic statistical tests\nEvery single one of these is a model\nWe will also focus on developing your intuition\nWhen you face new models, come back to these basics"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "What are models?\nGLM basics\nGLM basics - 2\nGLM basics - 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Psychological Statistics",
    "section": "",
    "text": "1/31: Homework #2 is now posted. Please turn it in to canvas by 2/5 at 9:59am\n1/24: Homework #1 is now posted. Please turn it in to canvas by 1/29 at 9:59am\n1/17: Welcome to the first day of class! If you want to follow along with examples, head over to github for all of the code and data. https://github.com/josh-jackson/GLM_3890"
  },
  {
    "objectID": "1-intro.html#how-do-we-know-if-a-model-is-good",
    "href": "1-intro.html#how-do-we-know-if-a-model-is-good",
    "title": "What are models?",
    "section": "How do we know if a model is good?",
    "text": "How do we know if a model is good?\n\nWhat makes it good?"
  },
  {
    "objectID": "2-Basics.html#thinking-in-terms-of-models",
    "href": "2-Basics.html#thinking-in-terms-of-models",
    "title": "GLM basics",
    "section": "Thinking in terms of models",
    "text": "Thinking in terms of models\n\nOur DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (here forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "2-Basics.html#how-can-we-visualize-data-to-make-sense-of-it",
    "href": "2-Basics.html#how-can-we-visualize-data-to-make-sense-of-it",
    "title": "GLM basics",
    "section": "How can we visualize data to make sense of it?",
    "text": "How can we visualize data to make sense of it?\n\n\nCode\nlibrary(broom)\nset.seed(123)\nx.1 &lt;- rnorm(100, 0, 1)\ne.1 &lt;- rnorm(100, 0, 2)\ny.1 &lt;- .5 + .55 * x.1 + e.1\nd.1 &lt;- data.frame(x.1,y.1)\nm.1 &lt;- lm(y.1 ~ x.1, data = d.1)\nd1.f&lt;- augment(m.1)\nd.1\n\n\n             x.1         y.1\n1   -0.560475647 -1.22907473\n2   -0.230177489  0.88716980\n3    1.558708314  0.86390582\n4    0.070508391 -0.15630558\n5    0.129287735 -1.33212888\n6    1.715064987  1.35323029\n7    0.460916206 -0.81630503\n8   -1.265061235 -3.53166755\n9   -0.686852852 -0.63822211\n10  -0.445661970  2.09287913\n11   1.224081797  0.02255106\n12   0.359813827  1.91382625\n13   0.400771451 -2.51534112\n14   0.110682716  0.44975156\n15  -0.555841135  1.23310178\n16   1.786913137  2.08510895\n17   0.497850478  0.98517015\n18  -1.966617157 -1.86305145\n19   0.701355902 -0.81366295\n20  -0.472791408 -1.80829286\n21  -1.067823706  0.14799016\n22  -0.217974915 -1.51483543\n23  -1.026004448 -1.04541733\n24  -0.728891229 -0.41307456\n25  -0.625039268  3.84395241\n26  -1.686693311 -1.73158112\n27   0.837787044  1.43155602\n28   0.153373118  0.74027691\n29  -1.138136937 -2.04968858\n30   1.253814921  1.04698203\n31   0.426464221  3.62365704\n32  -0.295071483  1.24071879\n33   0.895125661  1.07478496\n34   0.878133488  0.13797975\n35   0.821581082 -3.15462485\n36   0.688640254  3.14142657\n37   0.553917654 -2.11662543\n38  -0.061911711  1.94584358\n39  -0.305962664  4.14992767\n40  -0.380471001 -2.59704537\n41  -0.694706979  1.52147983\n42  -0.207917278 -0.13874948\n43  -1.265396352 -3.34025631\n44   2.168955965 -1.33640953\n45   1.207961998 -2.03869325\n46  -1.123108583 -1.17952277\n47  -0.402884835 -2.64509783\n48  -0.466655354  1.61917310\n49   0.779965118  5.12919870\n50  -0.083369066 -2.11991394\n51   0.253318514  2.21480288\n52  -0.028546755  2.02238377\n53  -0.042870457  1.14082641\n54   1.368602284 -0.76402196\n55  -0.225770986  0.13692074\n56   1.516470604  0.77326816\n57  -1.548752804  0.77416502\n58   0.584613750  0.07666005\n59   0.123854244  2.52206661\n60   0.215941569 -0.13039385\n61   0.379639483  2.81422465\n62  -0.502323453 -1.87463191\n63  -0.333207384 -2.20357455\n64  -1.018575383  6.42186341\n65  -1.071791226 -0.92320035\n66   0.303528641  1.26339594\n67   0.448209779  2.01965473\n68   0.053004227 -0.43840893\n69   0.922267468  2.04097120\n70   2.050084686  2.36547563\n71  -0.491031166 -0.20082816\n72  -2.309168876 -0.63945681\n73   1.005738524  0.98502168\n74  -0.709200763  4.36684338\n75  -0.688008616 -1.36107693\n76   1.025571370 -1.12792828\n77  -0.284773007  0.41895164\n78  -1.220717712  0.44956676\n79   0.181303480  1.47276387\n80  -0.138891362 -0.49312091\n81   0.005764186 -1.62348197\n82   0.385280401  3.23827457\n83  -0.370660032 -0.40316379\n84   0.644376549 -0.87661862\n85  -0.220486562 -0.09382675\n86   0.331781964  0.28812829\n87   1.096839013  3.32310204\n88   0.435181491  0.90882440\n89  -0.325931586  1.82884520\n90   1.148807618  0.13326016\n91   0.993503856  1.47531774\n92   0.548396960  0.15224650\n93   0.238731735  0.82046951\n94  -0.627906076 -1.63607506\n95   1.360652449 -1.37324422\n96  -0.600259587  4.16428400\n97   2.187332993  2.90445079\n98   1.532610626 -1.15960688\n99  -0.235700359 -0.85196703\n100 -1.026420900 -2.43549166"
  },
  {
    "objectID": "2-Basics.html#how-do-we-visualize-categorical-data",
    "href": "2-Basics.html#how-do-we-visualize-categorical-data",
    "title": "GLM basics",
    "section": "How do we visualize categorical data?",
    "text": "How do we visualize categorical data?\nNominal/categorical data does not have any inherent numbers associated with it. Think control/tx, eye color, etc.\n\n\nCode\nset.seed(123)\ngroup &lt;- c(0, 1)\nx.2 &lt;- rep(group, times = 50)\ne.1 &lt;- rnorm(100, 0, 1)\ny.1 &lt;- .5 + .85 * x.2 + e.1\nd.2 &lt;- data.frame(x.2,y.1)\nm.2 &lt;- lm(y.1 ~ x.2, data = d.2)\nd2.f&lt;- augment(m.2)\nd.2\n\n\n    x.2          y.1\n1     0 -0.060475647\n2     1  1.119822511\n3     0  2.058708314\n4     1  1.420508391\n5     0  0.629287735\n6     1  3.065064987\n7     0  0.960916206\n8     1  0.084938765\n9     0 -0.186852852\n10    1  0.904338030\n11    0  1.724081797\n12    1  1.709813827\n13    0  0.900771451\n14    1  1.460682716\n15    0 -0.055841135\n16    1  3.136913137\n17    0  0.997850478\n18    1 -0.616617157\n19    0  1.201355902\n20    1  0.877208592\n21    0 -0.567823706\n22    1  1.132025085\n23    0 -0.526004448\n24    1  0.621108771\n25    0 -0.125039268\n26    1 -0.336693311\n27    0  1.337787044\n28    1  1.503373118\n29    0 -0.638136937\n30    1  2.603814921\n31    0  0.926464221\n32    1  1.054928517\n33    0  1.395125661\n34    1  2.228133488\n35    0  1.321581082\n36    1  2.038640254\n37    0  1.053917654\n38    1  1.288088289\n39    0  0.194037336\n40    1  0.969528999\n41    0 -0.194706979\n42    1  1.142082722\n43    0 -0.765396352\n44    1  3.518955965\n45    0  1.707961998\n46    1  0.226891417\n47    0  0.097115165\n48    1  0.883344646\n49    0  1.279965118\n50    1  1.266630934\n51    0  0.753318514\n52    1  1.321453245\n53    0  0.457129543\n54    1  2.718602284\n55    0  0.274229014\n56    1  2.866470604\n57    0 -1.048752804\n58    1  1.934613750\n59    0  0.623854244\n60    1  1.565941569\n61    0  0.879639483\n62    1  0.847676547\n63    0  0.166792616\n64    1  0.331424617\n65    0 -0.571791226\n66    1  1.653528641\n67    0  0.948209779\n68    1  1.403004227\n69    0  1.422267468\n70    1  3.400084686\n71    0  0.008968834\n72    1 -0.959168876\n73    0  1.505738524\n74    1  0.640799237\n75    0 -0.188008616\n76    1  2.375571370\n77    0  0.215226993\n78    1  0.129282288\n79    0  0.681303480\n80    1  1.211108638\n81    0  0.505764186\n82    1  1.735280401\n83    0  0.129339968\n84    1  1.994376549\n85    0  0.279513438\n86    1  1.681781964\n87    0  1.596839013\n88    1  1.785181491\n89    0  0.174068414\n90    1  2.498807618\n91    0  1.493503856\n92    1  1.898396960\n93    0  0.738731735\n94    1  0.722093924\n95    0  1.860652449\n96    1  0.749740413\n97    0  2.687332993\n98    1  2.882610626\n99    0  0.264299641\n100   1  0.323579100"
  },
  {
    "objectID": "2-Basics.html#what-do-these-visualizations-have-in-common",
    "href": "2-Basics.html#what-do-these-visualizations-have-in-common",
    "title": "GLM basics",
    "section": "What do these visualizations have in common?",
    "text": "What do these visualizations have in common?\n\nLINES!\nMost of what we are going to do is represent the relationship between variables with lines (or planes or hyperplanes once we get into 2 or more variables)"
  },
  {
    "objectID": "2-Basics.html#thinking-in-terms-of-models-1",
    "href": "2-Basics.html#thinking-in-terms-of-models-1",
    "title": "GLM basics",
    "section": "Thinking in terms of models",
    "text": "Thinking in terms of models\n\nModels help us draw the lines\nOur DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (hence forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)\n\n\\(b_{1}\\) describes the strength of association i.e. the line!"
  },
  {
    "objectID": "2-Basics.html#see-this-in-our-r-code",
    "href": "2-Basics.html#see-this-in-our-r-code",
    "title": "GLM basics",
    "section": "See this in our R code",
    "text": "See this in our R code\nIndependent samples t-test\n\n\nCode\nt.1 &lt;- t.test(y.1 ~ x.2, data = d.2) \nt.1\n\n\n\n    Welch Two Sample t-test\n\ndata:  y.1 by x.2\nt = -4.4144, df = 93.846, p-value = 2.706e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1.1740658 -0.4455722\nsample estimates:\nmean in group 0 mean in group 1 \n      0.6104964       1.4203154"
  },
  {
    "objectID": "2-Basics.html#comparison-across-these-three-models",
    "href": "2-Basics.html#comparison-across-these-three-models",
    "title": "GLM basics",
    "section": "Comparison across these three models",
    "text": "Comparison across these three models\n\nNote that each of these three models (t, anova, regression) were exactly the same in terms of the mode: Y ~ X\nThat is because they are the same model! Different terms referring to the same thing is one of the major stumbling blocks of stats.\nYet they gave us different information. Depending on what you are interested in some information may be more pertinent.\nWe will focus on the regression model (glm) as it is most flexible"
  },
  {
    "objectID": "2-Basics.html#general-linear-model-glm",
    "href": "2-Basics.html#general-linear-model-glm",
    "title": "GLM basics",
    "section": "General linear model (GLM)",
    "text": "General linear model (GLM)\n\nThis model (equation) can be very simple as in a treatment/control experiment\nIt can be very complex in terms of trying to understand something like academic achievement\nThe majority of our models fall under the umbrella of a general(ized) linear model (often referred to as regression models)\nModels imply our theory about how the data are generated (ie how the world works)"
  },
  {
    "objectID": "2-Basics.html#regression-equation",
    "href": "2-Basics.html#regression-equation",
    "title": "GLM basics",
    "section": "Regression Equation",
    "text": "Regression Equation\n\\[Y_i = b_{0} + b_{1}X_i +e_i\\]\n\n\\(Y_i \\sim Normal(\\mu, \\sigma)\\)\nThe DV, \\(Y\\) for each person \\(i\\) is distributed normaly, with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\)"
  },
  {
    "objectID": "2-Basics.html#regression-terms",
    "href": "2-Basics.html#regression-terms",
    "title": "GLM basics",
    "section": "Regression terms",
    "text": "Regression terms\n\nY / DV / Outcome / Response / Criterion\nX / IV / Predictor / Explanatory variable\nRegression coefficient (weight) / b / b* / \\(\\beta\\)\nIntercept \\(b_0\\) / \\(\\beta_{0}\\)\nError / Residuals \\(e\\)\nPredictions \\(\\hat{Y}\\)"
  },
  {
    "objectID": "2-Basics.html#regression-models",
    "href": "2-Basics.html#regression-models",
    "title": "GLM basics",
    "section": "Regression models",
    "text": "Regression models\n\nThese models are a way to convey the relationship between two (or more) variables. They translate our hypotheses into math.\nWe can use these models to get information we may be interested in (e.g. means, SEs) and test hypotheses about the relationship among variables\n“All models are wrong but some are useful (and some are better than others)” - George Box"
  },
  {
    "objectID": "2-Basics.html#another-example",
    "href": "2-Basics.html#another-example",
    "title": "GLM basics",
    "section": "Another example",
    "text": "Another example\n\n\nCode\nlibrary(tidyverse)\nlibrary(readr)\nexample.data &lt;- read_csv(\"exampleData.csv\")\nexample.data &lt;- na.omit(example.data)\nexample.data\n\n\n# A tibble: 270 × 3\n      id    tx traffic.risk\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1         1.86\n 2     2     1         1   \n 3     3     1         3.29\n 4     4     1         2   \n 5     5     1         2.43\n 6     6     1         3.29\n 7     7     0         1.17\n 8     8     0         2.43\n 9     9     0         3   \n10    10     1         1.71\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#example-summary",
    "href": "2-Basics.html#example-summary",
    "title": "GLM basics",
    "section": "Example summary",
    "text": "Example summary\n\nSame p-values for each test; same SS; same test!\nt-test is a special form of a linear model\nanova is a special form of a linear model\nBecause the anova and t-test are narrower models, we will be working with the general linear model"
  },
  {
    "objectID": "2-Basics.html#parts-of-the-model",
    "href": "2-Basics.html#parts-of-the-model",
    "title": "GLM basics",
    "section": "Parts of the model",
    "text": "Parts of the model\n\\[Y_i = b_{0} + b_{1}X_i + e_i\\] \\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\nEach individual has a unique Y value an X value and a residual/error term\n\nThe model only has a single \\(b_{0}\\) and \\(b_{1}\\) term. These are the regression parameters. \\(b_{0}\\) is the intercept and \\(b_{1}\\) quantifies the relationship between your model of the world and the DV."
  },
  {
    "objectID": "2-Basics.html#what-do-the-estimates-tell-us",
    "href": "2-Basics.html#what-do-the-estimates-tell-us",
    "title": "GLM basics",
    "section": "What do the estimates tell us?",
    "text": "What do the estimates tell us?\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\nlibrary(broom)\ntidy(mod.1)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#how-to-interpret-regression-estimates",
    "href": "2-Basics.html#how-to-interpret-regression-estimates",
    "title": "GLM basics",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nIntercept is the mean of group of variable tx that is coded 0\nRegression coefficient is the slope or rise over run, scaled as a 1 unit on the x axis\n“For a one unit change in X, there is a b1 predicted change in Y.”\nRegression coefficient is the difference in means between the groups, given that we coded our groups as 0 and 1."
  },
  {
    "objectID": "2-Basics.html#how-to-interpret-regression-estimates-1",
    "href": "2-Basics.html#how-to-interpret-regression-estimates-1",
    "title": "GLM basics",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nThe entire class will go over different ways to interpret these estimates/parameters/coefficients\nIntercept (b0) signifies the level of Y when your model IVs (Xs) are zero\nRegression (b1) signifies the difference for a one unit change in your X"
  },
  {
    "objectID": "2-Basics.html#standard-errors",
    "href": "2-Basics.html#standard-errors",
    "title": "GLM basics",
    "section": "Standard errors",
    "text": "Standard errors\n\nThese coefficients are “best guesses” at some population parameter we want to make inferences about.\nTo do so we must balance our signal to our noise. If we have a strong signal (steep regression line/difference between groups) that would imply the groups differ.\nIf there was a lot of noise in that assessment then a big difference between groups may not be meaningful. We assess this “noise” component with our standard errors"
  },
  {
    "objectID": "2-Basics.html#sampling-distribution-refresher",
    "href": "2-Basics.html#sampling-distribution-refresher",
    "title": "GLM basics",
    "section": "Sampling distribution refresher",
    "text": "Sampling distribution refresher\n\nWe collect a sample and calculate a sample statistic \\(b_1\\)\nThis statistic is not a perfect assessment of the population\nWe calculate a sampling distribution to represent all possible samples we could have gotten from the same population with the same sample size\nThe standard deviation of the sampling distribution (standard error) reflects the spread of the hypothetical scores we could have gotten\nA large standard error means we have a flat sampling distribution and thus should not trust the estimate.\nPer convention, if our estimate is &gt; 2xSE away from 0, we say our estimate is “significantly different from zero”"
  },
  {
    "objectID": "2-Basics.html#predicted-scores",
    "href": "2-Basics.html#predicted-scores",
    "title": "GLM basics",
    "section": "Predicted scores",
    "text": "Predicted scores\n\nBased on the output how do I calculate means for each group?\n\n\ntidy(mod.1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#anova-as-regression",
    "href": "2-Basics.html#anova-as-regression",
    "title": "GLM basics",
    "section": "ANOVA as regression",
    "text": "ANOVA as regression\n\n“For a one unit change in X, there is a b1 predicted change in Y” will always be true.\nNominal/categorical variables do not have any inherent numbers associated with them so we need to assign them numbers\nWhat numbers you assign will impact the equation/estimates/hypothesis you can test\n\nBehoove you to code them as useful numbers. O and 1 are useful and are the default in R."
  },
  {
    "objectID": "2-Basics.html#anova-as-regression-1",
    "href": "2-Basics.html#anova-as-regression-1",
    "title": "GLM basics",
    "section": "ANOVA as regression",
    "text": "ANOVA as regression\n\\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\n\nCode\nexample.data\n\n\n# A tibble: 270 × 3\n      id    tx traffic.risk\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1         1.86\n 2     2     1         1   \n 3     3     1         3.29\n 4     4     1         2   \n 5     5     1         2.43\n 6     6     1         3.29\n 7     7     0         1.17\n 8     8     0         2.43\n 9     9     0         3   \n10    10     1         1.71\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#what-if-we-changed-0-and-1-to-other-values",
    "href": "2-Basics.html#what-if-we-changed-0-and-1-to-other-values",
    "title": "GLM basics",
    "section": "What if we changed 0 and 1 to other values?",
    "text": "What if we changed 0 and 1 to other values?\n\nInfinite number of ways to code categorical/nominal variables, only a few meaningful ways\n\nThe R default is called “dummy coding”\n\nUses 0s and 1s to put numbers to categories. We will soon see what this looks like when you have more than 2 groups.\nChanging the numbers changes…?"
  },
  {
    "objectID": "2-Basics.html#effect-coding",
    "href": "2-Basics.html#effect-coding",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\n\n\nCode\nexample.data$tx.effect &lt;- dplyr::recode(example.data$tx, '0' = -1, '1' = 1) \n\n\n\n\nCode\nexample.data\n\n\n# A tibble: 270 × 5\n      id    tx traffic.risk tx.r      tx.effect\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1     1     1         1.86 treatment         1\n 2     2     1         1    treatment         1\n 3     3     1         3.29 treatment         1\n 4     4     1         2    treatment         1\n 5     5     1         2.43 treatment         1\n 6     6     1         3.29 treatment         1\n 7     7     0         1.17 control          -1\n 8     8     0         2.43 control          -1\n 9     9     0         3    control          -1\n10    10     1         1.71 treatment         1\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#effect-coding-1",
    "href": "2-Basics.html#effect-coding-1",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\n\n\nCode\nmod.1.eff &lt;- lm(traffic.risk ~ tx.effect, data = example.data)\ntidy(mod.1.eff)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.41     0.0487     49.5  8.15e-137\n2 tx.effect     -0.241    0.0487     -4.94 1.38e-  6\n\n\n\nsystematically changes both the intercept and the regression estimate"
  },
  {
    "objectID": "2-Basics.html#effect-coding-2",
    "href": "2-Basics.html#effect-coding-2",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\nConsists of -1, 1s (And zeros for more than 2 groups)\n\nThe intercept is the “grand mean” or “mean of means” if unbalanced\nThe regression coefficient represents the group “effect” ie the difference between the grand mean and the group labeled 1 (we will revisit this when we have more than 2 groups as it will make more sense)\n\n\nCommon to use for Factorial ANOVA designs"
  },
  {
    "objectID": "2-Basics.html#dummy-coding",
    "href": "2-Basics.html#dummy-coding",
    "title": "GLM basics",
    "section": "Dummy coding",
    "text": "Dummy coding\n\nMore appropriate when you are interested in comparing to a specific group rather than an “average person.”\nIntercept: value of the group coded zero\nRegression coefficient: mean difference between groups"
  },
  {
    "objectID": "2-Basics.html#contrast-coding",
    "href": "2-Basics.html#contrast-coding",
    "title": "GLM basics",
    "section": "Contrast coding",
    "text": "Contrast coding\n\nAs our models get more complex our coding schemes can too\nWhat happens if you code the groups -.5 and .5?\nThese make more sense when we have more groups. More groups require more independent variables, however."
  },
  {
    "objectID": "2-Basics.html#categorical-coding-summary",
    "href": "2-Basics.html#categorical-coding-summary",
    "title": "GLM basics",
    "section": "categorical coding summary",
    "text": "categorical coding summary\n\nIn the end, it really doesn’t matter how you code your model. The overall “fit” of the model will be exactly the same because it is the same model.\nThe only thing that changes is the interpretation of your coefficients.\nEven then, you can recreate any test you want regardless of coding scheme. As a result, we often leave the default coding in place."
  },
  {
    "objectID": "2-Basics.html#statistical-inference",
    "href": "2-Basics.html#statistical-inference",
    "title": "GLM basics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nThe way the world is = our model + error\nHow good is our model? Is it a good representation of reality? Does it “fit” the data well?\nNeed to go beyond asking if it is significant, because what does that mean? Remember, all models are wrong\nWe are going to make predictions and see if the predictions (based on our model) matches our data\nWe can then compare one model to another to see which one matches our data better ie which one is a better representation of reality."
  },
  {
    "objectID": "2-Basics.html#predictions",
    "href": "2-Basics.html#predictions",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\nOur model is a prediction machine.\nThey are created by simply plugging a persons Xs into the created model\nIf you have bs and have Xs you can create a prediction\n\n\\(\\hat{Y}_{i}\\) = 2.65064 + -0.48111* \\(X_{i}\\)"
  },
  {
    "objectID": "2-Basics.html#predictions-1",
    "href": "2-Basics.html#predictions-1",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\nWe want our predictions to be close to our actual data for each person ( \\(Y_{i}\\) )\nThe difference between the actual data and our our prediction ( \\(Y_{i} - \\hat{Y}_{i} = e\\) ) is the residual, how far we are “off”. This tells us how good our fit is.\nYou can have the same estimates for two models but completely different fit.\nPreviously you may have evaluated overall model fit by looking at Eta Squared, SS Error and visualizing observations around group means"
  },
  {
    "objectID": "2-Basics.html#which-one-has-better-fit",
    "href": "2-Basics.html#which-one-has-better-fit",
    "title": "GLM basics",
    "section": "Which one has better fit?",
    "text": "Which one has better fit?\n\nCan you point out the predictions?\n\n\n\nCode\ntwogroup_fun = function(nrep = 100, b0 = 6, b1 = -2, sigma = 1) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\n\ntwogroup_fun2 = function(nrep = 100, b0 = 6, b1 = -2, sigma = 2) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\nset.seed(16)\nlibrary(broom)\nlm1 &lt;- augment(twogroup_fun())\n\nset.seed(16)\nlm2 &lt;- augment(twogroup_fun2())\n\nplot1&lt;- ggplot(lm1) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\nplot2&lt;- ggplot(lm2) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\n\nlibrary(gridExtra)\n grid.arrange(plot1, plot2, ncol=2)"
  },
  {
    "objectID": "2-Basics.html#easy-to-examine-fit-with-lm-objects",
    "href": "2-Basics.html#easy-to-examine-fit-with-lm-objects",
    "title": "GLM basics",
    "section": "Easy to examine fit with lm objects",
    "text": "Easy to examine fit with lm objects\n\nThese are automatically created anytime you run a lm in R\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\n\n\n\ncoefficients(mod.1)       # coefficients\nresiduals(mod.1)          # residuals\nfitted.values(mod.1)      # fitted values ie predicted\nsummary(mod.1)$r.squared  # R-sq for the model\nsummary(mod.1)$sigma      # sd of residuals"
  },
  {
    "objectID": "2-Basics.html#pop-quiz",
    "href": "2-Basics.html#pop-quiz",
    "title": "GLM basics",
    "section": "Pop quiz",
    "text": "Pop quiz\n\\[\\hat{Y}_{i} = b_{0} + b_{1}X_{i}\\]\n\\[Y_{i} = b_{0} + b_{1}X_{i} +e_{i}\\]\n\\[Y_{i}  - \\hat{Y}_{i} = e\\]\n\nCan you plug in numbers and calculate subject 3’s predicted and residual scores without explicitly asking for lm object residuals and fitted values? (using the same model from slide 41 on)\nPost answers in Slack – see if you and your peers get the same results!"
  },
  {
    "objectID": "2-Basics.html#residuals",
    "href": "2-Basics.html#residuals",
    "title": "GLM basics",
    "section": "Residuals",
    "text": "Residuals\n\n\nCode\nfit.1.data &lt;- augment(mod.1) \nfit.1.data\n\n\n# A tibble: 270 × 8\n   traffic.risk    tx .fitted .resid    .hat .sigma  .cooksd .std.resid\n          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1         1.86     1    2.17 -0.312 0.00602  0.780 0.000490     -0.402\n 2         1        1    2.17 -1.17  0.00602  0.777 0.00687      -1.51 \n 3         3.29     1    2.17  1.12  0.00602  0.777 0.00626       1.44 \n 4         2        1    2.17 -0.170 0.00602  0.780 0.000144     -0.218\n 5         2.43     1    2.17  0.259 0.00602  0.780 0.000337      0.334\n 6         3.29     1    2.17  1.12  0.00602  0.777 0.00626       1.44 \n 7         1.17     0    2.65 -1.48  0.00962  0.775 0.0178       -1.91 \n 8         2.43     0    2.65 -0.222 0.00962  0.780 0.000398     -0.287\n 9         3        0    2.65  0.349 0.00962  0.780 0.000986      0.451\n10         1.71     1    2.17 -0.455 0.00602  0.780 0.00104      -0.586\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#an-aside-concerning-lm-objects",
    "href": "2-Basics.html#an-aside-concerning-lm-objects",
    "title": "GLM basics",
    "section": "An aside concerning lm objects",
    "text": "An aside concerning lm objects\n\nlm objects consist of the information embedded in your linear model\nR often handles model objects poorly due to them not necessarily being in a usable data frame (lists!)\nthe broom package makes model objects into dataframes\n\n\n\nCode\nlibrary(broom)\nfit.1.tidy &lt;- tidy(mod.1)  \nfit.1.tidy\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#pop-quiz-2",
    "href": "2-Basics.html#pop-quiz-2",
    "title": "GLM basics",
    "section": "Pop quiz #2",
    "text": "Pop quiz #2\n\nFor a two group ANOVA how many different predicted values will we have? Residuals?\nPost your answer in Slack and see how you compare to your peers!"
  },
  {
    "objectID": "2-Basics.html#statistical-inference-1",
    "href": "2-Basics.html#statistical-inference-1",
    "title": "GLM basics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nTo the extent that we can generate different predicted values of Y based on the values of the predictors, our model is doing well\nSaid differently, the closer our model is to the “actual” data generating model, our guesses ( \\(\\hat{Y}\\) ) will be closer to our actual data ( \\(Y\\) )"
  },
  {
    "objectID": "2-Basics.html#statistical-inference-2",
    "href": "2-Basics.html#statistical-inference-2",
    "title": "GLM basics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nTo the extent that we can generate different predicted values of Y based on the values of the predictors, our model is doing well\nSaid differently, the closer our model is to the “actual” data generating model, our guesses ( \\(\\hat{Y}\\) ) will be closer to our actual data ( \\(Y\\) )"
  },
  {
    "objectID": "2-Basics.html#partitioning-the-variation-in-y",
    "href": "2-Basics.html#partitioning-the-variation-in-y",
    "title": "GLM basics",
    "section": "Partitioning the variation in Y",
    "text": "Partitioning the variation in Y\n\\[ \\sum (Y_i - \\bar{Y})^2 = \\sum (\\hat{Y}_i -\\bar{Y})^2 + \\sum(Y_i - \\hat{Y}_i)^2 \\]\n\nSS total = SS between + SS within\nSS total = SS regression + SS residual (or error)"
  },
  {
    "objectID": "2-Basics.html#what-can-we-do-with-this",
    "href": "2-Basics.html#what-can-we-do-with-this",
    "title": "GLM basics",
    "section": "What can we do with this?",
    "text": "What can we do with this?\n\nomnibus F tests (ANOVA)\nWhat hypothesis does the omnibus F test test, generally?\n\n\\[s_{y}^2 = s_{regression}^2 + s_{residual}^2\\]\n\\[1 = \\frac{s_{regression}^2}{s_{y}^2} + \\frac{s_{residual}^2}{s_{y}^2}\\]"
  },
  {
    "objectID": "2-Basics.html#coefficient-of-determination",
    "href": "2-Basics.html#coefficient-of-determination",
    "title": "GLM basics",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\\[\\frac{s_{regression}^2}{s_{y}^2} = \\frac{SS_{regression}}{SS_{Y}} = R^2\\]\n\nPercent (of total) variance explained by your model…which currently are groups\nAnother way of asking how much variance group status explains"
  },
  {
    "objectID": "2-Basics.html#r2-and-eta-squared",
    "href": "2-Basics.html#r2-and-eta-squared",
    "title": "GLM basics",
    "section": "\\(R^2\\) and Eta squared",
    "text": "\\(R^2\\) and Eta squared\n\nsummary(mod.1)$r.squared\n\n[1] 0.08344007\n\n\n\nlibrary(lsr)\netaSquared(mod.1)\n\n       eta.sq eta.sq.part\ntx 0.08344007  0.08344007"
  },
  {
    "objectID": "2-Basics.html#r2-for-different-coding-schemes",
    "href": "2-Basics.html#r2-for-different-coding-schemes",
    "title": "GLM basics",
    "section": "\\(R^2\\) for different coding schemes",
    "text": "\\(R^2\\) for different coding schemes\n\nglance(mod.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0834        0.0800 0.779      24.4 0.00000138     1  -315.  635.  646.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(mod.1.eff)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0834        0.0800 0.779      24.4 0.00000138     1  -315.  635.  646.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "2-Basics.html#note-the-r2-p-value",
    "href": "2-Basics.html#note-the-r2-p-value",
    "title": "GLM basics",
    "section": "Note the \\(R^2\\) p-value",
    "text": "Note the \\(R^2\\) p-value\n\n\nCode\nsummary(mod.1)\n\n\n\nCall:\nlm(formula = traffic.risk ~ tx, data = example.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.65064 -0.59811 -0.02668  0.54475  2.54475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65064    0.07637  34.707  &lt; 2e-16 ***\ntx          -0.48111    0.09740  -4.939 1.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7789 on 268 degrees of freedom\nMultiple R-squared:  0.08344,   Adjusted R-squared:  0.08002 \nF-statistic:  24.4 on 1 and 268 DF,  p-value: 1.381e-06"
  },
  {
    "objectID": "2-Basics.html#summary",
    "href": "2-Basics.html#summary",
    "title": "GLM basics",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "2-Basics.html#summary-1",
    "href": "2-Basics.html#summary-1",
    "title": "GLM basics",
    "section": "Summary",
    "text": "Summary\n\nWe are using linear models to do the exact same tests as t-tests and ANOVAs\nIt is the exact same because t-tests and ANOVAs are part of the general linear model\nThe GLM provides a more systematic way at 1) building and testing your theoretical model and 2) comparing between alternative theoretical models\nYou can get 1) estimates and 2) fit statistics from the model. Both are important."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Download syllabus"
  },
  {
    "objectID": "1-HW.html",
    "href": "1-HW.html",
    "title": "Homework 1",
    "section": "",
    "text": "We will use the `palmerpenguins` dataset. Please install the pacakge `palmerpenguins` if you do not already have it and use the `penguins` data.frame.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "1-HW.html#question-1",
    "href": "1-HW.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\nRun a model to test the question:\n&gt; Do male and female penguins have different length flippers?\n\nYes or no, is the flipper length of male penguins signiciantly different from the flipper length of female penguins? How do you know?\nWhat is the (mean) flipper length for males and females, respectively?\nIs this a good model? How do you know?\nUsing the output, write the formal equation (see Slide 26 if confused)\nInterpret the intercept and the regression coefficient\nIf you were back in your OG Psych Stats days, what would we call this statistical test?\nMake a figure to illustrate this relationship. In it, you should include a summary metric (e.g., the mean or median) as well as information about the distribution of data, the raw data, or both. Look at slides for inspiration!"
  },
  {
    "objectID": "1-HW.html#question-2",
    "href": "1-HW.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\nRun a model to test the question:\n&gt; Are penguins on different islands the same size?\n\nTry to write the formal equation -- just do your best! In a sentence or two, what makes this equation different from the one you wrote in Question 1?\nIf you were back in your OG Psych Stats days, what would we call this statistical test?\nGiven the name of this statistical test, replace `lm` with a different function. That is, use a function that is *not* `lm` to run the same model. (I’m being intentionally vague here…). What changes about this new output, compared to the model in Question 1, and what stays the same?\nMake a figure to compare the size of penguins on different islands. Make sure to give an interval around the point estimate (*hint: this usually comes in the form of ‘bars’*)\nIs this a good model? How do you know?\nGiven the name of this statistical test, calculate an effect size. What do you notice between this effect size and the `lm` object output? Any similarities?"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "homework",
    "section": "",
    "text": "Homework 2\nHomework 1"
  },
  {
    "objectID": "3-Basics.html#this-time",
    "href": "3-Basics.html#this-time",
    "title": "GLM basics II",
    "section": "This time",
    "text": "This time\n\nWhat is regression and why is it useful?\nNuts and bolts\n\nEquation\nOrdinary least squares\nInterpretation"
  },
  {
    "objectID": "3-Basics.html#regression",
    "href": "3-Basics.html#regression",
    "title": "GLM basics II",
    "section": "Regression",
    "text": "Regression\nRegression is a general data analytic system, meaning lots of things fall under the umbrella of regression. This system can handle a variety of forms of relations, although all forms have to be specified in a linear way. Usefully, we can incorporate IVs of all nature – continuous, categorical, nominal, ordinal….\nThe output of regression includes both effect sizes and, if using frequentist or Bayesian software, statistical significance. We can also incorporate multiple influences (IVs) and account for their intercorrelations."
  },
  {
    "objectID": "3-Basics.html#regression-1",
    "href": "3-Basics.html#regression-1",
    "title": "GLM basics II",
    "section": "Regression",
    "text": "Regression\n\nScientific use: explaining the influence of one or more variables on some outcome.\n\nDoes this intervention affect reaction time? Does self-esteem predict relationship quality?\n\nPrediction use: We can develop models based on what’s happened in the past to predict what will happen in the figure.\n\nInsurance premiums? Graduate school… success?\n\nAdjustment: Statistically control for known effects\n\nIf everyone had the same level of SES, would abuse still be associated with criminal behavior?"
  },
  {
    "objectID": "3-Basics.html#regression-equation",
    "href": "3-Basics.html#regression-equation",
    "title": "GLM basics II",
    "section": "Regression equation",
    "text": "Regression equation\nWhat is a regression equation?\n\nFunctional relationship\n\nIdeally like a physical law \\((E = MC^2)\\)\nIn practice, it’s never as robust as that\n\n\nHow do we uncover the relationship?"
  },
  {
    "objectID": "3-Basics.html#how-does-y-vary-with-x",
    "href": "3-Basics.html#how-does-y-vary-with-x",
    "title": "GLM basics II",
    "section": "How does Y vary with X?",
    "text": "How does Y vary with X?\n\nThe regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X\n“Our best guess” regardless of whether our model includes categories or continuous predictor variables\nWe will evaluate our guesses based on how far away we are from the mean. But how do we come up with those guesses in the first place?"
  },
  {
    "objectID": "3-Basics.html#regression-equation-1",
    "href": "3-Basics.html#regression-equation-1",
    "title": "GLM basics II",
    "section": "Regression Equation",
    "text": "Regression Equation\n\\[\\Large Y = b_{0} + b_{1}X +e\\]\n\\[\\Large \\hat{Y} = b_{0} + b_{1}X\\]\n\\(\\hat{Y}\\) signifies the predicted score – no error\nThe difference between the predicted and observed score is the residual ( \\(e_i\\) )"
  },
  {
    "objectID": "3-Basics.html#ols",
    "href": "3-Basics.html#ols",
    "title": "GLM basics II",
    "section": "OLS",
    "text": "OLS\n\nHow do we find the regression estimates?\nOrdinary Least Squares (OLS) estimation\nMinimizes deviations\n\n\\[ min\\sum(Y_{i}-\\hat{Y})^{2} \\]\n\nOther estimation procedures possible (and necessary in some cases)"
  },
  {
    "objectID": "3-Basics.html#compare-with-bad-fit",
    "href": "3-Basics.html#compare-with-bad-fit",
    "title": "GLM basics II",
    "section": "compare with bad fit",
    "text": "compare with bad fit\n\n\nCode\nnew.i = 1.1\nnew.slope = -0.7\nd1.f$new.fitted = 1.1 -0.7*d1.f$x.1\n\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_abline(intercept = new.i, slope = new.slope, color = \"blue\", linewidth = 1) +\n  geom_point(aes(y = new.fitted), shape = 1, size = 2) +\n  geom_segment(aes( xend = x.1, yend = new.fitted))+\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "3-Basics.html#what-is-error",
    "href": "3-Basics.html#what-is-error",
    "title": "GLM basics II",
    "section": "What is error?",
    "text": "What is error?\n\\[\\Large Y = b_{0} + b_{1}X +e\\]\n\\[\\Large \\hat{Y} = b_{0} + b_{1}X\\]\n\\[\\Large Y_i = \\hat{Y_i} + e_i\\]\n\\[\\Large e_i = Y_i - \\hat{Y_i}\\]"
  },
  {
    "objectID": "3-Basics.html#ols-1",
    "href": "3-Basics.html#ols-1",
    "title": "GLM basics II",
    "section": "OLS",
    "text": "OLS\nThe line that yields the smallest sum of squared deviations\n\\[\\Large \\Sigma(Y_i - \\hat{Y_i})^2\\] \\[\\Large = \\Sigma(Y_i - (b_0+b_{1}X_i))^2\\] \\[\\Large = \\Sigma(e_i)^2\\]"
  },
  {
    "objectID": "3-Basics.html#regression-coefficient-b_1",
    "href": "3-Basics.html#regression-coefficient-b_1",
    "title": "GLM basics II",
    "section": "Regression coefficient, \\(b_{1}\\)",
    "text": "Regression coefficient, \\(b_{1}\\)\n\\[\\large b_{1} = \\frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \\frac{s_{y}}{s_{x}}\\]\n\\[\\large r_{xy} = \\frac{s_{xy}}{s_xs_y}\\]\n\nThe regression coefficient (slope) equals the estimated change in Y for a 1-unit change in X"
  },
  {
    "objectID": "3-Basics.html#standardized-regression",
    "href": "3-Basics.html#standardized-regression",
    "title": "GLM basics II",
    "section": "Standardized regression",
    "text": "Standardized regression\n\nRegression using z-scores for Y and X\nCorrelation equals standardized regression coefficient\n\n\\[\\large b_{1} = r_{xy} \\frac{s_{y}}{s_{x}}\\]\n\\[ \\large r_{xy} = b_1\\frac{s_x}{s_y} \\]\nIf the variance of both X and Y is equal to 1 (as in z-scores):\n\\[\\large \\beta_{1} = b_{1}^* = r_{xy}\\]"
  },
  {
    "objectID": "3-Basics.html#standardized-regression-equation",
    "href": "3-Basics.html#standardized-regression-equation",
    "title": "GLM basics II",
    "section": "Standardized regression equation",
    "text": "Standardized regression equation\n\\[\\large Y = b_{1}^*X+e\\]\n\\[\\large b_{1}^* = b_{1}\\frac{s_x}{s_y}\\]\nWhen \\(X = 0, Y = 0\\). A one-standard deviation increase in X is associated with a \\(b_{1}^*\\) standard deviation increase in Y. Our regression coefficient is equivalent to the correlation coefficient when we have only one predictor in our model."
  },
  {
    "objectID": "3-Basics.html#estimating-the-intercept-b_0",
    "href": "3-Basics.html#estimating-the-intercept-b_0",
    "title": "GLM basics II",
    "section": "Estimating the intercept, \\(b_0\\)",
    "text": "Estimating the intercept, \\(b_0\\)\n\nRe-write equation to include the means of Y and X ( \\(\\bar{X}\\) & \\(\\bar{Y}\\) )\nIntercept serves to adjust for differences in means between X and Y\n\n\\[\\Large \\hat{Y} = \\bar{Y} + r_{xy} \\frac{s_{y}}{s_{x}}(X-\\bar{X})\\] - If standardized, intercept drops out. Otherwise, intercept is where regression line crosses the y-axis at X = 0\n- Notice that when \\(X = \\bar{X}\\) the regression line goes through \\(\\bar{Y}\\). This is true for all regressions such that the regression line must pass through \\(\\bar{X}\\) and \\(\\bar{Y}\\)"
  },
  {
    "objectID": "3-Basics.html#example",
    "href": "3-Basics.html#example",
    "title": "GLM basics II",
    "section": "Example",
    "text": "Example\n\n\nCode\nlibrary(psych)\ngalton.data &lt;- psychTools::galton\nhead(galton.data)\n\n\n  parent child\n1   70.5  61.7\n2   68.5  61.7\n3   65.5  61.7\n4   64.5  61.7\n5   64.0  61.7\n6   67.5  62.2\n\n\nCode\ndescribe(galton.data, fast = T)\n\n\n       vars   n  mean   sd  min  max range   se\nparent    1 928 68.31 1.79 64.0 73.0     9 0.06\nchild     2 928 68.09 2.52 61.7 73.7    12 0.08\n\n\nCode\ncor(galton.data)\n\n\n          parent     child\nparent 1.0000000 0.4587624\nchild  0.4587624 1.0000000"
  },
  {
    "objectID": "3-Basics.html#in-r",
    "href": "3-Basics.html#in-r",
    "title": "GLM basics II",
    "section": "In R",
    "text": "In R\n\n\nCode\nfit.1 &lt;- lm(child ~ parent, data = galton.data)\nsummary(fit.1)\n\n\n\nCall:\nlm(formula = child ~ parent, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "3-Basics.html#reversed",
    "href": "3-Basics.html#reversed",
    "title": "GLM basics II",
    "section": "Reversed",
    "text": "Reversed\n\n\nCode\nsummary(lm(parent ~ child, data = galton.data))\n\n\n\nCall:\nlm(formula = parent ~ child, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6702 -1.1702 -0.1471  1.1324  4.2722 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***\nchild        0.32565    0.02073   15.71   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.589 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "3-Basics.html#data-predicted-and-residuals",
    "href": "3-Basics.html#data-predicted-and-residuals",
    "title": "GLM basics II",
    "section": "Data, predicted, and residuals",
    "text": "Data, predicted, and residuals\n\n\nCode\nlibrary(broom)\nmodel_info = augment(fit.1)\nhead(model_info)\n\n\n# A tibble: 6 × 8\n  child parent .fitted .resid    .hat .sigma .cooksd .std.resid\n  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1  61.7   70.5    69.5  -7.81 0.00270   2.22 0.0165       -3.49\n2  61.7   68.5    68.2  -6.51 0.00109   2.23 0.00462      -2.91\n3  61.7   65.5    66.3  -4.57 0.00374   2.23 0.00787      -2.05\n4  61.7   64.5    65.6  -3.93 0.00597   2.24 0.00931      -1.76\n5  61.7   64      65.3  -3.60 0.00735   2.24 0.00966      -1.62\n6  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40"
  },
  {
    "objectID": "3-Basics.html#residuals",
    "href": "3-Basics.html#residuals",
    "title": "GLM basics II",
    "section": "Residuals",
    "text": "Residuals\n\nDispersion of residuals can be thought of as what is left over in Y that is not explained by our model. As residuals get smaller on average, so will the SD of the residuals.\nSigma ( \\(\\sigma\\) ) is the SD of residuals. It can be thought of as how much left over in Y that we cannot explain by our model."
  },
  {
    "objectID": "3-Basics.html#residuals-summary",
    "href": "3-Basics.html#residuals-summary",
    "title": "GLM basics II",
    "section": "Residuals Summary",
    "text": "Residuals Summary\n\nResiduals are not correlated with \\(X\\) and \\(\\hat{Y}\\) because those two are perfectly correlated with one another (that is, \\(r_{\\text{fitted,x}} = 1\\) )\n\\(X\\) and \\(\\hat{Y}\\) represent the same information. We use our model ( \\(X\\) ) to make a prediction ( \\(\\hat{Y}\\) ). These predictions are entirely based on the model.\nThere is no correlation between residuals with \\(X\\) and \\(\\hat{Y}\\) because they are created by subtracting them out of \\(Y\\). That is, ( \\(\\epsilon = Y - \\hat{Y}\\) )\nSigma ( \\(\\sigma\\) ; the SD of residuals) can be thought of as how much left over in \\(Y\\) after we take out all of the information our model provides."
  },
  {
    "objectID": "2-HW.html",
    "href": "2-HW.html",
    "title": "Homework 2",
    "section": "",
    "text": "Data: You will need to use the Dawtry et al., 2015 Psychological Science article and the accompanying dataset to answer all questions. Please download this dawtry-hw2-data dataset for the homework, which corresponds to study 1a of the paper. The paper can be found here.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd (or .qmd) and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "2-HW.html#question-1",
    "href": "2-HW.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nGraph the distributions of the following items. Be sure to comment on anything you find irregular or interesting. The variables to be plotted are:\n\nHousehold Income\nFairness and Satisfaction\nSocial Circle Mean Income\nPopulation Mean Income\nPolitical Preference"
  },
  {
    "objectID": "2-HW.html#question-2",
    "href": "2-HW.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nRun a regression where political preferences predict redistribution:\n\nWrite the formal equation (be specific!)\nInterpret the intercept and the regression coefficient (be specific!)\nStandardize the variables, rerun the model, write the formal equation (be specific!) and interpret the intercept and the regression coefficient (be specific!)\nCalculate the correlation of the two variables (use the original variables in raw units). How does this relate to the standardized and unstandardized regression coefficients?\nCreated a fitted vs. residuals plot. Do not use the defaults – you must (at the very least) change the labels, colors, and give it a titles. Then interpret this plot."
  },
  {
    "objectID": "hw-02.html",
    "href": "hw-02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Data: You will need to use the Dawtry et al., 2015 Psychological Science article and the accompanying dataset to answer all questions. Please download this dawtry-hw2-data dataset for the homework, which corresponds to study 1a of the paper. The paper can be found here.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "hw-02.html#question-1",
    "href": "hw-02.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nGraph the distributions of the following items. Be sure to comment on anything you find irregular or interesting. The variables to be plotted are:\n\nHousehold Income\nFairness and Satisfaction\nSocial Circle Mean Income\nPopulation Mean Income\nPolitical Preference"
  },
  {
    "objectID": "hw-02.html#question-2",
    "href": "hw-02.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nRun a regression where political preferences predict redistribution:\n\nWrite the formal equation (be specific!)\nInterpret the intercept and the regression coefficient (be specific!)\nStandardize the variables, rerun the model, write the formal equation (be specific!) and interpret the intercept and the regression coefficient (be specific!)\nCalculate the correlation of the two variables (use the original variables in raw units). How does this relate to the standardized and unstandardized regression coefficients?\nCreated a fitted vs. residuals plot. Do not use the defaults – you must (at the very least) change the labels, colors, and give it a titles. Then interpret this plot."
  },
  {
    "objectID": "4-Basics.html#statistical-inference",
    "href": "4-Basics.html#statistical-inference",
    "title": "GLM basics III",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nThe way the world is = our model + error\nHow good is our model? Does it “fit” the data well?\nTo assess how well our model fits the data, we simply take all the variability in our outcome and partition it into different categories. For now, we will partition it into two categories: the variability that is predicted by (explained by) our model, and variability that is not."
  },
  {
    "objectID": "4-Basics.html#partitioning-variation",
    "href": "4-Basics.html#partitioning-variation",
    "title": "GLM basics III",
    "section": "Partitioning variation",
    "text": "Partitioning variation\n\nWe formally test how well we are doing with our guesses by partitioning variation\nTo the extent that we can generate different predicted values of Y, based on the values of the predicors, we are doing well in our prediction\n\n\\[\\sum (Y - \\bar{Y})^2 = \\sum (\\hat{Y} -\\bar{Y})^2 + \\sum(Y - \\hat{Y})^2\\]\nEach of these is the sum of a squared deviation from an expected value of Y. We can abbreviate the sum of squared deviations:\n\\[SS_{Y} = SS_{\\text{Model}} + SS_{\\text{Residual}}\\]\n\\[\\frac{s^2_{regression}}{s^2_y} = \\frac{SS_{regression}}{SS_Y} = R^2\\]"
  },
  {
    "objectID": "4-Basics.html#partitioning-variance",
    "href": "4-Basics.html#partitioning-variance",
    "title": "GLM basics III",
    "section": "Partitioning Variance",
    "text": "Partitioning Variance\nThe relative magnitude of sums of squares, especially in more complex designs, provides a way of identifying particularly large and important sources of variability. In the future, we can further partition \\(SS_{\\text{Model}}\\) and \\(SS_{\\text{Residual}}\\) into smaller pieces, which will help us make more specific inferences and increase statistical power, respectively.\n\\[\\Large s^2_Y = s^2_{\\hat{Y}} + s^2_{e}\\]"
  },
  {
    "objectID": "4-Basics.html#partitioning-variance-in-y",
    "href": "4-Basics.html#partitioning-variance-in-y",
    "title": "GLM basics III",
    "section": "Partitioning variance in Y",
    "text": "Partitioning variance in Y\nConsider the case with no correlation between X and Y"
  },
  {
    "objectID": "4-Basics.html#large-haty-bary-r_xy-fracs_ys_xx-barx",
    "href": "4-Basics.html#large-haty-bary-r_xy-fracs_ys_xx-barx",
    "title": "GLM basics III",
    "section": "\\[\\Large \\hat{Y} = \\bar{Y} + r_{xy} \\frac{s_{y}}{s_{x}}(X-\\bar{X})\\]",
    "text": "\\[\\Large \\hat{Y} = \\bar{Y} + r_{xy} \\frac{s_{y}}{s_{x}}(X-\\bar{X})\\]\n\\[\\Large \\hat{Y} = \\bar{Y}\\]\nTo the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction\n\\[\\large \\sum (Y - \\bar{Y})^2 = \\sum (\\hat{Y} -\\bar{Y})^2 + \\sum(Y - \\hat{Y})^2\\]\n\\[\\Large SS_Y = SS_{\\text{Model}} + SS_{\\text{Residual}}\\]"
  },
  {
    "objectID": "4-Basics.html#coefficient-of-determination",
    "href": "4-Basics.html#coefficient-of-determination",
    "title": "GLM basics III",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\\[\\Large \\frac{s_{Model}^2}{s_{y}^2} = \\frac{SS_{Model}}{SS_{Y}} = R^2\\]\n\\(R^2\\) represents the proportion of variance in Y that is explained by the model.\n\\(\\sqrt{R^2} = R\\) is the correlation between the predicted values of Y from the model and the actual values of Y\n\\[\\large \\sqrt{R^2} = r_{Y\\hat{Y}}\\]"
  },
  {
    "objectID": "4-Basics.html#example",
    "href": "4-Basics.html#example",
    "title": "GLM basics III",
    "section": "Example",
    "text": "Example\n\n\nCode\ngalton.data &lt;- psychTools::galton\nfit.1 = lm(child ~ parent, data = galton.data)\nsummary(fit.1) \n\n\n\nCall:\nlm(formula = child ~ parent, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.1)$r.squared\n\n\n[1] 0.2104629"
  },
  {
    "objectID": "4-Basics.html#example-1",
    "href": "4-Basics.html#example-1",
    "title": "GLM basics III",
    "section": "Example",
    "text": "Example\n\n\nCode\ncor(galton.data$parent, galton.data$child, use = \"pairwise\")\n\n\n[1] 0.4587624\n\n\n\n\nCode\ncor(galton.data$parent, galton.data$child)^2\n\n\n[1] 0.2104629"
  },
  {
    "objectID": "4-Basics.html#computing-sum-of-squares",
    "href": "4-Basics.html#computing-sum-of-squares",
    "title": "GLM basics III",
    "section": "Computing Sum of Squares",
    "text": "Computing Sum of Squares\n\\[\\Large \\frac{SS_{Model}}{SS_{Y}} = R^2\\] \\[\\Large SS_{Model} = R^2({SS_{Y})}\\]\n\\[\\Large SS_{Y} = SS_{Model} + SS_{residual}\\]\n\\[\\Large SS_{residual} = SS_{Y} - R^2({SS_{Y})}\\]\n\\[\\Large  SS_{residual} = (1- R^2){SS_{Y}}\\]"
  },
  {
    "objectID": "4-Basics.html#using-r-to-check-yourself",
    "href": "4-Basics.html#using-r-to-check-yourself",
    "title": "GLM basics III",
    "section": "Using R To Check Yourself",
    "text": "Using R To Check Yourself\n\\[SS_{residual} = (1- R^2){SS_{Y}}\\]\n\nr2 = summary(fit.1)$r.squared\n\nfit.1.anova = summary(aov(fit.1))\n\nssTotal = fit.1.anova[[1]]$`Sum Sq`[1] + fit.1.anova[[1]]$`Sum Sq`[2]\n\nssResidual = (1 - r2) * ssTotal\n\n# print things out\nfit.1.anova\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nparent        1   1237    1237   246.8 &lt;2e-16 ***\nResiduals   926   4640       5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nssResidual\n\n[1] 4640.273\n\n\n???"
  },
  {
    "objectID": "4-Basics.html#mean-square-error-mse-mswmsr",
    "href": "4-Basics.html#mean-square-error-mse-mswmsr",
    "title": "GLM basics III",
    "section": "Mean Square Error (MSE) (msw/msr)",
    "text": "Mean Square Error (MSE) (msw/msr)\n\nAKA square of residual standard error/deviation (sigma)\nUnbiased estimate of error variance\nMeasure of discrepancy between the data and the model\nThe MSE is the variance of data around the fitted regression line\nJust like with MSwithin, which was variance around predicted group means\nIt is the mean of the square of the residuals\n\n\nhead(fit.1$residuals)\n\n        1         2         3         4         5         6 \n-7.805016 -6.512435 -4.573563 -3.927273 -3.604127 -5.366144 \n\nmse = round(mean(fit.1$residuals ^ 2), digits = 2)\nmse\n\n[1] 5"
  },
  {
    "objectID": "4-Basics.html#mse",
    "href": "4-Basics.html#mse",
    "title": "GLM basics III",
    "section": "MSE",
    "text": "MSE\n\nIt is the square of the residual standard error/deviation (sigma) aka \\(RSE^2\\)\n\n\nmse = round((summary(fit.1)[[\"sigma\"]]) ^2, digits = 2)\nmse\n\n[1] 5.01"
  },
  {
    "objectID": "4-Basics.html#residual-standard-error",
    "href": "4-Basics.html#residual-standard-error",
    "title": "GLM basics III",
    "section": "Residual Standard Error",
    "text": "Residual Standard Error\n\n\n\nCall:\nlm(formula = child ~ parent, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "4-Basics.html#residual-standard-errordeviation",
    "href": "4-Basics.html#residual-standard-errordeviation",
    "title": "GLM basics III",
    "section": "Residual standard error/deviation",
    "text": "Residual standard error/deviation\n\naka standard deviation of the residual\naka standard error of the estimate\n\n\\[\\hat{\\sigma} = \\sqrt{\\frac{SS_{\\text{Residual}}}{df_{\\text{Residual}}}} = s_{Y|X} = \\sqrt{\\frac{\\Sigma(Y_i -\\hat{Y_i})^2}{N-2}}\\]\n\ninterpreted in original units (unlike \\(R^2\\))\nstandard deviation of Y not accounted by model"
  },
  {
    "objectID": "4-Basics.html#residual-standard-errordeviation-or-standard-error-of-the-estimate",
    "href": "4-Basics.html#residual-standard-errordeviation-or-standard-error-of-the-estimate",
    "title": "GLM basics III",
    "section": "Residual standard error/deviation or standard error of the estimate",
    "text": "Residual standard error/deviation or standard error of the estimate\n\nsummary(fit.1)$sigma \n\n[1] 2.238547\n\ngalton.data.1 = broom::augment(fit.1)\npsych::describe(galton.data.1$.resid)\n\n   vars   n mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 928    0 2.24   0.05    0.06 2.26 -7.81 5.93 13.73 -0.24    -0.23 0.07\n\nsd(galton.data$child)\n\n[1] 2.517941\n\n\nBecause the size of \\(\\hat{\\sigma}\\) depends on both how well the model does as well as the original units of measurement, it is important to compare it to Y to evaluate!"
  },
  {
    "objectID": "4-Basics.html#rse-vs-mse",
    "href": "4-Basics.html#rse-vs-mse",
    "title": "GLM basics III",
    "section": "RSE vs MSE",
    "text": "RSE vs MSE\nResidual standard error = square root of the mean square error\nBoth measuring error, but RSE is a little more useful\n\nsqrt(mse)\n\n[1] 2.238303"
  },
  {
    "objectID": "4-Basics.html#residual-standard-error-and-sigma",
    "href": "4-Basics.html#residual-standard-error-and-sigma",
    "title": "GLM basics III",
    "section": "Residual Standard Error and Sigma",
    "text": "Residual Standard Error and Sigma\n\nSo many names to represent the spread of data around the regression line\nStandard deviation of the residual, standard error of the estimate, MSE…\nWe will refer to this as sigma, and use estimated sigma, as we do not know the population value ( \\(\\hat{\\sigma}\\) )\nIt is interpreted in original units (unlike \\(R^2\\) )\nIt is the standard deviation of Y not accounted by the model (i.e., residuals)"
  },
  {
    "objectID": "4-Basics.html#why-do-we-care-about-sigma",
    "href": "4-Basics.html#why-do-we-care-about-sigma",
    "title": "GLM basics III",
    "section": "Why do we care about sigma?",
    "text": "Why do we care about sigma?\n\nLet’s simulate!\nData generating process: \\[Y_{i} \\sim \\mathcal{N}(\\mu,\\,\\sigma)\\ \\]\nIn English: our DV for individual \\(i\\) is distributed normally with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\)\nThis describes how we think our DVs are generated, and the paramters of interest A a standard regression model assumes this, but we will see other DGPs such as binomial or poisson that do not\nFor normal, \\(\\mu\\) gets all the focus but \\(\\sigma\\) is just as important"
  },
  {
    "objectID": "4-Basics.html#r2-and-residual-standard-deviation",
    "href": "4-Basics.html#r2-and-residual-standard-deviation",
    "title": "GLM basics III",
    "section": "R2 and residual standard deviation",
    "text": "R2 and residual standard deviation\n\ntwo sides of same coin\none in original units, the other standardized\n\\(R^2\\) can be tricky because the numerator and denominator can be changed in different ways.\nfor example if variance in Y is changed but with the same regression model and residual standard error, \\(R^2\\) could increase or decrease"
  },
  {
    "objectID": "4-Basics.html#inferential-tests",
    "href": "4-Basics.html#inferential-tests",
    "title": "GLM basics III",
    "section": "Inferential tests",
    "text": "Inferential tests\nNHST is about making decisions:\n\nthese two means are/are not different\nthis correlation is/is not significantly different from 0\nthe distribution of this categorical variable is/is not different between these groups\n\nIn regression, there are several inferential tests being conducted at once. The first is called the omnibus test – this is a test of whether the model fits the data."
  },
  {
    "objectID": "4-Basics.html#omnibus-test",
    "href": "4-Basics.html#omnibus-test",
    "title": "GLM basics III",
    "section": "Omnibus test",
    "text": "Omnibus test\n\\[\\Large H_{0}: \\rho_{XY}^2= 0\\]\n\\[\\Large H_{0}: \\rho_{XY}^2 \\neq 0\\]\nIt is possible to calculate the significance of your regression with a correlation test. In fact, it would seem quite practical and logical to do so."
  },
  {
    "objectID": "4-Basics.html#model-comparisons",
    "href": "4-Basics.html#model-comparisons",
    "title": "GLM basics III",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\nSo the omnibus \\(F\\)-statistic is the ratio of MSregression to MSresidual\nTest of overall significance. Does your model give a better fit to the data than a model that contains no independent variables?\nThis is truly a question of model comparisons!"
  },
  {
    "objectID": "4-Basics.html#full-vs.-restricted-models",
    "href": "4-Basics.html#full-vs.-restricted-models",
    "title": "GLM basics III",
    "section": "Full vs. Restricted Models",
    "text": "Full vs. Restricted Models\n\nfit.1 &lt;- lm(child ~ parent, data = galton.data)\nfit.0 &lt;- lm(child ~ 1, data = galton.data)\n\nsummary(fit.0)\n\n\nCall:\nlm(formula = child ~ 1, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3885 -1.8885  0.1115  2.1115  5.6115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 68.08847    0.08266   823.8   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.518 on 927 degrees of freedom"
  },
  {
    "objectID": "4-Basics.html#the-comparison",
    "href": "4-Basics.html#the-comparison",
    "title": "GLM basics III",
    "section": "The comparison!",
    "text": "The comparison!\n\nanova(fit.1, fit.0)\n\nAnalysis of Variance Table\n\nModel 1: child ~ parent\nModel 2: child ~ 1\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    926 4640.3                                  \n2    927 5877.2 -1   -1236.9 246.84 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-1",
    "href": "4-Basics.html#model-comparisons-1",
    "title": "GLM basics III",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\nModel comparisons are redundant with nil/null hypotheses and coefficient tests right now, be they’ll be more flexible down the road\nKey is to start thinking about your implicit alternative models\nThe ultimate goal would be to create two models that represent two equally plausible theories\nTheory A is made up of components XYZ, whereas theory B has QRS components. You can then ask which theory (model) is better.\nA model embodies your hypothesis! It is the mathematical expression of your hypothesis!"
  },
  {
    "objectID": "4-Basics.html#regression-coefficient",
    "href": "4-Basics.html#regression-coefficient",
    "title": "GLM basics III",
    "section": "Regression coefficient",
    "text": "Regression coefficient\n\\[\\Large H_{0}: \\beta_{1}= 0\\] \\[\\Large H_{1}: \\beta_{1} \\neq 0\\]"
  },
  {
    "objectID": "4-Basics.html#what-does-the-regression-coefficient-test",
    "href": "4-Basics.html#what-does-the-regression-coefficient-test",
    "title": "GLM basics III",
    "section": "What does the regression coefficient test?",
    "text": "What does the regression coefficient test?\n\nDoes X provide any predictive information?\nDoes X provide any explanatory power regarding the variability of Y?\nIs the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)\nIs the regression line flat?\nAre X and Y correlated?"
  },
  {
    "objectID": "4-Basics.html#regression-coefficient-1",
    "href": "4-Basics.html#regression-coefficient-1",
    "title": "GLM basics III",
    "section": "Regression coefficient",
    "text": "Regression coefficient\n\\[\\Large se_{b} = \\frac{s_{Y}}{s_{X}}{\\sqrt{\\frac {1-r_{xy}^2}{n-2}}}\\] \\[\\Large t(n-2) = \\frac{b_{1}}{se_{b}}\\]"
  },
  {
    "objectID": "4-Basics.html#se_b",
    "href": "4-Basics.html#se_b",
    "title": "GLM basics III",
    "section": "SE_b",
    "text": "SE_b\n\nstandard errors for the slope coefficient\nrepresent our uncertainty (noise) in our estimate of the regression coefficient\ndifferent from residual standard error/deviation (but proportional to)\nmuch like previously we can take our estimate (b) and put confidence regions around it to get an estimate of what could be “possible” if we ran the study again"
  },
  {
    "objectID": "4-Basics.html#intercept",
    "href": "4-Basics.html#intercept",
    "title": "GLM basics III",
    "section": "Intercept",
    "text": "Intercept\n\nmore complex standard error calculation as the calculation depends on how far the X value (here zero) is away from the mean of X\n\nfarther from the mean, less information, thus more uncertainty"
  },
  {
    "objectID": "4-Basics.html#confidence-interval-for-coefficents",
    "href": "4-Basics.html#confidence-interval-for-coefficents",
    "title": "GLM basics III",
    "section": "Confidence interval for coefficents",
    "text": "Confidence interval for coefficents\n\nSame equation as we’ve been working with\nEstimate plus minus 1.96*se"
  },
  {
    "objectID": "4-Basics.html#confidence-bands-for-regression-line",
    "href": "4-Basics.html#confidence-bands-for-regression-line",
    "title": "GLM basics III",
    "section": "Confidence bands for regression line",
    "text": "Confidence bands for regression line\n\n\nCode\nset.seed(123)\npx.1 &lt;- rnorm(1000, 0, 1)\npe.1 &lt;- rnorm(1000, 0, 1)\npy.1 &lt;- .5 + .55 * px.1 + pe.1\npd.1 &lt;- data.frame(px.1,py.1)\npx.2 &lt;- rnorm(100, 0, 1)\npe.2 &lt;- rnorm(100, 0, 1)\npy.2 &lt;- .5 + .55 * px.2 + pe.2\npd.2 &lt;- data.frame(px.2,py.2)\np1 &lt;- ggplot(pd.1, aes(x = px.1,y =  py.1)) +\n    geom_point() +    \n    geom_smooth(method = lm) +\n   scale_x_continuous(limits = c(-3, 3))  +\n  scale_y_continuous(limits = c(-3, 3))\np2 &lt;- ggplot(pd.2, aes(x=px.2, y=py.2)) +\n    geom_point() +    \n    geom_smooth(method=lm) +\n  scale_x_continuous(limits = c(-3, 3))  +\n  scale_y_continuous(limits = c(-3, 3))\nlibrary(cowplot)\nplot_grid(p1, p2, ncol=2, labels = c(\"N = 1000\", \"N = 100\"))"
  },
  {
    "objectID": "4-Basics.html#confidence-bands",
    "href": "4-Basics.html#confidence-bands",
    "title": "GLM basics III",
    "section": "Confidence Bands",
    "text": "Confidence Bands\n\\[\\hat{Y}\\pm t_{critical} * se_{residual}*\\sqrt{\\frac {1}{n}+\\frac{(X-\\bar{X})^2}{(n-1)s_{X}^2}}\\]"
  },
  {
    "objectID": "4-Basics.html#prediction-band",
    "href": "4-Basics.html#prediction-band",
    "title": "GLM basics III",
    "section": "Prediction band",
    "text": "Prediction band\n\nWe are predicting and individual i’s score, not the \\(\\hat{Y}\\) for a particular level of \\(X\\). (A new \\(Y_i\\) given \\(X\\), rather than \\(\\bar{Y}\\) given \\(X\\) )\nBecause there is greater variation in predicting an individual value rather than a collection of individual values (i.e., the mean) the prediction band is greater\nCombines unknown variability in 1) the estimated mean (as reflected in se of b) 2) peoples scores around mean (residual standard error)\n\n\\[\\hat{Y}\\pm t_{critical} * se_{residual}*\\sqrt{1+ \\frac {1}{n}+\\frac{(X-\\bar{X})^2}{(n-1)s_{X}^2}}\\]"
  },
  {
    "objectID": "4-Basics.html#regression-equation",
    "href": "4-Basics.html#regression-equation",
    "title": "GLM basics III",
    "section": "Regression equation",
    "text": "Regression equation\n\\[\\large \\hat{Y} = b_0 + b_1X_1 + b_2X_2 + \\dots+b_kX_k\\]\n\nregression coefficients are “partial” regression coefficients\n\npredicted change in \\(Y\\) for a 1 unit change in \\(X\\), holding all other predictors constant\nsimilar to semi-partial correlation – represents part of each \\(X\\)"
  },
  {
    "objectID": "4-Basics.html#interpretting-multiple-regression-model",
    "href": "4-Basics.html#interpretting-multiple-regression-model",
    "title": "GLM basics III",
    "section": "Interpretting multiple regression model",
    "text": "Interpretting multiple regression model\n\\[\\large \\hat{Y} = b_0 + b_1X_1 + b_2X_2 + \\dots+b_kX_k\\]\n\nIntercept is the value of \\(Y\\) when all predictors = 0\nRegression coefficients are the predicted change in \\(Y\\) for a 1 unit change in \\(X\\), holding all other predictors constant"
  },
  {
    "objectID": "4-Basics.html#example-2",
    "href": "4-Basics.html#example-2",
    "title": "GLM basics III",
    "section": "Example",
    "text": "Example\n\n\nCode\nlibrary(here)\nstress.data = read.csv(here(\"stress.csv\"))\nlibrary(psych)\ndescribe(stress.data$Stress)\n\n\n   vars   n mean   sd median trimmed  mad  min   max range skew kurtosis   se\nX1    1 118 5.18 1.88   5.27    5.17 1.65 0.62 10.32  9.71 0.08     0.22 0.17"
  },
  {
    "objectID": "4-Basics.html#example-3",
    "href": "4-Basics.html#example-3",
    "title": "GLM basics III",
    "section": "Example",
    "text": "Example\n\n\nCode\nmr.model &lt;- lm(Stress ~ Support + Anxiety, data = stress.data)\ntidy(mr.model)\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.316    0.856     -0.369 7.13e- 1\n2 Support        0.406    0.0512     7.94  1.49e-12\n3 Anxiety        0.256    0.0674     3.80  2.34e- 4\n\n\n???\nIf a univariate regression is estimating the best-fit line, what is this estimating?"
  },
  {
    "objectID": "4-Basics.html#visualizing-multiple-regression",
    "href": "4-Basics.html#visualizing-multiple-regression",
    "title": "GLM basics III",
    "section": "Visualizing multiple regression",
    "text": "Visualizing multiple regression\n\n\nCode\nlibrary(visreg)\n\nvisreg2d(mr.model,\"Support\", \"Anxiety\", plot.type = \"persp\")"
  },
  {
    "objectID": "4-Basics.html#calculating-coefficients",
    "href": "4-Basics.html#calculating-coefficients",
    "title": "GLM basics III",
    "section": "Calculating coefficients",
    "text": "Calculating coefficients\nJust like with univariate regression, we calculate the OLS solution. As a reminder, this calculation will yield the estimate that reduces the sum of the squared deviations from the line:\n\\[\\large \\hat{Y} = b_0 + b_{1}X1 + b_{2}X_2\\] \\[\\large \\text{minimize} \\sum (Y-\\hat{Y})^2 \\]"
  },
  {
    "objectID": "4-Basics.html#calculating-the-standardized-partial-regression-coefficient",
    "href": "4-Basics.html#calculating-the-standardized-partial-regression-coefficient",
    "title": "GLM basics III",
    "section": "Calculating the standardized partial regression coefficient",
    "text": "Calculating the standardized partial regression coefficient\n\\[b_{1}^* = \\frac{r_{Y1}-r_{Y2}r_{12}}{1-r_{12}^2}\\]\n\\[b_{2}^* = \\frac{r_{Y2}-r_{Y1}r_{12}}{1-r_{12}^2}\\]"
  },
  {
    "objectID": "4-Basics.html#original-metric",
    "href": "4-Basics.html#original-metric",
    "title": "GLM basics III",
    "section": "Original Metric",
    "text": "Original Metric\n\\[b_{1} = b_{1}^*\\frac{s_{Y}}{s_{X1}}\\]\n\\[b_{1}^* = b_{1}\\frac{s_{X1}}{s_{Y}}\\]\n\\[b_{0} = \\bar{Y} - b_{1}\\bar{X_{1}} - b_{2}\\bar{X_{2}}\\]"
  },
  {
    "objectID": "4-Basics.html#controlling-for",
    "href": "4-Basics.html#controlling-for",
    "title": "GLM basics III",
    "section": "“Controlling for”",
    "text": "“Controlling for”\n\nTaken from @nickchk"
  },
  {
    "objectID": "4-Basics.html#estimating-model-fit",
    "href": "4-Basics.html#estimating-model-fit",
    "title": "GLM basics III",
    "section": "Estimating model fit",
    "text": "Estimating model fit\n\n\nCode\nmr.model &lt;- lm(Stress ~ Support + Anxiety, data = stress.data)\nsummary(mr.model)\n\n\n\nCall:\nlm(formula = Stress ~ Support + Anxiety, data = stress.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1958 -0.8994 -0.1370  0.9990  3.6995 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.31587    0.85596  -0.369 0.712792    \nSupport      0.40618    0.05115   7.941 1.49e-12 ***\nAnxiety      0.25609    0.06740   3.799 0.000234 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.519 on 115 degrees of freedom\nMultiple R-squared:  0.3556,    Adjusted R-squared:  0.3444 \nF-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11"
  },
  {
    "objectID": "4-Basics.html#multiple-correlation-r",
    "href": "4-Basics.html#multiple-correlation-r",
    "title": "GLM basics III",
    "section": "Multiple correlation, R",
    "text": "Multiple correlation, R\n\\[\\large \\hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2}\\]\n\n\\(\\hat{Y}\\) is a linear combination of Xs\n\\(r_{Y\\hat{Y}}\\) = multiple correlation = R\n\n–\n\\[\\large R = \\sqrt{b_{1}^*r_{Y1} + b_{2}^*r_{Y2}}\\] \\[\\large R^2 = {b_{1}^*r_{Y1} + b_{2}^*r_{Y2}}\\]"
  },
  {
    "objectID": "4-Basics.html#significance-tests",
    "href": "4-Basics.html#significance-tests",
    "title": "GLM basics III",
    "section": "significance tests",
    "text": "significance tests\n\n\\(R^2\\) (omnibus)\n\nRegression Coefficients\n\nIncrements to \\(R^2\\)"
  },
  {
    "objectID": "4-Basics.html#r-squared-r2",
    "href": "4-Basics.html#r-squared-r2",
    "title": "GLM basics III",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\nSame interpretation as before\nAdding predictors into your model will increase \\(R^2\\) – regardless of whether or not the predictor is significantly correlated with Y.\nAdjusted/Shrunken \\(R^2\\) takes into account the number of predictors in your model"
  },
  {
    "objectID": "4-Basics.html#adjusted-r-squared-textadj-r2",
    "href": "4-Basics.html#adjusted-r-squared-textadj-r2",
    "title": "GLM basics III",
    "section": "Adjusted R-squared, \\(\\text{Adj} R^2\\)",
    "text": "Adjusted R-squared, \\(\\text{Adj} R^2\\)\n\\[\\large R_{A}^2 = 1 - \\frac{Var_{res}}{Var_{total}}\\]\n\\[\\large R_{A}^2 = 1 - \\frac{\\frac{SS_{res}}{n-p-1}}{\\frac{SS_{total}}{n-1}}\\] \\[\\large R_{A}^2 = 1 - (1 -R^2)\\frac{n-1}{n-p-1}\\]"
  },
  {
    "objectID": "4-Basics.html#adjusted-r-squared-textadj-r2-1",
    "href": "4-Basics.html#adjusted-r-squared-textadj-r2-1",
    "title": "GLM basics III",
    "section": "Adjusted R-squared, \\(\\text{Adj} R^2\\)",
    "text": "Adjusted R-squared, \\(\\text{Adj} R^2\\)\n\\[\\large R_{A}^2 = 1 - (1 -R^2)\\frac{n-1}{n-p-1}\\] - What happens if you add many IV’s to your model that are uncorrelated with your DV?\n\nWhat happens as you add more covariates to your model that are highly correlated with your key predictor, X?\n\n\\[b_{1}^* = \\frac{r_{Y1}-r_{Y2}r_{12}}{1-r_{12}^2}\\]"
  },
  {
    "objectID": "4-Basics.html#anova",
    "href": "4-Basics.html#anova",
    "title": "GLM basics III",
    "section": "ANOVA",
    "text": "ANOVA\n\n\nCode\nsummary(mr.model)\n\n\n\nCall:\nlm(formula = Stress ~ Support + Anxiety, data = stress.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1958 -0.8994 -0.1370  0.9990  3.6995 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.31587    0.85596  -0.369 0.712792    \nSupport      0.40618    0.05115   7.941 1.49e-12 ***\nAnxiety      0.25609    0.06740   3.799 0.000234 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.519 on 115 degrees of freedom\nMultiple R-squared:  0.3556,    Adjusted R-squared:  0.3444 \nF-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11"
  },
  {
    "objectID": "4-Basics.html#anova-1",
    "href": "4-Basics.html#anova-1",
    "title": "GLM basics III",
    "section": "ANOVA",
    "text": "ANOVA\n\n\nCode\nanova(mr.model)\n\n\nAnalysis of Variance Table\n\nResponse: Stress\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nSupport     1 113.151 113.151  49.028 1.807e-10 ***\nAnxiety     1  33.314  33.314  14.435 0.0002336 ***\nResiduals 115 265.407   2.308                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#test-of-individual-regression-coefficients",
    "href": "4-Basics.html#test-of-individual-regression-coefficients",
    "title": "GLM basics III",
    "section": "Test of individual regression coefficients",
    "text": "Test of individual regression coefficients\n\\[\\Large H_{0}: \\beta_{X}= 0\\] \\[\\Large H_{1}: \\beta_{X} \\neq 0\\]"
  },
  {
    "objectID": "4-Basics.html#test-of-individual-regression-coefficients-1",
    "href": "4-Basics.html#test-of-individual-regression-coefficients-1",
    "title": "GLM basics III",
    "section": "Test of individual regression coefficients",
    "text": "Test of individual regression coefficients\nIn the case of univariate regression:\n\\[\\Large se_{b} = \\frac{s_{Y}}{s_{X}}{\\sqrt{\\frac {1-r_{xy}^2}{n-2}}}\\]\nIn the case of multiple regression:\n\\[\\Large se_{b} = \\frac{s_{Y}}{s_{X}}{\\sqrt{\\frac {1-R_{Y\\hat{Y}}^2}{n-p-1}}} \\sqrt{\\frac {1}{1-R_{i.jkl...p}^2}}\\]\n\nAs N increases…\nAs variance explained increases…"
  },
  {
    "objectID": "4-Basics.html#can-you",
    "href": "4-Basics.html#can-you",
    "title": "GLM basics III",
    "section": "Can you…",
    "text": "Can you…\n\nwrite out standardized and unstandardized regression equations?\ninterpret the coefficients of a multiple regression?\ndraw comparisons from ANOVA and regression?\ncalculate \\(R^2\\)?"
  },
  {
    "objectID": "4-Basics.html#warm-up",
    "href": "4-Basics.html#warm-up",
    "title": "GLM basics III",
    "section": "Warm Up",
    "text": "Warm Up\n\n\n\nCall:\nlm(formula = Weight ~ Age + Poverty, data = nhanes)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47.54 -19.90   0.82  16.96  65.53 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  58.8926     7.0353   8.371 8.69e-13 ***\nAge           0.3537     0.1310   2.699  0.00835 ** \nPoverty      -0.3501     1.5890  -0.220  0.82612    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.93 on 87 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.07949,   Adjusted R-squared:  0.05833 \nF-statistic: 3.756 on 2 and 87 DF,  p-value: 0.02724\n\n\n\nInterpret all coefficients\nInterpret all significance tests of coefficients\nIs it a good model?"
  },
  {
    "objectID": "4-Basics.html#which-variables-to-include",
    "href": "4-Basics.html#which-variables-to-include",
    "title": "GLM basics III",
    "section": "Which variables to include",
    "text": "Which variables to include\n\nYour goal should be to match the population model (theoretically)\nIncluding many variables will not bias parameter estimates but will potentially increase degrees of freedom and standard errors; in other words, putting too many variables in your model may make it more difficult to find a statistically significant result\nBut that’s only the case if you add variables unrelated to Y or X; there are some cases in which adding the wrong variables can lead to spurious results. [Stay tuned for the lecture on causal models.]"
  },
  {
    "objectID": "4-Basics.html#hierarchical-regression-model-comparison",
    "href": "4-Basics.html#hierarchical-regression-model-comparison",
    "title": "GLM basics III",
    "section": "Hierarchical regression / Model Comparison",
    "text": "Hierarchical regression / Model Comparison\nModel comparison: Comparing how well two (or more) models fit the data in order to determine which model is better.\nIf we’re comparing nested models by incrementally adding or subtracting variables, this is known as hierarchical regression.\n\nMultiple models are calculated\nEach predictor (or set of predictors) is assessed in terms of what it adds (in terms of variance explained) at the time it is entered\nOrder is dependent on an a priori hypothesis"
  },
  {
    "objectID": "4-Basics.html#r-square-change",
    "href": "4-Basics.html#r-square-change",
    "title": "GLM basics III",
    "section": "R-square change",
    "text": "R-square change\n\ndistributed as an F \\[F(p.new, N - 1 - p.all) = \\frac {R_{m.2}^2- R_{m.1}^2} {1-R_{m.2}^2} (\\frac {N-1-p.all}{p.new})\\]\ncan also be written in terms of SSresiduals"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-2",
    "href": "4-Basics.html#model-comparisons-2",
    "title": "GLM basics III",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nm.1 &lt;- lm(Stress ~ Support, data = stress.data)\nm.2 &lt;- lm(Stress ~ Support + Anxiety, data = stress.data)\nanova(m.1, m.2)\n\nAnalysis of Variance Table\n\nModel 1: Stress ~ Support\nModel 2: Stress ~ Support + Anxiety\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    116 298.72                                  \n2    115 265.41  1    33.314 14.435 0.0002336 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-3",
    "href": "4-Basics.html#model-comparisons-3",
    "title": "GLM basics III",
    "section": "model comparisons",
    "text": "model comparisons\n\nanova(m.1)\n\nAnalysis of Variance Table\n\nResponse: Stress\n           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nSupport     1 113.15 113.151  43.939 1.12e-09 ***\nResiduals 116 298.72   2.575                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-4",
    "href": "4-Basics.html#model-comparisons-4",
    "title": "GLM basics III",
    "section": "model comparisons",
    "text": "model comparisons\n\nanova(m.2)\n\nAnalysis of Variance Table\n\nResponse: Stress\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nSupport     1 113.151 113.151  49.028 1.807e-10 ***\nAnxiety     1  33.314  33.314  14.435 0.0002336 ***\nResiduals 115 265.407   2.308                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-5",
    "href": "4-Basics.html#model-comparisons-5",
    "title": "GLM basics III",
    "section": "model comparisons",
    "text": "model comparisons\n\n\nCode\nsummary(m.2)\n\n\n\nCall:\nlm(formula = Stress ~ Support + Anxiety, data = stress.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1958 -0.8994 -0.1370  0.9990  3.6995 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.31587    0.85596  -0.369 0.712792    \nSupport      0.40618    0.05115   7.941 1.49e-12 ***\nAnxiety      0.25609    0.06740   3.799 0.000234 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.519 on 115 degrees of freedom\nMultiple R-squared:  0.3556,    Adjusted R-squared:  0.3444 \nF-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-6",
    "href": "4-Basics.html#model-comparisons-6",
    "title": "GLM basics III",
    "section": "model comparisons",
    "text": "model comparisons\n\n\n\nCall:\nlm(formula = Stress ~ Support, data = stress.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8215 -1.2145 -0.1796  1.0806  3.4326 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.56046    0.42189   6.069 1.66e-08 ***\nSupport      0.30006    0.04527   6.629 1.12e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.605 on 116 degrees of freedom\nMultiple R-squared:  0.2747,    Adjusted R-squared:  0.2685 \nF-statistic: 43.94 on 1 and 116 DF,  p-value: 1.12e-09"
  }
]