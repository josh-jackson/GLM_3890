[
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "Untitled",
    "section": "",
    "text": "General(ized) Linear Model\nA workhorse that is responsible for &gt;99% of statistical tests in psychology, as well as the building block of many machine learning models"
  },
  {
    "objectID": "1-intro.html#glm",
    "href": "1-intro.html#glm",
    "title": "What are models?",
    "section": "GLM",
    "text": "GLM\n\nGeneral(ized) Linear Model\nA workhorse that is responsible for &gt;99% of statistical tests in psychology, as well as the building block of many machine learning models"
  },
  {
    "objectID": "1-intro.html#what-do-we-mean-by-generalized",
    "href": "1-intro.html#what-do-we-mean-by-generalized",
    "title": "What are models?",
    "section": "What do we mean by General(ized)?",
    "text": "What do we mean by General(ized)?\n\nIt is general in that it refers to a broad set of similar models that can applied to almost any context"
  },
  {
    "objectID": "1-intro.html#what-do-we-mean-by-linear",
    "href": "1-intro.html#what-do-we-mean-by-linear",
    "title": "What are models?",
    "section": "What do we mean by linear?",
    "text": "What do we mean by linear?\n\nWe try to understand our dependent variable (DV) via a linear combination predictor variables.\nA linear combination a way of combining things (variables) using scalar multiplication and addition"
  },
  {
    "objectID": "1-intro.html#what-is-a-model",
    "href": "1-intro.html#what-is-a-model",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?"
  },
  {
    "objectID": "1-intro.html#what-is-a-model-1",
    "href": "1-intro.html#what-is-a-model-1",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?\n\na representation of the world\na statistical model uses math to make predictions about the world"
  },
  {
    "objectID": "1-intro.html#middle-school-math",
    "href": "1-intro.html#middle-school-math",
    "title": "What are models?",
    "section": "Middle School Math",
    "text": "Middle School Math\n\\[ y = mx + b \\] - what is \\(y\\)?\n\nwhat is \\(m\\)?\nwhat is \\(x\\)?\nwhat is \\(b\\)?"
  },
  {
    "objectID": "1-intro.html#lets-rewrite-this",
    "href": "1-intro.html#lets-rewrite-this",
    "title": "What are models?",
    "section": "Let’s rewrite this",
    "text": "Let’s rewrite this\n\\[y = b_0 + b_{1}X\\]\n\nwhat is \\(y\\)?\nwhat is \\(b_0\\)?\nwhat is \\(b_1\\)?\nwhat is \\(X\\)?"
  },
  {
    "objectID": "1-intro.html#are-models-always-right",
    "href": "1-intro.html#are-models-always-right",
    "title": "What are models?",
    "section": "Are models always right?",
    "text": "Are models always right?"
  },
  {
    "objectID": "1-intro.html#models-are-flawed",
    "href": "1-intro.html#models-are-flawed",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?"
  },
  {
    "objectID": "1-intro.html#models-are-flawed-1",
    "href": "1-intro.html#models-are-flawed-1",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "1-intro.html#models",
    "href": "1-intro.html#models",
    "title": "What are models?",
    "section": "Models",
    "text": "Models\n\nWhat are the goals of modeling?\nWhat do you need in order to develop a model?"
  },
  {
    "objectID": "1-intro.html#how-do-we-know-if-a-model-is-good-what-makes-it-good",
    "href": "1-intro.html#how-do-we-know-if-a-model-is-good-what-makes-it-good",
    "title": "What are models?",
    "section": "How do we know if a model is good? What makes it good?",
    "text": "How do we know if a model is good? What makes it good?"
  },
  {
    "objectID": "1-intro.html#how-will-we-use-models",
    "href": "1-intro.html#how-will-we-use-models",
    "title": "What are models?",
    "section": "How will we use models?",
    "text": "How will we use models?\n\nThis semester, we will mainly focus on classic statistical tests\nEvery single one of these is a model\nWe will also focus on developing your intuition\nWhen you face new models, come back to these basics"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "What are models?\nGLM basics\nGLM basics - 2\nGLM basics - 3\nInteractions\nInteractions - 2\nMLM\nSEM\nlogistic\nBayes\nMachine learning 1\nMachine learning 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Psychological Statistics",
    "section": "",
    "text": "4/8: Class is canceled so as to enjoy the eclipse!\n2/12: HW #4 is posted. Please turn it into canvas by 2/19 at 9:59am\n2/5: HW #3 is posted. Please turn it into canvas by 2/12 at 9:59am\n1/31: Homework #2 is now posted. Please turn it in to canvas by 2/5 at 9:59am\n1/24: Homework #1 is now posted. Please turn it in to canvas by 1/29 at 9:59am\n1/17: Welcome to the first day of class! If you want to follow along with examples, head over to github for all of the code and data. https://github.com/josh-jackson/GLM_3890"
  },
  {
    "objectID": "1-intro.html#how-do-we-know-if-a-model-is-good",
    "href": "1-intro.html#how-do-we-know-if-a-model-is-good",
    "title": "What are models?",
    "section": "How do we know if a model is good?",
    "text": "How do we know if a model is good?\n\nWhat makes it good?"
  },
  {
    "objectID": "2-Basics.html#thinking-in-terms-of-models",
    "href": "2-Basics.html#thinking-in-terms-of-models",
    "title": "GLM basics",
    "section": "",
    "text": "Our DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (here forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "2-Basics.html#how-can-we-visualize-data-to-make-sense-of-it",
    "href": "2-Basics.html#how-can-we-visualize-data-to-make-sense-of-it",
    "title": "GLM basics",
    "section": "How can we visualize data to make sense of it?",
    "text": "How can we visualize data to make sense of it?\n\n\nCode\nlibrary(broom)\nset.seed(123)\nx.1 &lt;- rnorm(100, 0, 1)\ne.1 &lt;- rnorm(100, 0, 2)\ny.1 &lt;- .5 + .55 * x.1 + e.1\nd.1 &lt;- data.frame(x.1,y.1)\nm.1 &lt;- lm(y.1 ~ x.1, data = d.1)\nd1.f&lt;- augment(m.1)\nd.1\n\n\n             x.1         y.1\n1   -0.560475647 -1.22907473\n2   -0.230177489  0.88716980\n3    1.558708314  0.86390582\n4    0.070508391 -0.15630558\n5    0.129287735 -1.33212888\n6    1.715064987  1.35323029\n7    0.460916206 -0.81630503\n8   -1.265061235 -3.53166755\n9   -0.686852852 -0.63822211\n10  -0.445661970  2.09287913\n11   1.224081797  0.02255106\n12   0.359813827  1.91382625\n13   0.400771451 -2.51534112\n14   0.110682716  0.44975156\n15  -0.555841135  1.23310178\n16   1.786913137  2.08510895\n17   0.497850478  0.98517015\n18  -1.966617157 -1.86305145\n19   0.701355902 -0.81366295\n20  -0.472791408 -1.80829286\n21  -1.067823706  0.14799016\n22  -0.217974915 -1.51483543\n23  -1.026004448 -1.04541733\n24  -0.728891229 -0.41307456\n25  -0.625039268  3.84395241\n26  -1.686693311 -1.73158112\n27   0.837787044  1.43155602\n28   0.153373118  0.74027691\n29  -1.138136937 -2.04968858\n30   1.253814921  1.04698203\n31   0.426464221  3.62365704\n32  -0.295071483  1.24071879\n33   0.895125661  1.07478496\n34   0.878133488  0.13797975\n35   0.821581082 -3.15462485\n36   0.688640254  3.14142657\n37   0.553917654 -2.11662543\n38  -0.061911711  1.94584358\n39  -0.305962664  4.14992767\n40  -0.380471001 -2.59704537\n41  -0.694706979  1.52147983\n42  -0.207917278 -0.13874948\n43  -1.265396352 -3.34025631\n44   2.168955965 -1.33640953\n45   1.207961998 -2.03869325\n46  -1.123108583 -1.17952277\n47  -0.402884835 -2.64509783\n48  -0.466655354  1.61917310\n49   0.779965118  5.12919870\n50  -0.083369066 -2.11991394\n51   0.253318514  2.21480288\n52  -0.028546755  2.02238377\n53  -0.042870457  1.14082641\n54   1.368602284 -0.76402196\n55  -0.225770986  0.13692074\n56   1.516470604  0.77326816\n57  -1.548752804  0.77416502\n58   0.584613750  0.07666005\n59   0.123854244  2.52206661\n60   0.215941569 -0.13039385\n61   0.379639483  2.81422465\n62  -0.502323453 -1.87463191\n63  -0.333207384 -2.20357455\n64  -1.018575383  6.42186341\n65  -1.071791226 -0.92320035\n66   0.303528641  1.26339594\n67   0.448209779  2.01965473\n68   0.053004227 -0.43840893\n69   0.922267468  2.04097120\n70   2.050084686  2.36547563\n71  -0.491031166 -0.20082816\n72  -2.309168876 -0.63945681\n73   1.005738524  0.98502168\n74  -0.709200763  4.36684338\n75  -0.688008616 -1.36107693\n76   1.025571370 -1.12792828\n77  -0.284773007  0.41895164\n78  -1.220717712  0.44956676\n79   0.181303480  1.47276387\n80  -0.138891362 -0.49312091\n81   0.005764186 -1.62348197\n82   0.385280401  3.23827457\n83  -0.370660032 -0.40316379\n84   0.644376549 -0.87661862\n85  -0.220486562 -0.09382675\n86   0.331781964  0.28812829\n87   1.096839013  3.32310204\n88   0.435181491  0.90882440\n89  -0.325931586  1.82884520\n90   1.148807618  0.13326016\n91   0.993503856  1.47531774\n92   0.548396960  0.15224650\n93   0.238731735  0.82046951\n94  -0.627906076 -1.63607506\n95   1.360652449 -1.37324422\n96  -0.600259587  4.16428400\n97   2.187332993  2.90445079\n98   1.532610626 -1.15960688\n99  -0.235700359 -0.85196703\n100 -1.026420900 -2.43549166\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2)\n\n\n\n\n\n\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE) \n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "2-Basics.html#how-do-we-visualize-categorical-data",
    "href": "2-Basics.html#how-do-we-visualize-categorical-data",
    "title": "GLM basics",
    "section": "How do we visualize categorical data?",
    "text": "How do we visualize categorical data?\nNominal/categorical data does not have any inherent numbers associated with it. Think control/tx, eye color, etc.\n\n\nCode\nset.seed(123)\ngroup &lt;- c(0, 1)\nx.2 &lt;- rep(group, times = 50)\ne.1 &lt;- rnorm(100, 0, 1)\ny.1 &lt;- .5 + .85 * x.2 + e.1\nd.2 &lt;- data.frame(x.2,y.1)\nm.2 &lt;- lm(y.1 ~ x.2, data = d.2)\nd2.f&lt;- augment(m.2)\nd.2\n\n\n    x.2          y.1\n1     0 -0.060475647\n2     1  1.119822511\n3     0  2.058708314\n4     1  1.420508391\n5     0  0.629287735\n6     1  3.065064987\n7     0  0.960916206\n8     1  0.084938765\n9     0 -0.186852852\n10    1  0.904338030\n11    0  1.724081797\n12    1  1.709813827\n13    0  0.900771451\n14    1  1.460682716\n15    0 -0.055841135\n16    1  3.136913137\n17    0  0.997850478\n18    1 -0.616617157\n19    0  1.201355902\n20    1  0.877208592\n21    0 -0.567823706\n22    1  1.132025085\n23    0 -0.526004448\n24    1  0.621108771\n25    0 -0.125039268\n26    1 -0.336693311\n27    0  1.337787044\n28    1  1.503373118\n29    0 -0.638136937\n30    1  2.603814921\n31    0  0.926464221\n32    1  1.054928517\n33    0  1.395125661\n34    1  2.228133488\n35    0  1.321581082\n36    1  2.038640254\n37    0  1.053917654\n38    1  1.288088289\n39    0  0.194037336\n40    1  0.969528999\n41    0 -0.194706979\n42    1  1.142082722\n43    0 -0.765396352\n44    1  3.518955965\n45    0  1.707961998\n46    1  0.226891417\n47    0  0.097115165\n48    1  0.883344646\n49    0  1.279965118\n50    1  1.266630934\n51    0  0.753318514\n52    1  1.321453245\n53    0  0.457129543\n54    1  2.718602284\n55    0  0.274229014\n56    1  2.866470604\n57    0 -1.048752804\n58    1  1.934613750\n59    0  0.623854244\n60    1  1.565941569\n61    0  0.879639483\n62    1  0.847676547\n63    0  0.166792616\n64    1  0.331424617\n65    0 -0.571791226\n66    1  1.653528641\n67    0  0.948209779\n68    1  1.403004227\n69    0  1.422267468\n70    1  3.400084686\n71    0  0.008968834\n72    1 -0.959168876\n73    0  1.505738524\n74    1  0.640799237\n75    0 -0.188008616\n76    1  2.375571370\n77    0  0.215226993\n78    1  0.129282288\n79    0  0.681303480\n80    1  1.211108638\n81    0  0.505764186\n82    1  1.735280401\n83    0  0.129339968\n84    1  1.994376549\n85    0  0.279513438\n86    1  1.681781964\n87    0  1.596839013\n88    1  1.785181491\n89    0  0.174068414\n90    1  2.498807618\n91    0  1.493503856\n92    1  1.898396960\n93    0  0.738731735\n94    1  0.722093924\n95    0  1.860652449\n96    1  0.749740413\n97    0  2.687332993\n98    1  2.882610626\n99    0  0.264299641\n100   1  0.323579100\n\n\n\n\n\nCode\nggplot(d2.f , aes(x=x.2, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE) \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCode\nggplot(d2.f,\n       aes(x = as.factor(x.2),\n           y = y.1)) +\n  geom_violin(aes(fill = as.factor(x.2)),\n              alpha = .3,\n              show.legend = FALSE) +\n  geom_boxplot(aes(color = as.factor(x.2)),\n               fill = \"white\",\n               width = .2) +\n  geom_jitter(aes(color = as.factor(x.2))) +\n  labs(x = \"Treatment\",\n       y = \"y\")"
  },
  {
    "objectID": "2-Basics.html#what-do-these-visualizations-have-in-common",
    "href": "2-Basics.html#what-do-these-visualizations-have-in-common",
    "title": "GLM basics",
    "section": "What do these visualizations have in common?",
    "text": "What do these visualizations have in common?\n. . .\nLINES!\nMost of what we are going to do is represent the relationship between variables with lines (or planes or hyperplanes once we get into 2 or more variables)"
  },
  {
    "objectID": "2-Basics.html#thinking-in-terms-of-models-1",
    "href": "2-Basics.html#thinking-in-terms-of-models-1",
    "title": "GLM basics",
    "section": "Thinking in terms of models",
    "text": "Thinking in terms of models\n\nModels help us draw the lines\nOur DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (hence forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)\n\n\\(b_{1}\\) describes the strength of association i.e. the line!"
  },
  {
    "objectID": "2-Basics.html#see-this-in-our-r-code",
    "href": "2-Basics.html#see-this-in-our-r-code",
    "title": "GLM basics",
    "section": "See this in our R code",
    "text": "See this in our R code\nIndependent samples t-test\n\n\nCode\nt.1 &lt;- t.test(y.1 ~ x.2, data = d.2) \nt.1\n\n\n\n    Welch Two Sample t-test\n\ndata:  y.1 by x.2\nt = -4.4144, df = 93.846, p-value = 2.706e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1.1740658 -0.4455722\nsample estimates:\nmean in group 0 mean in group 1 \n      0.6104964       1.4203154 \n\n\n\nOne-way ANOVA\n\n\nCode\na.1 &lt;- aov(y.1 ~ x.2, data=d.2)\nsummary(a.1)\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx.2          1  16.40  16.395   19.49 2.61e-05 ***\nResiduals   98  82.45   0.841                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nregression model\n\n\nCode\nlm.1 &lt;- lm(y.1 ~ x.2, data=d.2)\nsummary(lm.1)\n\n\n\nCall:\nlm(formula = y.1 ~ x.2, data = d.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.37948 -0.57986 -0.00856  0.59773  2.09864 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6105     0.1297   4.706 8.29e-06 ***\nx.2           0.8098     0.1834   4.414 2.61e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9172 on 98 degrees of freedom\nMultiple R-squared:  0.1659,    Adjusted R-squared:  0.1574 \nF-statistic: 19.49 on 1 and 98 DF,  p-value: 2.607e-05"
  },
  {
    "objectID": "2-Basics.html#comparison-across-these-three-models",
    "href": "2-Basics.html#comparison-across-these-three-models",
    "title": "GLM basics",
    "section": "Comparison across these three models",
    "text": "Comparison across these three models\n\nNote that each of these three models (t, anova, regression) were exactly the same in terms of the mode: Y ~ X\nThat is because they are the same model! Different terms referring to the same thing is one of the major stumbling blocks of stats.\nYet they gave us different information. Depending on what you are interested in some information may be more pertinent.\nWe will focus on the regression model (glm) as it is most flexible"
  },
  {
    "objectID": "2-Basics.html#general-linear-model-glm",
    "href": "2-Basics.html#general-linear-model-glm",
    "title": "GLM basics",
    "section": "General linear model (GLM)",
    "text": "General linear model (GLM)\n\nThis model (equation) can be very simple as in a treatment/control experiment\nIt can be very complex in terms of trying to understand something like academic achievement\nThe majority of our models fall under the umbrella of a general(ized) linear model (often referred to as regression models)\nModels imply our theory about how the data are generated (ie how the world works)"
  },
  {
    "objectID": "2-Basics.html#regression-equation",
    "href": "2-Basics.html#regression-equation",
    "title": "GLM basics",
    "section": "Regression Equation",
    "text": "Regression Equation\n\\[Y_i = b_{0} + b_{1}X_i +e_i\\]\n\n\\(Y_i \\sim Normal(\\mu, \\sigma)\\)\nThe DV, \\(Y\\) for each person \\(i\\) is distributed normaly, with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\)"
  },
  {
    "objectID": "2-Basics.html#regression-terms",
    "href": "2-Basics.html#regression-terms",
    "title": "GLM basics",
    "section": "Regression terms",
    "text": "Regression terms\n\nY / DV / Outcome / Response / Criterion\nX / IV / Predictor / Explanatory variable\nRegression coefficient (weight) / b / b* / \\(\\beta\\)\nIntercept \\(b_0\\) / \\(\\beta_{0}\\)\nError / Residuals \\(e\\)\nPredictions \\(\\hat{Y}\\)"
  },
  {
    "objectID": "2-Basics.html#regression-models",
    "href": "2-Basics.html#regression-models",
    "title": "GLM basics",
    "section": "Regression models",
    "text": "Regression models\n\nThese models are a way to convey the relationship between two (or more) variables. They translate our hypotheses into math.\nWe can use these models to get information we may be interested in (e.g. means, SEs) and test hypotheses about the relationship among variables\n“All models are wrong but some are useful (and some are better than others)” - George Box"
  },
  {
    "objectID": "2-Basics.html#another-example",
    "href": "2-Basics.html#another-example",
    "title": "GLM basics",
    "section": "Another example",
    "text": "Another example\n\n\nCode\nlibrary(tidyverse)\nlibrary(readr)\nexample.data &lt;- read_csv(\"exampleData.csv\")\n\n\nRows: 280 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): id, tx, traffic.risk\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nexample.data &lt;- na.omit(example.data)\nexample.data\n\n\n# A tibble: 270 × 3\n      id    tx traffic.risk\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1         1.86\n 2     2     1         1   \n 3     3     1         3.29\n 4     4     1         2   \n 5     5     1         2.43\n 6     6     1         3.29\n 7     7     0         1.17\n 8     8     0         2.43\n 9     9     0         3   \n10    10     1         1.71\n# ℹ 260 more rows\n\n\n\n\n\nCode\nggplot(example.data,\n       aes(x = as.factor(tx),\n           y = traffic.risk)) +\n  geom_violin(aes(fill = as.factor(tx)),\n              alpha = .3,\n              show.legend = FALSE) +\n  geom_boxplot(aes(color = as.factor(tx)),\n               fill = \"white\",\n               width = .2) +\n  geom_jitter(aes(color = as.factor(tx))) +\n  labs(x = \"Treatment\",\n       y = \"Traffic Risk\")\n\n\n\n\n\n\n\n\nCode\nt.test(traffic.risk ~ tx, data = example.data, \n              var.equal = TRUE) \n\n\n\n    Two Sample t-test\n\ndata:  traffic.risk by tx\nt = 4.9394, df = 268, p-value = 1.381e-06\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.2893360 0.6728755\nsample estimates:\nmean in group 0 mean in group 1 \n       2.650641        2.169535 \n\n\n\n\n\nCode\na.1 &lt;- aov(traffic.risk ~ tx, data = example.data) \nsummary(a.1)\n\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntx            1   14.8  14.800    24.4 1.38e-06 ***\nResiduals   268  162.6   0.607                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\nsummary(mod.1)\n\n\n\nCall:\nlm(formula = traffic.risk ~ tx, data = example.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.65064 -0.59811 -0.02668  0.54475  2.54475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65064    0.07637  34.707  &lt; 2e-16 ***\ntx          -0.48111    0.09740  -4.939 1.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7789 on 268 degrees of freedom\nMultiple R-squared:  0.08344,   Adjusted R-squared:  0.08002 \nF-statistic:  24.4 on 1 and 268 DF,  p-value: 1.381e-06\n\n\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\nanova(mod.1)\n\n\nAnalysis of Variance Table\n\nResponse: traffic.risk\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ntx          1  14.80 14.7999  24.398 1.381e-06 ***\nResiduals 268 162.57  0.6066                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2-Basics.html#example-summary",
    "href": "2-Basics.html#example-summary",
    "title": "GLM basics",
    "section": "Example summary",
    "text": "Example summary\n\nSame p-values for each test; same SS; same test!\nt-test is a special form of a linear model\nanova is a special form of a linear model\nBecause the anova and t-test are narrower models, we will be working with the general linear model"
  },
  {
    "objectID": "2-Basics.html#parts-of-the-model",
    "href": "2-Basics.html#parts-of-the-model",
    "title": "GLM basics",
    "section": "Parts of the model",
    "text": "Parts of the model\n\\[Y_i = b_{0} + b_{1}X_i + e_i\\] \\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\nEach individual has a unique Y value an X value and a residual/error term\n\nThe model only has a single \\(b_{0}\\) and \\(b_{1}\\) term. These are the regression parameters. \\(b_{0}\\) is the intercept and \\(b_{1}\\) quantifies the relationship between your model of the world and the DV."
  },
  {
    "objectID": "2-Basics.html#what-do-the-estimates-tell-us",
    "href": "2-Basics.html#what-do-the-estimates-tell-us",
    "title": "GLM basics",
    "section": "What do the estimates tell us?",
    "text": "What do the estimates tell us?\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\nlibrary(broom)\ntidy(mod.1)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6\n\n\n\n\n\nCode\nexample.data %&gt;% \n  group_by(tx) %&gt;% \n  summarise(mean(traffic.risk))\n\n\n# A tibble: 2 × 2\n     tx `mean(traffic.risk)`\n  &lt;dbl&gt;                &lt;dbl&gt;\n1     0                 2.65\n2     1                 2.17"
  },
  {
    "objectID": "2-Basics.html#how-to-interpret-regression-estimates",
    "href": "2-Basics.html#how-to-interpret-regression-estimates",
    "title": "GLM basics",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nIntercept is the mean of group of variable tx that is coded 0\nRegression coefficient is the slope or rise over run, scaled as a 1 unit on the x axis\n“For a one unit change in X, there is a b1 predicted change in Y.”\nRegression coefficient is the difference in means between the groups, given that we coded our groups as 0 and 1.\n\n\n\n\nCode\nlibrary(ggstatsplot)\n\n\nYou can cite this package as:\n     Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n     Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\n\nCode\nggstatsplot::ggbetweenstats(\n  data = example.data,\n  x = tx,\n  y = traffic.risk,\n  ylab = \"Traffic Risk Score\", # label for the y-axis variable\n  xlab = \"Treatment group\", # label for the x-axis variable\n  bf.message = FALSE, \n  messages = FALSE\n) \n\n\n\n\n\n\nNote that the same interpretation for a regression line holds: for a 1 unit change in X (tx) there is a predicted b change in Y (traffic risk)\n\n\nCode\nlibrary(ggplot2)\nggplot(example.data, aes(x=tx, y=traffic.risk)) +\n    geom_point() +    \n    geom_smooth(method=lm,   # Add linear regression line\n                se=FALSE)    # Don't add shaded confidence region\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "2-Basics.html#how-to-interpret-regression-estimates-1",
    "href": "2-Basics.html#how-to-interpret-regression-estimates-1",
    "title": "GLM basics",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nThe entire class will go over different ways to interpret these estimates/parameters/coefficients\nIntercept (b0) signifies the level of Y when your model IVs (Xs) are zero\nRegression (b1) signifies the difference for a one unit change in your X"
  },
  {
    "objectID": "2-Basics.html#standard-errors",
    "href": "2-Basics.html#standard-errors",
    "title": "GLM basics",
    "section": "Standard errors",
    "text": "Standard errors\n\nThese coefficients are “best guesses” at some population parameter we want to make inferences about.\nTo do so we must balance our signal to our noise. If we have a strong signal (steep regression line/difference between groups) that would imply the groups differ.\nIf there was a lot of noise in that assessment then a big difference between groups may not be meaningful. We assess this “noise” component with our standard errors"
  },
  {
    "objectID": "2-Basics.html#sampling-distribution-refresher",
    "href": "2-Basics.html#sampling-distribution-refresher",
    "title": "GLM basics",
    "section": "Sampling distribution refresher",
    "text": "Sampling distribution refresher\n\nWe collect a sample and calculate a sample statistic \\(b_1\\)\nThis statistic is not a perfect assessment of the population\nWe calculate a sampling distribution to represent all possible samples we could have gotten from the same population with the same sample size\nThe standard deviation of the sampling distribution (standard error) reflects the spread of the hypothetical scores we could have gotten\nA large standard error means we have a flat sampling distribution and thus should not trust the estimate.\nPer convention, if our estimate is &gt; 2xSE away from 0, we say our estimate is “significantly different from zero”"
  },
  {
    "objectID": "2-Basics.html#predicted-scores",
    "href": "2-Basics.html#predicted-scores",
    "title": "GLM basics",
    "section": "Predicted scores",
    "text": "Predicted scores\n\nBased on the output how do I calculate means for each group?\n\n\ntidy(mod.1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#anova-as-regression",
    "href": "2-Basics.html#anova-as-regression",
    "title": "GLM basics",
    "section": "ANOVA as regression",
    "text": "ANOVA as regression\n\n“For a one unit change in X, there is a b1 predicted change in Y” will always be true.\nNominal/categorical variables do not have any inherent numbers associated with them so we need to assign them numbers\nWhat numbers you assign will impact the equation/estimates/hypothesis you can test\n\nBehoove you to code them as useful numbers. O and 1 are useful and are the default in R."
  },
  {
    "objectID": "2-Basics.html#anova-as-regression-1",
    "href": "2-Basics.html#anova-as-regression-1",
    "title": "GLM basics",
    "section": "ANOVA as regression",
    "text": "ANOVA as regression\n\\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\n\nCode\nexample.data\n\n\n# A tibble: 270 × 3\n      id    tx traffic.risk\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1         1.86\n 2     2     1         1   \n 3     3     1         3.29\n 4     4     1         2   \n 5     5     1         2.43\n 6     6     1         3.29\n 7     7     0         1.17\n 8     8     0         2.43\n 9     9     0         3   \n10    10     1         1.71\n# ℹ 260 more rows\n\n\n\n\n\nCode\nlibrary(dplyr)\nexample.data$tx.r &lt;- as.factor(example.data$tx)\nexample.data$tx.r &lt;- recode_factor(example.data$tx.r, \"0\" = \"control\", \"1\" = \"treatment\") \n\n\nCreate a new variable that is not numeric\n\n\nCode\nexample.data\n\n\n# A tibble: 270 × 4\n      id    tx traffic.risk tx.r     \n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;    \n 1     1     1         1.86 treatment\n 2     2     1         1    treatment\n 3     3     1         3.29 treatment\n 4     4     1         2    treatment\n 5     5     1         2.43 treatment\n 6     6     1         3.29 treatment\n 7     7     0         1.17 control  \n 8     8     0         2.43 control  \n 9     9     0         3    control  \n10    10     1         1.71 treatment\n# ℹ 260 more rows\n\n\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\ntidy(mod.1)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6\n\n\n\n\nCode\nmod.1r &lt;- lm(traffic.risk ~ tx.r, data = example.data)\ntidy(mod.1r)\n\n\n# A tibble: 2 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      2.65     0.0764     34.7  3.80e-101\n2 tx.rtreatment   -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#what-if-we-changed-0-and-1-to-other-values",
    "href": "2-Basics.html#what-if-we-changed-0-and-1-to-other-values",
    "title": "GLM basics",
    "section": "What if we changed 0 and 1 to other values?",
    "text": "What if we changed 0 and 1 to other values?\n\nInfinite number of ways to code categorical/nominal variables, only a few meaningful ways\n\nThe R default is called “dummy coding”\n\nUses 0s and 1s to put numbers to categories. We will soon see what this looks like when you have more than 2 groups.\nChanging the numbers changes…?"
  },
  {
    "objectID": "2-Basics.html#effect-coding",
    "href": "2-Basics.html#effect-coding",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\n\n\nCode\nexample.data$tx.effect &lt;- dplyr::recode(example.data$tx, '0' = -1, '1' = 1) \n\n\n\n\nCode\nexample.data\n\n\n# A tibble: 270 × 5\n      id    tx traffic.risk tx.r      tx.effect\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1     1     1         1.86 treatment         1\n 2     2     1         1    treatment         1\n 3     3     1         3.29 treatment         1\n 4     4     1         2    treatment         1\n 5     5     1         2.43 treatment         1\n 6     6     1         3.29 treatment         1\n 7     7     0         1.17 control          -1\n 8     8     0         2.43 control          -1\n 9     9     0         3    control          -1\n10    10     1         1.71 treatment         1\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#effect-coding-1",
    "href": "2-Basics.html#effect-coding-1",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\n\n\nCode\nmod.1.eff &lt;- lm(traffic.risk ~ tx.effect, data = example.data)\ntidy(mod.1.eff)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.41     0.0487     49.5  8.15e-137\n2 tx.effect     -0.241    0.0487     -4.94 1.38e-  6\n\n\n\nsystematically changes both the intercept and the regression estimate\n\n\n\n\nCode\neffect &lt;- tidy(mod.1.eff)\neffect\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.41     0.0487     49.5  8.15e-137\n2 tx.effect     -0.241    0.0487     -4.94 1.38e-  6\n\n\n\n\nCode\ndummy &lt;- tidy(mod.1)\ndummy\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6\n\n\n\nIntercept: value when your predictor X is zero\nRegression coefficient: one unit increase in X is associated with a (regression estimate) predicted increase in Y"
  },
  {
    "objectID": "2-Basics.html#effect-coding-2",
    "href": "2-Basics.html#effect-coding-2",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\nConsists of -1, 1s (And zeros for more than 2 groups)\n\nThe intercept is the “grand mean” or “mean of means” if unbalanced\nThe regression coefficient represents the group “effect” ie the difference between the grand mean and the group labeled 1 (we will revisit this when we have more than 2 groups as it will make more sense)\n\n\nCommon to use for Factorial ANOVA designs"
  },
  {
    "objectID": "2-Basics.html#dummy-coding",
    "href": "2-Basics.html#dummy-coding",
    "title": "GLM basics",
    "section": "Dummy coding",
    "text": "Dummy coding\n\nMore appropriate when you are interested in comparing to a specific group rather than an “average person.”\nIntercept: value of the group coded zero\nRegression coefficient: mean difference between groups"
  },
  {
    "objectID": "2-Basics.html#contrast-coding",
    "href": "2-Basics.html#contrast-coding",
    "title": "GLM basics",
    "section": "Contrast coding",
    "text": "Contrast coding\n\nAs our models get more complex our coding schemes can too\nWhat happens if you code the groups -.5 and .5?\nThese make more sense when we have more groups. More groups require more independent variables, however."
  },
  {
    "objectID": "2-Basics.html#categorical-coding-summary",
    "href": "2-Basics.html#categorical-coding-summary",
    "title": "GLM basics",
    "section": "categorical coding summary",
    "text": "categorical coding summary\n\nIn the end, it really doesn’t matter how you code your model. The overall “fit” of the model will be exactly the same because it is the same model.\nThe only thing that changes is the interpretation of your coefficients.\nEven then, you can recreate any test you want regardless of coding scheme. As a result, we often leave the default coding in place."
  },
  {
    "objectID": "2-Basics.html#statistical-inference",
    "href": "2-Basics.html#statistical-inference",
    "title": "GLM basics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nThe way the world is = our model + error\nHow good is our model? Is it a good representation of reality? Does it “fit” the data well?\nNeed to go beyond asking if it is significant, because what does that mean? Remember, all models are wrong\nWe are going to make predictions and see if the predictions (based on our model) matches our data\nWe can then compare one model to another to see which one matches our data better ie which one is a better representation of reality."
  },
  {
    "objectID": "2-Basics.html#predictions",
    "href": "2-Basics.html#predictions",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\nOur model is a prediction machine.\nThey are created by simply plugging a persons Xs into the created model\nIf you have bs and have Xs you can create a prediction\n\n\\(\\hat{Y}_{i}\\) = 2.65064 + -0.48111* \\(X_{i}\\)"
  },
  {
    "objectID": "2-Basics.html#predictions-1",
    "href": "2-Basics.html#predictions-1",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\nWe want our predictions to be close to our actual data for each person ( \\(Y_{i}\\) )\nThe difference between the actual data and our our prediction ( \\(Y_{i} - \\hat{Y}_{i} = e\\) ) is the residual, how far we are “off”. This tells us how good our fit is.\nYou can have the same estimates for two models but completely different fit.\nPreviously you may have evaluated overall model fit by looking at Eta Squared, SS Error and visualizing observations around group means"
  },
  {
    "objectID": "2-Basics.html#which-one-has-better-fit",
    "href": "2-Basics.html#which-one-has-better-fit",
    "title": "GLM basics",
    "section": "Which one has better fit?",
    "text": "Which one has better fit?\n\nCan you point out the predictions?\n\n\n\nCode\ntwogroup_fun = function(nrep = 100, b0 = 6, b1 = -2, sigma = 1) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\n\ntwogroup_fun2 = function(nrep = 100, b0 = 6, b1 = -2, sigma = 2) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\nset.seed(16)\nlibrary(broom)\nlm1 &lt;- augment(twogroup_fun())\n\nset.seed(16)\nlm2 &lt;- augment(twogroup_fun2())\n\nplot1&lt;- ggplot(lm1) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\nplot2&lt;- ggplot(lm2) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\n\nlibrary(gridExtra)\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\n grid.arrange(plot1, plot2, ncol=2)\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "2-Basics.html#easy-to-examine-fit-with-lm-objects",
    "href": "2-Basics.html#easy-to-examine-fit-with-lm-objects",
    "title": "GLM basics",
    "section": "Easy to examine fit with lm objects",
    "text": "Easy to examine fit with lm objects\n\nThese are automatically created anytime you run a lm in R\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\n\n\n\ncoefficients(mod.1)       # coefficients\nresiduals(mod.1)          # residuals\nfitted.values(mod.1)      # fitted values ie predicted\nsummary(mod.1)$r.squared  # R-sq for the model\nsummary(mod.1)$sigma      # sd of residuals\n\n\n\ncoefficients(mod.1)\n\n(Intercept)          tx \n  2.6506410  -0.4811057 \n\n\n\n\nfitted.values(mod.1)\n\n       1        2        3        4        5        6        7        8 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 2.650641 \n       9       10       11       12       13       14       15       16 \n2.650641 2.169535 2.169535 2.650641 2.650641 2.650641 2.650641 2.650641 \n      17       18       19       20       21       22       23       24 \n2.650641 2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 \n      25       26       27       28       29       30       31       32 \n2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 \n      33       34       35       36       37       38       39       40 \n2.650641 2.650641 2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 \n      41       42       43       44       45       46       47       48 \n2.650641 2.169535 2.650641 2.650641 2.169535 2.650641 2.650641 2.650641 \n      49       50       51       52       53       54       55       56 \n2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 \n      57       58       59       60       61       62       63       64 \n2.169535 2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 2.650641 \n      65       66       67       68       69       70       71       72 \n2.169535 2.169535 2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 \n      73       74       75       76       77       78       79       80 \n2.169535 2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 2.650641 \n      81       82       83       84       85       86       87       88 \n2.169535 2.650641 2.650641 2.650641 2.650641 2.169535 2.169535 2.650641 \n      89       90       91       92       93       94       95       96 \n2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 \n      97       98       99      100      101      102      103      104 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     105      106      107      108      109      110      111      112 \n2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 \n     113      114      115      116      117      118      119      120 \n2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.169535 2.650641 \n     121      122      123      124      125      126      127      128 \n2.650641 2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 \n     129      130      131      132      133      134      135      136 \n2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     137      138      139      140      141      142      143      144 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     145      146      147      148      149      150      151      152 \n2.169535 2.169535 2.169535 2.169535 2.650641 2.650641 2.650641 2.650641 \n     153      154      155      156      157      158      159      160 \n2.650641 2.650641 2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 \n     161      162      163      164      165      166      167      168 \n2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     169      170      171      172      173      174      175      176 \n2.169535 2.169535 2.650641 2.650641 2.169535 2.169535 2.169535 2.650641 \n     177      178      179      180      181      182      183      184 \n2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 \n     185      186      187      188      189      190      191      192 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 \n     193      194      195      196      197      198      199      200 \n2.169535 2.650641 2.169535 2.169535 2.650641 2.169535 2.169535 2.169535 \n     201      202      203      204      205      206      207      208 \n2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     209      210      211      212      213      214      215      216 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 2.650641 2.169535 \n     217      218      219      220      221      222      223      224 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 2.169535 \n     225      226      227      228      229      230      231      232 \n2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 2.650641 2.169535 \n     233      234      235      236      237      238      239      240 \n2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 \n     241      242      243      244      245      246      247      248 \n2.650641 2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 2.650641 \n     249      250      251      252      253      254      255      256 \n2.650641 2.650641 2.169535 2.650641 2.650641 2.169535 2.169535 2.169535 \n     257      258      259      260      261      262      263      264 \n2.169535 2.169535 2.650641 2.169535 2.169535 2.650641 2.650641 2.650641 \n     265      266      267      268      269      270 \n2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 \n\n\n\n\nresiduals(mod.1)\n\n           1            2            3            4            5            6 \n-0.312392427 -1.169535284  1.116179002 -0.169535284  0.259036145  1.116179002 \n           7            8            9           10           11           12 \n-1.483974359 -0.222069597  0.349358974 -0.455249570  1.687607573 -0.079212455 \n          13           14           15           16           17           18 \n 0.206501831  0.920787545  1.349358974  0.349358974 -0.936355312 -1.364926740 \n          19           20           21           22           23           24 \n-0.364926740 -0.312392427 -0.598106713 -0.026678141  0.116179002 -0.455249570 \n          25           26           27           28           29           30 \n 1.063644688 -0.650641026 -0.079212455  0.206501831  0.777930403 -1.079212455 \n          31           32           33           34           35           36 \n 0.063644688 -0.507783883  0.777930403 -0.936355312 -1.079212455 -1.079212455 \n          37           38           39           40           41           42 \n 0.116179002 -0.598106713 -0.598106713  0.116179002  0.635073260 -0.598106713 \n          43           44           45           46           47           48 \n-0.507783883  0.635073260  0.116179002 -0.793498169  0.635073260  1.349358974 \n          49           50           51           52           53           54 \n 0.830464716  0.492216117  0.497131383 -0.598106713 -0.312392427 -0.883820998 \n          55           56           57           58           59           60 \n 0.973321859 -0.222069597 -1.169535284  0.259036145 -0.455249570 -0.169535284 \n          61           62           63           64           65           66 \n-0.079212455  0.544750430  1.259036145  0.920787545  0.830464716  2.259036145 \n          67           68           69           70           71           72 \n-0.650641026 -1.650641026 -0.169535284  1.687607573 -1.026678141  0.401893287 \n          73           74           75           76           77           78 \n 0.973321859 -1.026678141 -0.598106713  0.973321859  0.849358974 -0.598106713 \n          79           80           81           82           83           84 \n 0.973321859  0.777930403  1.401893287 -1.364926740 -1.079212455 -0.222069597 \n          85           86           87           88           89           90 \n 0.492216117  1.687607573 -1.026678141 -0.650641026 -0.312392427 -0.312392427 \n          91           92           93           94           95           96 \n 1.973321859  0.920787545 -1.169535284  0.116179002 -1.026678141  0.973321859 \n          97           98           99          100          101          102 \n-0.026678141 -0.598106713 -0.883820998  1.116179002 -0.883820998 -0.169535284 \n         103          104          105          106          107          108 \n-1.026678141  0.116179002  0.063644688  0.349358974  0.206501831  0.349358974 \n         109          110          111          112          113          114 \n 1.206501831  0.777930403  0.777930403 -0.079212455  1.206501831  0.349358974 \n         115          116          117          118          119          120 \n 0.492216117  0.777930403 -0.650641026  0.206501831  0.544750430  0.920787545 \n         121          122          123          124          125          126 \n-0.222069597  0.349358974 -0.793498169  0.973321859 -0.026678141  1.544750430 \n         127          128          129          130          131          132 \n 0.259036145 -0.883820998 -1.079212455 -1.026678141 -0.598106713 -0.169535284 \n         133          134          135          136          137          138 \n 0.259036145 -1.169535284  1.259036145 -0.740963855 -0.169535284 -0.169535284 \n         139          140          141          142          143          144 \n 0.116179002  1.116179002 -0.026678141 -0.502868617 -0.312392427  1.259036145 \n         145          146          147          148          149          150 \n 2.544750430 -1.026678141  0.401893287  0.830464716  0.920787545 -1.507783883 \n         151          152          153          154          155          156 \n 0.920787545 -1.364926740 -0.222069597  0.063644688 -0.312392427  0.349358974 \n         157          158          159          160          161          162 \n 0.401893287 -0.883820998 -0.455249570 -0.312392427  0.349358974 -1.026678141 \n         163          164          165          166          167          168 \n 0.687607573 -0.169535284  1.116179002  0.116179002 -0.598106713 -0.669535284 \n         169          170          171          172          173          174 \n 0.259036145 -0.455249570  0.492216117 -0.936355312 -0.598106713  0.687607573 \n         175          176          177          178          179          180 \n 0.401893287  0.492216117  0.920787545 -0.364926740 -0.026678141 -0.740963855 \n         181          182          183          184          185          186 \n-0.026678141 -0.026678141  0.259036145 -1.364926740 -0.598106713 -0.598106713 \n         187          188          189          190          191          192 \n-1.026678141 -1.169535284 -0.002868617 -0.936355312 -0.740963855  0.687607573 \n         193          194          195          196          197          198 \n-0.598106713  1.349358974 -0.883820998  0.259036145 -0.364926740 -0.740963855 \n         199          200          201          202          203          204 \n 0.401893287 -0.883820998 -0.312392427 -0.079212455  0.401893287  0.259036145 \n         205          206          207          208          209          210 \n 0.687607573 -0.740963855 -0.598106713 -0.455249570  0.259036145  0.259036145 \n         211          212          213          214          215          216 \n 0.401893287 -0.740963855 -0.312392427  0.635073260 -1.507783883 -0.598106713 \n         217          218          219          220          221          222 \n 0.973321859 -0.312392427  0.116179002 -0.026678141  1.259036145 -0.169535284 \n         223          224          225          226          227          228 \n-1.079212455  0.830464716 -0.883820998 -0.026678141  0.401893287  0.635073260 \n         229          230          231          232          233          234 \n-0.740963855  0.116179002 -0.793498169 -0.598106713 -0.026678141  1.492216117 \n         235          236          237          238          239          240 \n 0.544750430  0.544750430 -0.312392427 -0.455249570 -0.455249570 -0.936355312 \n         241          242          243          244          245          246 \n 0.063644688  0.401893287 -0.650641026 -0.312392427  0.401893287  0.259036145 \n         247          248          249          250          251          252 \n-1.026678141  0.777930403  1.063644688  0.063644688  0.973321859  0.920787545 \n         253          254          255          256          257          258 \n 0.349358974  0.544750430  0.259036145  0.973321859 -0.455249570 -0.740963855 \n         259          260          261          262          263          264 \n-0.936355312 -1.169535284 -0.169535284  0.635073260 -0.222069597 -0.222069597 \n         265          266          267          268          269          270 \n-0.222069597  0.349358974 -0.364926740 -0.079212455 -0.507783883 -0.079212455"
  },
  {
    "objectID": "2-Basics.html#pop-quiz",
    "href": "2-Basics.html#pop-quiz",
    "title": "GLM basics",
    "section": "Pop quiz",
    "text": "Pop quiz\n\\[\\hat{Y}_{i} = b_{0} + b_{1}X_{i}\\]\n\\[Y_{i} = b_{0} + b_{1}X_{i} +e_{i}\\]\n\\[Y_{i}  - \\hat{Y}_{i} = e\\]\n\nCan you plug in numbers and calculate subject 3’s predicted and residual scores without explicitly asking for lm object residuals and fitted values? (using the same model from slide 41 on)\nPost answers in Slack – see if you and your peers get the same results!"
  },
  {
    "objectID": "2-Basics.html#residuals",
    "href": "2-Basics.html#residuals",
    "title": "GLM basics",
    "section": "Residuals",
    "text": "Residuals\n\n\nCode\nfit.1.data &lt;- augment(mod.1) \nfit.1.data\n\n\n# A tibble: 270 × 8\n   traffic.risk    tx .fitted .resid    .hat .sigma  .cooksd .std.resid\n          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1         1.86     1    2.17 -0.312 0.00602  0.780 0.000490     -0.402\n 2         1        1    2.17 -1.17  0.00602  0.777 0.00687      -1.51 \n 3         3.29     1    2.17  1.12  0.00602  0.777 0.00626       1.44 \n 4         2        1    2.17 -0.170 0.00602  0.780 0.000144     -0.218\n 5         2.43     1    2.17  0.259 0.00602  0.780 0.000337      0.334\n 6         3.29     1    2.17  1.12  0.00602  0.777 0.00626       1.44 \n 7         1.17     0    2.65 -1.48  0.00962  0.775 0.0178       -1.91 \n 8         2.43     0    2.65 -0.222 0.00962  0.780 0.000398     -0.287\n 9         3        0    2.65  0.349 0.00962  0.780 0.000986      0.451\n10         1.71     1    2.17 -0.455 0.00602  0.780 0.00104      -0.586\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#an-aside-concerning-lm-objects",
    "href": "2-Basics.html#an-aside-concerning-lm-objects",
    "title": "GLM basics",
    "section": "An aside concerning lm objects",
    "text": "An aside concerning lm objects\n\nlm objects consist of the information embedded in your linear model\nR often handles model objects poorly due to them not necessarily being in a usable data frame (lists!)\nthe broom package makes model objects into dataframes\n\n\n\nCode\nlibrary(broom)\nfit.1.tidy &lt;- tidy(mod.1)  \nfit.1.tidy\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6\n\n\n\n\nAugment function from the broom package amends the original dataset with lm object content. The new variable names of have a “.” in front of the name to distinguish\n\n\n\nCode\nfit.1.data &lt;- augment(mod.1) \nfit.1.data\n\n\n# A tibble: 270 × 8\n   traffic.risk    tx .fitted .resid    .hat .sigma  .cooksd .std.resid\n          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1         1.86     1    2.17 -0.312 0.00602  0.780 0.000490     -0.402\n 2         1        1    2.17 -1.17  0.00602  0.777 0.00687      -1.51 \n 3         3.29     1    2.17  1.12  0.00602  0.777 0.00626       1.44 \n 4         2        1    2.17 -0.170 0.00602  0.780 0.000144     -0.218\n 5         2.43     1    2.17  0.259 0.00602  0.780 0.000337      0.334\n 6         3.29     1    2.17  1.12  0.00602  0.777 0.00626       1.44 \n 7         1.17     0    2.65 -1.48  0.00962  0.775 0.0178       -1.91 \n 8         2.43     0    2.65 -0.222 0.00962  0.780 0.000398     -0.287\n 9         3        0    2.65  0.349 0.00962  0.780 0.000986      0.451\n10         1.71     1    2.17 -0.455 0.00602  0.780 0.00104      -0.586\n# ℹ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#pop-quiz-2",
    "href": "2-Basics.html#pop-quiz-2",
    "title": "GLM basics",
    "section": "Pop quiz #2",
    "text": "Pop quiz #2\n\nFor a two group ANOVA how many different predicted values will we have? Residuals?\nPost your answer in Slack and see how you compare to your peers!"
  },
  {
    "objectID": "2-Basics.html#statistical-inference-1",
    "href": "2-Basics.html#statistical-inference-1",
    "title": "GLM basics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nTo the extent that we can generate different predicted values of Y based on the values of the predictors, our model is doing well\nSaid differently, the closer our model is to the “actual” data generating model, our guesses ( \\(\\hat{Y}\\) ) will be closer to our actual data ( \\(Y\\) )\n\n\n\n\nCode\nlibrary(ggstatsplot)\nggstatsplot::ggbetweenstats(\n  data = example.data,\n  x = tx,\n  y = traffic.risk,\n  xlab = \"Traffic Risk Score\", # label for the x-axis variable\n  ylab = \"Treatment group\", # label for the y-axis variable\n  bf.message = FALSE,\n  mean.plotting = TRUE,\n  plot.type = \"violin\",\n  messages = FALSE\n) +\n  geom_hline(yintercept = 2.35)+\n  geom_text(aes(.55,2.5,label = \"Average\"))"
  },
  {
    "objectID": "2-Basics.html#statistical-inference-2",
    "href": "2-Basics.html#statistical-inference-2",
    "title": "GLM basics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nTo the extent that we can generate different predicted values of Y based on the values of the predictors, our model is doing well\nSaid differently, the closer our model is to the “actual” data generating model, our guesses ( \\(\\hat{Y}\\) ) will be closer to our actual data ( \\(Y\\) )"
  },
  {
    "objectID": "2-Basics.html#partitioning-the-variation-in-y",
    "href": "2-Basics.html#partitioning-the-variation-in-y",
    "title": "GLM basics",
    "section": "Partitioning the variation in Y",
    "text": "Partitioning the variation in Y\n\\[ \\sum (Y_i - \\bar{Y})^2 = \\sum (\\hat{Y}_i -\\bar{Y})^2 + \\sum(Y_i - \\hat{Y}_i)^2 \\]\n\nSS total = SS between + SS within\nSS total = SS regression + SS residual (or error)"
  },
  {
    "objectID": "2-Basics.html#what-can-we-do-with-this",
    "href": "2-Basics.html#what-can-we-do-with-this",
    "title": "GLM basics",
    "section": "What can we do with this?",
    "text": "What can we do with this?\n\nomnibus F tests (ANOVA)\nWhat hypothesis does the omnibus F test test, generally?\n\n\\[s_{y}^2 = s_{regression}^2 + s_{residual}^2\\]\n\\[1 = \\frac{s_{regression}^2}{s_{y}^2} + \\frac{s_{residual}^2}{s_{y}^2}\\]"
  },
  {
    "objectID": "2-Basics.html#coefficient-of-determination",
    "href": "2-Basics.html#coefficient-of-determination",
    "title": "GLM basics",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\\[\\frac{s_{regression}^2}{s_{y}^2} = \\frac{SS_{regression}}{SS_{Y}} = R^2\\]\n\nPercent (of total) variance explained by your model…which currently are groups\nAnother way of asking how much variance group status explains"
  },
  {
    "objectID": "2-Basics.html#r2-and-eta-squared",
    "href": "2-Basics.html#r2-and-eta-squared",
    "title": "GLM basics",
    "section": "\\(R^2\\) and Eta squared",
    "text": "\\(R^2\\) and Eta squared\n\nsummary(mod.1)$r.squared\n\n[1] 0.08344007\n\n\n\nlibrary(lsr)\netaSquared(mod.1)\n\n       eta.sq eta.sq.part\ntx 0.08344007  0.08344007"
  },
  {
    "objectID": "2-Basics.html#r2-for-different-coding-schemes",
    "href": "2-Basics.html#r2-for-different-coding-schemes",
    "title": "GLM basics",
    "section": "\\(R^2\\) for different coding schemes",
    "text": "\\(R^2\\) for different coding schemes\n\nglance(mod.1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0834        0.0800 0.779      24.4 0.00000138     1  -315.  635.  646.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(mod.1.eff)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0834        0.0800 0.779      24.4 0.00000138     1  -315.  635.  646.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "2-Basics.html#note-the-r2-p-value",
    "href": "2-Basics.html#note-the-r2-p-value",
    "title": "GLM basics",
    "section": "Note the \\(R^2\\) p-value",
    "text": "Note the \\(R^2\\) p-value\n\n\nCode\nsummary(mod.1)\n\n\n\nCall:\nlm(formula = traffic.risk ~ tx, data = example.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.65064 -0.59811 -0.02668  0.54475  2.54475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65064    0.07637  34.707  &lt; 2e-16 ***\ntx          -0.48111    0.09740  -4.939 1.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7789 on 268 degrees of freedom\nMultiple R-squared:  0.08344,   Adjusted R-squared:  0.08002 \nF-statistic:  24.4 on 1 and 268 DF,  p-value: 1.381e-06"
  },
  {
    "objectID": "2-Basics.html#summary",
    "href": "2-Basics.html#summary",
    "title": "GLM basics",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "2-Basics.html#summary-1",
    "href": "2-Basics.html#summary-1",
    "title": "GLM basics",
    "section": "Summary",
    "text": "Summary\n\nWe are using linear models to do the exact same tests as t-tests and ANOVAs\nIt is the exact same because t-tests and ANOVAs are part of the general linear model\nThe GLM provides a more systematic way at 1) building and testing your theoretical model and 2) comparing between alternative theoretical models\nYou can get 1) estimates and 2) fit statistics from the model. Both are important."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Download syllabus"
  },
  {
    "objectID": "1-HW.html",
    "href": "1-HW.html",
    "title": "Homework 1",
    "section": "",
    "text": "We will use the `palmerpenguins` dataset. Please install the pacakge `palmerpenguins` if you do not already have it and use the `penguins` data.frame.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "1-HW.html#question-1",
    "href": "1-HW.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\nRun a model to test the question:\n&gt; Do male and female penguins have different length flippers?\n\nYes or no, is the flipper length of male penguins signiciantly different from the flipper length of female penguins? How do you know?\nWhat is the (mean) flipper length for males and females, respectively?\nIs this a good model? How do you know?\nUsing the output, write the formal equation (see Slide 26 if confused)\nInterpret the intercept and the regression coefficient\nIf you were back in your OG Psych Stats days, what would we call this statistical test?\nMake a figure to illustrate this relationship. In it, you should include a summary metric (e.g., the mean or median) as well as information about the distribution of data, the raw data, or both. Look at slides for inspiration!"
  },
  {
    "objectID": "1-HW.html#question-2",
    "href": "1-HW.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\nRun a model to test the question:\n&gt; Are penguins on different islands the same size?\n\nTry to write the formal equation -- just do your best! In a sentence or two, what makes this equation different from the one you wrote in Question 1?\nIf you were back in your OG Psych Stats days, what would we call this statistical test?\nGiven the name of this statistical test, replace `lm` with a different function. That is, use a function that is *not* `lm` to run the same model. (I’m being intentionally vague here…). What changes about this new output, compared to the model in Question 1, and what stays the same?\nMake a figure to compare the size of penguins on different islands. Make sure to give an interval around the point estimate (*hint: this usually comes in the form of ‘bars’*)\nIs this a good model? How do you know?\nGiven the name of this statistical test, calculate an effect size. What do you notice between this effect size and the `lm` object output? Any similarities?"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "homework",
    "section": "",
    "text": "Homework 9\nHomework 8\nHomework 7\nHomework 6\nHomework 5\nHomework 4\nHomework 3\nHomework 2\nHomework 1"
  },
  {
    "objectID": "3-Basics.html#this-time",
    "href": "3-Basics.html#this-time",
    "title": "GLM basics II",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nWhat is regression and why is it useful?\nNuts and bolts\n\nEquation\nOrdinary least squares\nInterpretation"
  },
  {
    "objectID": "3-Basics.html#regression",
    "href": "3-Basics.html#regression",
    "title": "GLM basics II",
    "section": "Regression",
    "text": "Regression\nRegression is a general data analytic system, meaning lots of things fall under the umbrella of regression. This system can handle a variety of forms of relations, although all forms have to be specified in a linear way. Usefully, we can incorporate IVs of all nature – continuous, categorical, nominal, ordinal….\nThe output of regression includes both effect sizes and, if using frequentist or Bayesian software, statistical significance. We can also incorporate multiple influences (IVs) and account for their intercorrelations."
  },
  {
    "objectID": "3-Basics.html#regression-1",
    "href": "3-Basics.html#regression-1",
    "title": "GLM basics II",
    "section": "Regression",
    "text": "Regression\n\nScientific use: explaining the influence of one or more variables on some outcome.\n\nDoes this intervention affect reaction time? Does self-esteem predict relationship quality?\n\nPrediction use: We can develop models based on what’s happened in the past to predict what will happen in the figure.\n\nInsurance premiums? Graduate school… success?\n\nAdjustment: Statistically control for known effects\n\nIf everyone had the same level of SES, would abuse still be associated with criminal behavior?"
  },
  {
    "objectID": "3-Basics.html#regression-equation",
    "href": "3-Basics.html#regression-equation",
    "title": "GLM basics II",
    "section": "Regression equation",
    "text": "Regression equation\nWhat is a regression equation?\n\nFunctional relationship\n\nIdeally like a physical law \\((E = MC^2)\\)\nIn practice, it’s never as robust as that\n\n\nHow do we uncover the relationship?"
  },
  {
    "objectID": "3-Basics.html#how-does-y-vary-with-x",
    "href": "3-Basics.html#how-does-y-vary-with-x",
    "title": "GLM basics II",
    "section": "How does Y vary with X?",
    "text": "How does Y vary with X?\n\nThe regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X\n“Our best guess” regardless of whether our model includes categories or continuous predictor variables\nWe will evaluate our guesses based on how far away we are from the mean. But how do we come up with those guesses in the first place?"
  },
  {
    "objectID": "3-Basics.html#regression-equation-1",
    "href": "3-Basics.html#regression-equation-1",
    "title": "GLM basics II",
    "section": "Regression Equation",
    "text": "Regression Equation\n\\[\\Large Y = b_{0} + b_{1}X +e\\]\n\\[\\Large \\hat{Y} = b_{0} + b_{1}X\\]\n\\(\\hat{Y}\\) signifies the predicted score – no error\nThe difference between the predicted and observed score is the residual ( \\(e_i\\) )"
  },
  {
    "objectID": "3-Basics.html#ols",
    "href": "3-Basics.html#ols",
    "title": "GLM basics II",
    "section": "OLS",
    "text": "OLS\n\nHow do we find the regression estimates?\nOrdinary Least Squares (OLS) estimation\nMinimizes deviations\n\n\\[ min\\sum(Y_{i}-\\hat{Y})^{2} \\]\n\nOther estimation procedures possible (and necessary in some cases)\n\n\n\n\nCode\nset.seed(123)\nx.1 &lt;- rnorm(10, 0, 1)\ne.1 &lt;- rnorm(10, 0, 2)\ny.1 &lt;- .5 + .55 * x.1 + e.1\nd.1 &lt;- data.frame(x.1,y.1)\nm.1 &lt;- lm(y.1 ~ x.1, data = d.1)\nd1.f&lt;- augment(m.1)\n\n\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE) +\n  theme_bw(base_size = 20)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_point(aes(y = .fitted), shape = 1, size = 2) +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_point(aes(y = .fitted), shape = 1, size = 2) +\n  geom_segment(aes( xend = x.1, yend = .fitted))+\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE) +\n  geom_point(aes(y = .fitted), shape = 1, size = 2) +\n  geom_segment(aes( xend = x.1, yend = .fitted))+\n  theme_bw(base_size = 20)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "3-Basics.html#compare-with-bad-fit",
    "href": "3-Basics.html#compare-with-bad-fit",
    "title": "GLM basics II",
    "section": "compare with bad fit",
    "text": "compare with bad fit\n\n\nCode\nnew.i = 1.1\nnew.slope = -0.7\nd1.f$new.fitted = 1.1 -0.7*d1.f$x.1\n\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_abline(intercept = new.i, slope = new.slope, color = \"blue\", linewidth = 1) +\n  geom_point(aes(y = new.fitted), shape = 1, size = 2) +\n  geom_segment(aes( xend = x.1, yend = new.fitted))+\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "3-Basics.html#what-is-error",
    "href": "3-Basics.html#what-is-error",
    "title": "GLM basics II",
    "section": "What is error?",
    "text": "What is error?\n\\[\\Large Y = b_{0} + b_{1}X +e\\]\n\\[\\Large \\hat{Y} = b_{0} + b_{1}X\\]\n\\[\\Large Y_i = \\hat{Y_i} + e_i\\]\n\\[\\Large e_i = Y_i - \\hat{Y_i}\\]"
  },
  {
    "objectID": "3-Basics.html#ols-1",
    "href": "3-Basics.html#ols-1",
    "title": "GLM basics II",
    "section": "OLS",
    "text": "OLS\nThe line that yields the smallest sum of squared deviations\n\\[\\Large \\Sigma(Y_i - \\hat{Y_i})^2\\] \\[\\Large = \\Sigma(Y_i - (b_0+b_{1}X_i))^2\\] \\[\\Large = \\Sigma(e_i)^2\\]\n\nIn order to find the OLS solution, you could try many different coefficients \\((b_0 \\text{ and } b_{1})\\) until you find the one with the smallest sum squared deviation. Luckily, there are simple calculations that will yield the OLS solution every time."
  },
  {
    "objectID": "3-Basics.html#regression-coefficient-b_1",
    "href": "3-Basics.html#regression-coefficient-b_1",
    "title": "GLM basics II",
    "section": "Regression coefficient, \\(b_{1}\\)",
    "text": "Regression coefficient, \\(b_{1}\\)\n\\[\\large b_{1} = \\frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \\frac{s_{y}}{s_{x}}\\]\n\\[\\large r_{xy} = \\frac{s_{xy}}{s_xs_y}\\]\n\nThe regression coefficient (slope) equals the estimated change in Y for a 1-unit change in X"
  },
  {
    "objectID": "3-Basics.html#standardized-regression",
    "href": "3-Basics.html#standardized-regression",
    "title": "GLM basics II",
    "section": "Standardized regression",
    "text": "Standardized regression\n\nRegression using z-scores for Y and X\nCorrelation equals standardized regression coefficient\n\n\\[\\large b_{1} = r_{xy} \\frac{s_{y}}{s_{x}}\\]\n\\[ \\large r_{xy} = b_1\\frac{s_x}{s_y} \\]\nIf the variance of both X and Y is equal to 1 (as in z-scores):\n\\[\\large \\beta_{1} = b_{1}^* = r_{xy}\\]"
  },
  {
    "objectID": "3-Basics.html#standardized-regression-equation",
    "href": "3-Basics.html#standardized-regression-equation",
    "title": "GLM basics II",
    "section": "Standardized regression equation",
    "text": "Standardized regression equation\n\\[\\large Y = b_{1}^*X+e\\]\n\\[\\large b_{1}^* = b_{1}\\frac{s_x}{s_y}\\]\nWhen \\(X = 0, Y = 0\\). A one-standard deviation increase in X is associated with a \\(b_{1}^*\\) standard deviation increase in Y. Our regression coefficient is equivalent to the correlation coefficient when we have only one predictor in our model."
  },
  {
    "objectID": "3-Basics.html#estimating-the-intercept-b_0",
    "href": "3-Basics.html#estimating-the-intercept-b_0",
    "title": "GLM basics II",
    "section": "Estimating the intercept, \\(b_0\\)",
    "text": "Estimating the intercept, \\(b_0\\)\n\nRe-write equation to include the means of Y and X ( \\(\\bar{X}\\) & \\(\\bar{Y}\\) )\nIntercept serves to adjust for differences in means between X and Y\n\n\\[\\Large \\hat{Y} = \\bar{Y} + r_{xy} \\frac{s_{y}}{s_{x}}(X-\\bar{X})\\] - If standardized, intercept drops out. Otherwise, intercept is where regression line crosses the y-axis at X = 0\n- Notice that when \\(X = \\bar{X}\\) the regression line goes through \\(\\bar{Y}\\). This is true for all regressions such that the regression line must pass through \\(\\bar{X}\\) and \\(\\bar{Y}\\)"
  },
  {
    "objectID": "3-Basics.html#example",
    "href": "3-Basics.html#example",
    "title": "GLM basics II",
    "section": "Example",
    "text": "Example\n\n\nCode\nlibrary(psych)\n\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n\nCode\ngalton.data &lt;- psychTools::galton\nhead(galton.data)\n\n\n  parent child\n1   70.5  61.7\n2   68.5  61.7\n3   65.5  61.7\n4   64.5  61.7\n5   64.0  61.7\n6   67.5  62.2\n\n\nCode\ndescribe(galton.data, fast = T)\n\n\n       vars   n  mean   sd  min  max range   se\nparent    1 928 68.31 1.79 64.0 73.0     9 0.06\nchild     2 928 68.09 2.52 61.7 73.7    12 0.08\n\n\nCode\ncor(galton.data)\n\n\n          parent     child\nparent 1.0000000 0.4587624\nchild  0.4587624 1.0000000\n\n\n\nIf we regress child height onto parents’:\n\nr = cor(galton.data)[2,1]\nm_parent = mean(galton.data$parent)\nm_child = mean(galton.data$child)\ns_parent = sd(galton.data$parent)\ns_child = sd(galton.data$child)\n\n(b1 = r*(s_child/s_parent))\n\n[1] 0.6462906\n\n(b0 = m_child - b1*m_parent)\n\n[1] 23.94153\n\n\nHow will this change if we regress parent height onto child height?\n\n\n(b1 = r*(s_child/s_parent))\n\n[1] 0.6462906\n\n(b0 = m_child - b1*m_parent)\n\n[1] 23.94153\n\n\n\n(b1 = r*(s_parent/s_child))\n\n[1] 0.3256475\n\n(b0 = m_parent - b1*m_child)\n\n[1] 46.13535"
  },
  {
    "objectID": "3-Basics.html#in-r",
    "href": "3-Basics.html#in-r",
    "title": "GLM basics II",
    "section": "In R",
    "text": "In R\n\n\nCode\nfit.1 &lt;- lm(child ~ parent, data = galton.data)\nsummary(fit.1)\n\n\n\nCall:\nlm(formula = child ~ parent, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "3-Basics.html#reversed",
    "href": "3-Basics.html#reversed",
    "title": "GLM basics II",
    "section": "Reversed",
    "text": "Reversed\n\n\nCode\nsummary(lm(parent ~ child, data = galton.data))\n\n\n\nCall:\nlm(formula = parent ~ child, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6702 -1.1702 -0.1471  1.1324  4.2722 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 46.13535    1.41225   32.67   &lt;2e-16 ***\nchild        0.32565    0.02073   15.71   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.589 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nCode\nggplot(galton.data, aes(x=parent, y=child)) +\n    geom_point() +    \n    geom_smooth(method=lm,   # Add linear regression line\n                se=FALSE) +\n  theme_bw(base_size = 20)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "3-Basics.html#data-predicted-and-residuals",
    "href": "3-Basics.html#data-predicted-and-residuals",
    "title": "GLM basics II",
    "section": "Data, predicted, and residuals",
    "text": "Data, predicted, and residuals\n\n\nCode\nlibrary(broom)\nmodel_info = augment(fit.1)\nhead(model_info)\n\n\n# A tibble: 6 × 8\n  child parent .fitted .resid    .hat .sigma .cooksd .std.resid\n  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1  61.7   70.5    69.5  -7.81 0.00270   2.22 0.0165       -3.49\n2  61.7   68.5    68.2  -6.51 0.00109   2.23 0.00462      -2.91\n3  61.7   65.5    66.3  -4.57 0.00374   2.23 0.00787      -2.05\n4  61.7   64.5    65.6  -3.93 0.00597   2.24 0.00931      -1.76\n5  61.7   64      65.3  -3.60 0.00735   2.24 0.00966      -1.62\n6  62.2   67.5    67.6  -5.37 0.00130   2.23 0.00374      -2.40\n\n\n\n\ndescribe(model_info)\n\n           vars   n  mean   sd median trimmed  mad   min   max range  skew\nchild         1 928 68.09 2.52  68.20   68.12 2.97 61.70 73.70 12.00 -0.09\nparent        2 928 68.31 1.79  68.50   68.32 1.48 64.00 73.00  9.00 -0.04\n.fitted       3 928 68.09 1.16  68.21   68.10 0.96 65.30 71.12  5.82 -0.04\n.resid        4 928  0.00 2.24   0.05    0.06 2.26 -7.81  5.93 13.73 -0.24\n.hat          5 928  0.00 0.00   0.00    0.00 0.00  0.00  0.01  0.01  1.99\n.sigma        6 928  2.24 0.00   2.24    2.24 0.00  2.22  2.24  0.01 -2.39\n.cooksd       7 928  0.00 0.00   0.00    0.00 0.00  0.00  0.02  0.02  3.44\n.std.resid    8 928  0.00 1.00   0.02    0.03 1.01 -3.49  2.65  6.14 -0.24\n           kurtosis   se\nchild         -0.35 0.08\nparent         0.05 0.06\n.fitted        0.05 0.04\n.resid        -0.23 0.07\n.hat           3.47 0.00\n.sigma         8.62 0.00\n.cooksd       17.34 0.00\n.std.resid    -0.23 0.03\n\n\n\nThe average of the residuals is 0, just like average deviation from the mean is 0."
  },
  {
    "objectID": "3-Basics.html#residuals",
    "href": "3-Basics.html#residuals",
    "title": "GLM basics II",
    "section": "Residuals",
    "text": "Residuals\n\nDispersion of residuals can be thought of as what is left over in Y that is not explained by our model. As residuals get smaller on average, so will the SD of the residuals.\nSigma ( \\(\\sigma\\) ) is the SD of residuals. It can be thought of as how much left over in Y that we cannot explain by our model.\n\n\n\n\nCode\nmodel_info %&gt;% ggplot(aes(x = parent, y = .fitted)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + ggtitle(expression(paste(\"X is related to \", hat(Y))))+\n  scale_x_continuous(\"X\") + scale_y_continuous(expression(hat(Y))) + theme_bw(base_size = 30)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCode\nmodel_info %&gt;% ggplot(aes(x = parent, y = .resid)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + ggtitle(\"X is always unrelated to e\")+\n  scale_x_continuous(\"X\") + scale_y_continuous(\"e\") + theme_bw(base_size = 30)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCode\nmodel_info %&gt;% ggplot(aes(x = child, y = .fitted)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + ggtitle(expression(paste(\"Y can be related to \", hat(Y))))+\n  scale_x_continuous(\"Y\") + scale_y_continuous(expression(hat(Y))) + theme_bw(base_size = 30)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCode\nmodel_info %&gt;% ggplot(aes(x = child, y = .resid)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + ggtitle(\"Y is sometimes related to e\")+\n  scale_x_continuous(\"Y\") + scale_y_continuous(\"e\") + theme_bw(base_size = 25)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCode\nmodel_info %&gt;% ggplot(aes(x = .fitted, y = .resid)) +\n  geom_point() + geom_smooth(se = F, method = \"lm\") + ggtitle(expression(paste(hat(Y), \" is always unrelated to e\")))+\n  scale_y_continuous(\"e\") + scale_x_continuous(expression(hat(Y))) + theme_bw(base_size = 30)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCode\nmodel_info %&gt;% rename(y = child, x = parent) %&gt;% select(x,y,.fitted,.resid) %&gt;% gather(\"key\", \"value\") %&gt;%\n  ggplot(aes(value, fill = key)) + geom_histogram(bins = 25) + guides(fill = F)+\n  facet_wrap(~key, scales = \"free\") + theme_bw(base_size = 20)\n\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4."
  },
  {
    "objectID": "3-Basics.html#residuals-summary",
    "href": "3-Basics.html#residuals-summary",
    "title": "GLM basics II",
    "section": "Residuals Summary",
    "text": "Residuals Summary\n\nResiduals are not correlated with \\(X\\) and \\(\\hat{Y}\\) because those two are perfectly correlated with one another (that is, \\(r_{\\text{fitted,x}} = 1\\) )\n\\(X\\) and \\(\\hat{Y}\\) represent the same information. We use our model ( \\(X\\) ) to make a prediction ( \\(\\hat{Y}\\) ). These predictions are entirely based on the model.\nThere is no correlation between residuals with \\(X\\) and \\(\\hat{Y}\\) because they are created by subtracting them out of \\(Y\\). That is, ( \\(\\epsilon = Y - \\hat{Y}\\) )\nSigma ( \\(\\sigma\\) ; the SD of residuals) can be thought of as how much left over in \\(Y\\) after we take out all of the information our model provides."
  },
  {
    "objectID": "2-HW.html",
    "href": "2-HW.html",
    "title": "Homework 2",
    "section": "",
    "text": "Data: You will need to use the Dawtry et al., 2015 Psychological Science article and the accompanying dataset to answer all questions. Please download this dawtry-hw2-data dataset for the homework, which corresponds to study 1a of the paper. The paper can be found here.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd (or .qmd) and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "2-HW.html#question-1",
    "href": "2-HW.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nGraph the distributions of the following items. Be sure to comment on anything you find irregular or interesting. The variables to be plotted are:\n\nHousehold Income\nFairness and Satisfaction\nSocial Circle Mean Income\nPopulation Mean Income\nPolitical Preference"
  },
  {
    "objectID": "2-HW.html#question-2",
    "href": "2-HW.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nRun a regression where political preferences predict redistribution:\n\nWrite the formal equation (be specific!)\nInterpret the intercept and the regression coefficient (be specific!)\nStandardize the variables, rerun the model, write the formal equation (be specific!) and interpret the intercept and the regression coefficient (be specific!)\nCalculate the correlation of the two variables (use the original variables in raw units). How does this relate to the standardized and unstandardized regression coefficients?\nCreated a fitted vs. residuals plot. Do not use the defaults – you must (at the very least) change the labels, colors, and give it a titles. Then interpret this plot."
  },
  {
    "objectID": "hw-02.html",
    "href": "hw-02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Data: You will need to use the Dawtry et al., 2015 Psychological Science article and the accompanying dataset to answer all questions. Please download this dawtry-hw2-data dataset for the homework, which corresponds to study 1a of the paper. The paper can be found here.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "hw-02.html#question-1",
    "href": "hw-02.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nGraph the distributions of the following items. Be sure to comment on anything you find irregular or interesting. The variables to be plotted are:\n\nHousehold Income\nFairness and Satisfaction\nSocial Circle Mean Income\nPopulation Mean Income\nPolitical Preference"
  },
  {
    "objectID": "hw-02.html#question-2",
    "href": "hw-02.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nRun a regression where political preferences predict redistribution:\n\nWrite the formal equation (be specific!)\nInterpret the intercept and the regression coefficient (be specific!)\nStandardize the variables, rerun the model, write the formal equation (be specific!) and interpret the intercept and the regression coefficient (be specific!)\nCalculate the correlation of the two variables (use the original variables in raw units). How does this relate to the standardized and unstandardized regression coefficients?\nCreated a fitted vs. residuals plot. Do not use the defaults – you must (at the very least) change the labels, colors, and give it a titles. Then interpret this plot."
  },
  {
    "objectID": "4-Basics.html#statistical-inference",
    "href": "4-Basics.html#statistical-inference",
    "title": "GLM basics III",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nThe way the world is = our model + error\nHow good is our model? Does it “fit” the data well?\nTo assess how well our model fits the data, we simply take all the variability in our outcome and partition it into different categories. For now, we will partition it into two categories: the variability that is predicted by (explained by) our model, and variability that is not."
  },
  {
    "objectID": "4-Basics.html#partitioning-variation",
    "href": "4-Basics.html#partitioning-variation",
    "title": "GLM basics III",
    "section": "Partitioning variation",
    "text": "Partitioning variation\n\nWe formally test how well we are doing with our guesses by partitioning variation\nTo the extent that we can generate different predicted values of Y, based on the values of the predicors, we are doing well in our prediction\n\n\\[\\sum (Y - \\bar{Y})^2 = \\sum (\\hat{Y} -\\bar{Y})^2 + \\sum(Y - \\hat{Y})^2\\]\nEach of these is the sum of a squared deviation from an expected value of Y. We can abbreviate the sum of squared deviations:\n\\[SS_{Y} = SS_{\\text{Model}} + SS_{\\text{Residual}}\\]\n\\[\\frac{s^2_{regression}}{s^2_y} = \\frac{SS_{regression}}{SS_Y} = R^2\\]"
  },
  {
    "objectID": "4-Basics.html#partitioning-variance",
    "href": "4-Basics.html#partitioning-variance",
    "title": "GLM basics III",
    "section": "Partitioning Variance",
    "text": "Partitioning Variance\nThe relative magnitude of sums of squares, especially in more complex designs, provides a way of identifying particularly large and important sources of variability. In the future, we can further partition \\(SS_{\\text{Model}}\\) and \\(SS_{\\text{Residual}}\\) into smaller pieces, which will help us make more specific inferences and increase statistical power, respectively.\n\\[\\Large s^2_Y = s^2_{\\hat{Y}} + s^2_{e}\\]"
  },
  {
    "objectID": "4-Basics.html#partitioning-variance-in-y",
    "href": "4-Basics.html#partitioning-variance-in-y",
    "title": "GLM basics III",
    "section": "Partitioning variance in Y",
    "text": "Partitioning variance in Y\nConsider the case with no correlation between X and Y"
  },
  {
    "objectID": "4-Basics.html#large-haty-bary-r_xy-fracs_ys_xx-barx",
    "href": "4-Basics.html#large-haty-bary-r_xy-fracs_ys_xx-barx",
    "title": "GLM basics III",
    "section": "\\[\\Large \\hat{Y} = \\bar{Y} + r_{xy} \\frac{s_{y}}{s_{x}}(X-\\bar{X})\\]",
    "text": "\\[\\Large \\hat{Y} = \\bar{Y} + r_{xy} \\frac{s_{y}}{s_{x}}(X-\\bar{X})\\]\n\\[\\Large \\hat{Y} = \\bar{Y}\\]\nTo the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction\n\\[\\large \\sum (Y - \\bar{Y})^2 = \\sum (\\hat{Y} -\\bar{Y})^2 + \\sum(Y - \\hat{Y})^2\\]"
  },
  {
    "objectID": "4-Basics.html#coefficient-of-determination",
    "href": "4-Basics.html#coefficient-of-determination",
    "title": "GLM basics III",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\\[\\Large \\frac{s_{Model}^2}{s_{y}^2} = \\frac{SS_{Model}}{SS_{Y}} = R^2\\]\n\\(R^2\\) represents the proportion of variance in Y that is explained by the model.\n\\(\\sqrt{R^2} = R\\) is the correlation between the predicted values of Y from the model and the actual values of Y\n\\[\\large \\sqrt{R^2} = r_{Y\\hat{Y}}\\]"
  },
  {
    "objectID": "4-Basics.html#example",
    "href": "4-Basics.html#example",
    "title": "GLM basics III",
    "section": "Example",
    "text": "Example\n\n\nCode\ngalton.data &lt;- psychTools::galton\nfit.1 = lm(child ~ parent, data = galton.data)\nsummary(fit.1) \n\n\n\nCall:\nlm(formula = child ~ parent, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(fit.1)$r.squared\n\n\n[1] 0.2104629"
  },
  {
    "objectID": "4-Basics.html#example-1",
    "href": "4-Basics.html#example-1",
    "title": "GLM basics III",
    "section": "Example",
    "text": "Example\n\n\nCode\ncor(galton.data$parent, galton.data$child, use = \"pairwise\")\n\n\n[1] 0.4587624\n\n\n\n\nCode\ncor(galton.data$parent, galton.data$child)^2\n\n\n[1] 0.2104629"
  },
  {
    "objectID": "4-Basics.html#computing-sum-of-squares",
    "href": "4-Basics.html#computing-sum-of-squares",
    "title": "GLM basics III",
    "section": "Computing Sum of Squares",
    "text": "Computing Sum of Squares\n\\[\\Large \\frac{SS_{Model}}{SS_{Y}} = R^2\\] \\[\\Large SS_{Model} = R^2({SS_{Y})}\\]\n\\[\\Large SS_{Y} = SS_{Model} + SS_{residual}\\]\n\\[\\Large SS_{residual} = SS_{Y} - R^2({SS_{Y})}\\]\n\\[\\Large  SS_{residual} = (1- R^2){SS_{Y}}\\]"
  },
  {
    "objectID": "4-Basics.html#using-r-to-check-yourself",
    "href": "4-Basics.html#using-r-to-check-yourself",
    "title": "GLM basics III",
    "section": "Using R To Check Yourself",
    "text": "Using R To Check Yourself\n\\[SS_{residual} = (1- R^2){SS_{Y}}\\]\n\nr2 = summary(fit.1)$r.squared\n\nfit.1.anova = summary(aov(fit.1))\n\nssTotal = fit.1.anova[[1]]$`Sum Sq`[1] + fit.1.anova[[1]]$`Sum Sq`[2]\n\nssResidual = (1 - r2) * ssTotal\n\n# print things out\nfit.1.anova\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nparent        1   1237    1237   246.8 &lt;2e-16 ***\nResiduals   926   4640       5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nssResidual\n\n[1] 4640.273\n\n\n???"
  },
  {
    "objectID": "4-Basics.html#mean-square-error-mse-mswmsr",
    "href": "4-Basics.html#mean-square-error-mse-mswmsr",
    "title": "GLM basics III",
    "section": "Mean Square Error (MSE) (msw/msr)",
    "text": "Mean Square Error (MSE) (msw/msr)\n\nAKA square of residual standard error/deviation (sigma)\nUnbiased estimate of error variance\nMeasure of discrepancy between the data and the model\nThe MSE is the variance of data around the fitted regression line\nJust like with MSwithin, which was variance around predicted group means\nIt is the mean of the square of the residuals\n\n\nhead(fit.1$residuals)\n\n        1         2         3         4         5         6 \n-7.805016 -6.512435 -4.573563 -3.927273 -3.604127 -5.366144 \n\nmse = round(mean(fit.1$residuals ^ 2), digits = 2)\nmse\n\n[1] 5"
  },
  {
    "objectID": "4-Basics.html#mse",
    "href": "4-Basics.html#mse",
    "title": "GLM basics III",
    "section": "MSE",
    "text": "MSE\n\nIt is the square of the residual standard error/deviation (sigma) aka \\(RSE^2\\)\n\n\nmse = round((summary(fit.1)[[\"sigma\"]]) ^2, digits = 2)\nmse\n\n[1] 5.01"
  },
  {
    "objectID": "4-Basics.html#residual-standard-error",
    "href": "4-Basics.html#residual-standard-error",
    "title": "GLM basics III",
    "section": "Residual Standard Error",
    "text": "Residual Standard Error\n\n\n\nCall:\nlm(formula = child ~ parent, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "4-Basics.html#residual-standard-errordeviation",
    "href": "4-Basics.html#residual-standard-errordeviation",
    "title": "GLM basics III",
    "section": "Residual standard error/deviation",
    "text": "Residual standard error/deviation\n\naka standard deviation of the residual\naka standard error of the estimate\n\n\\[\\hat{\\sigma} = \\sqrt{\\frac{SS_{\\text{Residual}}}{df_{\\text{Residual}}}} = s_{Y|X} = \\sqrt{\\frac{\\Sigma(Y_i -\\hat{Y_i})^2}{N-2}}\\]\n\ninterpreted in original units (unlike \\(R^2\\))\nstandard deviation of Y not accounted by model"
  },
  {
    "objectID": "4-Basics.html#residual-standard-errordeviation-or-standard-error-of-the-estimate",
    "href": "4-Basics.html#residual-standard-errordeviation-or-standard-error-of-the-estimate",
    "title": "GLM basics III",
    "section": "Residual standard error/deviation or standard error of the estimate",
    "text": "Residual standard error/deviation or standard error of the estimate\n\nsummary(fit.1)$sigma \n\n[1] 2.238547\n\ngalton.data.1 = broom::augment(fit.1)\npsych::describe(galton.data.1$.resid)\n\n   vars   n mean   sd median trimmed  mad   min  max range  skew kurtosis   se\nX1    1 928    0 2.24   0.05    0.06 2.26 -7.81 5.93 13.73 -0.24    -0.23 0.07\n\nsd(galton.data$child)\n\n[1] 2.517941\n\n\nBecause the size of \\(\\hat{\\sigma}\\) depends on both how well the model does as well as the original units of measurement, it is important to compare it to Y to evaluate!"
  },
  {
    "objectID": "4-Basics.html#rse-vs-mse",
    "href": "4-Basics.html#rse-vs-mse",
    "title": "GLM basics III",
    "section": "RSE vs MSE",
    "text": "RSE vs MSE\nResidual standard error = square root of the mean square error\nBoth measuring error, but RSE is a little more useful\n\nsqrt(mse)\n\n[1] 2.238303"
  },
  {
    "objectID": "4-Basics.html#residual-standard-error-and-sigma",
    "href": "4-Basics.html#residual-standard-error-and-sigma",
    "title": "GLM basics III",
    "section": "Residual Standard Error and Sigma",
    "text": "Residual Standard Error and Sigma\n\nSo many names to represent the spread of data around the regression line\nStandard deviation of the residual, standard error of the estimate, MSE…\nWe will refer to this as sigma, and use estimated sigma, as we do not know the population value ( \\(\\hat{\\sigma}\\) )\nIt is interpreted in original units (unlike \\(R^2\\) )\nIt is the standard deviation of Y not accounted by the model (i.e., residuals)"
  },
  {
    "objectID": "4-Basics.html#why-do-we-care-about-sigma",
    "href": "4-Basics.html#why-do-we-care-about-sigma",
    "title": "GLM basics III",
    "section": "Why do we care about sigma?",
    "text": "Why do we care about sigma?\n\nLet’s simulate!\nData generating process:\n\n\\[Y_{i} \\sim\\mathcal{N}(\\mu,\\,\\sigma)\\ \\]\n\nIn English: our DV for individual \\(i\\) is distributed normally with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\)\nThis describes how we think our DVs are generated, and the paramters of interest A a standard regression model assumes this, but we will see other DGPs such as binomial or poisson that do not\nFor normal, \\(\\mu\\) gets all the focus but \\(\\sigma\\) is just as important"
  },
  {
    "objectID": "4-Basics.html#r2-and-residual-standard-deviation",
    "href": "4-Basics.html#r2-and-residual-standard-deviation",
    "title": "GLM basics III",
    "section": "R2 and residual standard deviation",
    "text": "R2 and residual standard deviation\n\ntwo sides of same coin\none in original units, the other standardized\n\\(R^2\\) can be tricky because the numerator and denominator can be changed in different ways.\nfor example if variance in Y is changed but with the same regression model and residual standard error, \\(R^2\\) could increase or decrease"
  },
  {
    "objectID": "4-Basics.html#inferential-tests",
    "href": "4-Basics.html#inferential-tests",
    "title": "GLM basics III",
    "section": "Inferential tests",
    "text": "Inferential tests\nNHST is about making decisions:\n\nthese two means are/are not different\nthis correlation is/is not significantly different from 0\nthe distribution of this categorical variable is/is not different between these groups\n\nIn regression, there are several inferential tests being conducted at once. The first is called the omnibus test – this is a test of whether the model fits the data."
  },
  {
    "objectID": "4-Basics.html#omnibus-test",
    "href": "4-Basics.html#omnibus-test",
    "title": "GLM basics III",
    "section": "Omnibus test",
    "text": "Omnibus test\n\\[\\Large H_{0}: \\rho_{XY}^2= 0\\]\n\\[\\Large H_{0}: \\rho_{XY}^2 \\neq 0\\]\nIt is possible to calculate the significance of your regression with a correlation test. In fact, it would seem quite practical and logical to do so."
  },
  {
    "objectID": "4-Basics.html#model-comparisons",
    "href": "4-Basics.html#model-comparisons",
    "title": "GLM basics III",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\nSo the omnibus \\(F\\)-statistic is the ratio of MSregression to MSresidual\nTest of overall significance. Does your model give a better fit to the data than a model that contains no independent variables?\nThis is truly a question of model comparisons!"
  },
  {
    "objectID": "4-Basics.html#full-vs.-restricted-models",
    "href": "4-Basics.html#full-vs.-restricted-models",
    "title": "GLM basics III",
    "section": "Full vs. Restricted Models",
    "text": "Full vs. Restricted Models\n\nfit.1 &lt;- lm(child ~ parent, data = galton.data)\nfit.0 &lt;- lm(child ~ 1, data = galton.data)\n\nsummary(fit.0)\n\n\nCall:\nlm(formula = child ~ 1, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3885 -1.8885  0.1115  2.1115  5.6115 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 68.08847    0.08266   823.8   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.518 on 927 degrees of freedom"
  },
  {
    "objectID": "4-Basics.html#the-comparison",
    "href": "4-Basics.html#the-comparison",
    "title": "GLM basics III",
    "section": "The comparison!",
    "text": "The comparison!\n\nanova(fit.1, fit.0)\n\nAnalysis of Variance Table\n\nModel 1: child ~ parent\nModel 2: child ~ 1\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    926 4640.3                                  \n2    927 5877.2 -1   -1236.9 246.84 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-1",
    "href": "4-Basics.html#model-comparisons-1",
    "title": "GLM basics III",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\nModel comparisons are redundant with nil/null hypotheses and coefficient tests right now, be they’ll be more flexible down the road\nKey is to start thinking about your implicit alternative models\nThe ultimate goal would be to create two models that represent two equally plausible theories\nTheory A is made up of components XYZ, whereas theory B has QRS components. You can then ask which theory (model) is better.\nA model embodies your hypothesis! It is the mathematical expression of your hypothesis!"
  },
  {
    "objectID": "4-Basics.html#regression-coefficient",
    "href": "4-Basics.html#regression-coefficient",
    "title": "GLM basics III",
    "section": "Regression coefficient",
    "text": "Regression coefficient\n\\[\\Large H_{0}: \\beta_{1}= 0\\] \\[\\Large H_{1}: \\beta_{1} \\neq 0\\]"
  },
  {
    "objectID": "4-Basics.html#what-does-the-regression-coefficient-test",
    "href": "4-Basics.html#what-does-the-regression-coefficient-test",
    "title": "GLM basics III",
    "section": "What does the regression coefficient test?",
    "text": "What does the regression coefficient test?\n\nDoes X provide any predictive information?\nDoes X provide any explanatory power regarding the variability of Y?\nIs the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)\nIs the regression line flat?\nAre X and Y correlated?"
  },
  {
    "objectID": "4-Basics.html#regression-coefficient-1",
    "href": "4-Basics.html#regression-coefficient-1",
    "title": "GLM basics III",
    "section": "Regression coefficient",
    "text": "Regression coefficient\n\\[\\Large se_{b} = \\frac{s_{Y}}{s_{X}}{\\sqrt{\\frac {1-r_{xy}^2}{n-2}}}\\] \\[\\Large t(n-2) = \\frac{b_{1}}{se_{b}}\\]"
  },
  {
    "objectID": "4-Basics.html#se_b",
    "href": "4-Basics.html#se_b",
    "title": "GLM basics III",
    "section": "SE_b",
    "text": "SE_b\n\nstandard errors for the slope coefficient\nrepresent our uncertainty (noise) in our estimate of the regression coefficient\ndifferent from residual standard error/deviation (but proportional to)\nmuch like previously we can take our estimate (b) and put confidence regions around it to get an estimate of what could be “possible” if we ran the study again"
  },
  {
    "objectID": "4-Basics.html#intercept",
    "href": "4-Basics.html#intercept",
    "title": "GLM basics III",
    "section": "Intercept",
    "text": "Intercept\n\nmore complex standard error calculation as the calculation depends on how far the X value (here zero) is away from the mean of X\n\nfarther from the mean, less information, thus more uncertainty"
  },
  {
    "objectID": "4-Basics.html#confidence-interval-for-coefficents",
    "href": "4-Basics.html#confidence-interval-for-coefficents",
    "title": "GLM basics III",
    "section": "Confidence interval for coefficents",
    "text": "Confidence interval for coefficents\n\nSame equation as we’ve been working with\nEstimate plus minus 1.96*se"
  },
  {
    "objectID": "4-Basics.html#confidence-bands-for-regression-line",
    "href": "4-Basics.html#confidence-bands-for-regression-line",
    "title": "GLM basics III",
    "section": "Confidence bands for regression line",
    "text": "Confidence bands for regression line\n\n\nCode\nset.seed(123)\npx.1 &lt;- rnorm(1000, 0, 1)\npe.1 &lt;- rnorm(1000, 0, 1)\npy.1 &lt;- .5 + .55 * px.1 + pe.1\npd.1 &lt;- data.frame(px.1,py.1)\npx.2 &lt;- rnorm(100, 0, 1)\npe.2 &lt;- rnorm(100, 0, 1)\npy.2 &lt;- .5 + .55 * px.2 + pe.2\npd.2 &lt;- data.frame(px.2,py.2)\np1 &lt;- ggplot(pd.1, aes(x = px.1,y =  py.1)) +\n    geom_point() +    \n    geom_smooth(method = lm) +\n   scale_x_continuous(limits = c(-3, 3))  +\n  scale_y_continuous(limits = c(-3, 3))\np2 &lt;- ggplot(pd.2, aes(x=px.2, y=py.2)) +\n    geom_point() +    \n    geom_smooth(method=lm) +\n  scale_x_continuous(limits = c(-3, 3))  +\n  scale_y_continuous(limits = c(-3, 3))\nlibrary(cowplot)\nplot_grid(p1, p2, ncol=2, labels = c(\"N = 1000\", \"N = 100\"))"
  },
  {
    "objectID": "4-Basics.html#confidence-bands",
    "href": "4-Basics.html#confidence-bands",
    "title": "GLM basics III",
    "section": "Confidence Bands",
    "text": "Confidence Bands\n\\[\\hat{Y}\\pm t_{critical} * se_{residual}*\\sqrt{\\frac {1}{n}+\\frac{(X-\\bar{X})^2}{(n-1)s_{X}^2}}\\]"
  },
  {
    "objectID": "4-Basics.html#prediction-band",
    "href": "4-Basics.html#prediction-band",
    "title": "GLM basics III",
    "section": "Prediction band",
    "text": "Prediction band\n\nWe are predicting and individual i’s score, not the \\(\\hat{Y}\\) for a particular level of \\(X\\). (A new \\(Y_i\\) given \\(X\\), rather than \\(\\bar{Y}\\) given \\(X\\) )\nBecause there is greater variation in predicting an individual value rather than a collection of individual values (i.e., the mean) the prediction band is greater\nCombines unknown variability in 1) the estimated mean (as reflected in se of b) 2) peoples scores around mean (residual standard error)\n\n\\[\\hat{Y}\\pm t_{critical} * se_{residual}*\\sqrt{1+ \\frac {1}{n}+\\frac{(X-\\bar{X})^2}{(n-1)s_{X}^2}}\\]"
  },
  {
    "objectID": "4-Basics.html#regression-equation",
    "href": "4-Basics.html#regression-equation",
    "title": "GLM basics III",
    "section": "Regression equation",
    "text": "Regression equation\n\\[\\large \\hat{Y} = b_0 + b_1X_1 + b_2X_2 + \\dots+b_kX_k\\]\n\nregression coefficients are “partial” regression coefficients\n\npredicted change in \\(Y\\) for a 1 unit change in \\(X\\), holding all other predictors constant\nsimilar to semi-partial correlation – represents part of each \\(X\\)"
  },
  {
    "objectID": "4-Basics.html#interpretting-multiple-regression-model",
    "href": "4-Basics.html#interpretting-multiple-regression-model",
    "title": "GLM basics III",
    "section": "Interpretting multiple regression model",
    "text": "Interpretting multiple regression model\n\\[\\large \\hat{Y} = b_0 + b_1X_1 + b_2X_2 + \\dots+b_kX_k\\]\n\nIntercept is the value of \\(Y\\) when all predictors = 0\nRegression coefficients are the predicted change in \\(Y\\) for a 1 unit change in \\(X\\), holding all other predictors constant"
  },
  {
    "objectID": "4-Basics.html#example-2",
    "href": "4-Basics.html#example-2",
    "title": "GLM basics III",
    "section": "Example",
    "text": "Example\n\n\nCode\nlibrary(here)\nstress.data = read.csv(here(\"stress.csv\"))\nlibrary(psych)\ndescribe(stress.data$Stress)\n\n\n   vars   n mean   sd median trimmed  mad  min   max range skew kurtosis   se\nX1    1 118 5.18 1.88   5.27    5.17 1.65 0.62 10.32  9.71 0.08     0.22 0.17"
  },
  {
    "objectID": "4-Basics.html#example-3",
    "href": "4-Basics.html#example-3",
    "title": "GLM basics III",
    "section": "Example",
    "text": "Example\n\n\nCode\nmr.model &lt;- lm(Stress ~ Support + Anxiety, data = stress.data)\ntidy(mr.model)\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.316    0.856     -0.369 7.13e- 1\n2 Support        0.406    0.0512     7.94  1.49e-12\n3 Anxiety        0.256    0.0674     3.80  2.34e- 4\n\n\n???\nIf a univariate regression is estimating the best-fit line, what is this estimating?"
  },
  {
    "objectID": "4-Basics.html#visualizing-multiple-regression",
    "href": "4-Basics.html#visualizing-multiple-regression",
    "title": "GLM basics III",
    "section": "Visualizing multiple regression",
    "text": "Visualizing multiple regression\n\n\nCode\nlibrary(visreg)\n\nvisreg2d(mr.model,\"Support\", \"Anxiety\", plot.type = \"persp\")"
  },
  {
    "objectID": "4-Basics.html#calculating-coefficients",
    "href": "4-Basics.html#calculating-coefficients",
    "title": "GLM basics III",
    "section": "Calculating coefficients",
    "text": "Calculating coefficients\nJust like with univariate regression, we calculate the OLS solution. As a reminder, this calculation will yield the estimate that reduces the sum of the squared deviations from the line:\n\\[\\large \\hat{Y} = b_0 + b_{1}X1 + b_{2}X_2\\] \\[\\large \\text{minimize} \\sum (Y-\\hat{Y})^2 \\]"
  },
  {
    "objectID": "4-Basics.html#calculating-the-standardized-partial-regression-coefficient",
    "href": "4-Basics.html#calculating-the-standardized-partial-regression-coefficient",
    "title": "GLM basics III",
    "section": "Calculating the standardized partial regression coefficient",
    "text": "Calculating the standardized partial regression coefficient\n\\[b_{1}^* = \\frac{r_{Y1}-r_{Y2}r_{12}}{1-r_{12}^2}\\]\n\\[b_{2}^* = \\frac{r_{Y2}-r_{Y1}r_{12}}{1-r_{12}^2}\\]"
  },
  {
    "objectID": "4-Basics.html#original-metric",
    "href": "4-Basics.html#original-metric",
    "title": "GLM basics III",
    "section": "Original Metric",
    "text": "Original Metric\n\\[b_{1} = b_{1}^*\\frac{s_{Y}}{s_{X1}}\\]\n\\[b_{1}^* = b_{1}\\frac{s_{X1}}{s_{Y}}\\]\n\\[b_{0} = \\bar{Y} - b_{1}\\bar{X_{1}} - b_{2}\\bar{X_{2}}\\]"
  },
  {
    "objectID": "4-Basics.html#controlling-for",
    "href": "4-Basics.html#controlling-for",
    "title": "GLM basics III",
    "section": "“Controlling for”",
    "text": "“Controlling for”\n\nTaken from @nickchk"
  },
  {
    "objectID": "4-Basics.html#estimating-model-fit",
    "href": "4-Basics.html#estimating-model-fit",
    "title": "GLM basics III",
    "section": "Estimating model fit",
    "text": "Estimating model fit\n\n\nCode\nmr.model &lt;- lm(Stress ~ Support + Anxiety, data = stress.data)\nsummary(mr.model)\n\n\n\nCall:\nlm(formula = Stress ~ Support + Anxiety, data = stress.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1958 -0.8994 -0.1370  0.9990  3.6995 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.31587    0.85596  -0.369 0.712792    \nSupport      0.40618    0.05115   7.941 1.49e-12 ***\nAnxiety      0.25609    0.06740   3.799 0.000234 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.519 on 115 degrees of freedom\nMultiple R-squared:  0.3556,    Adjusted R-squared:  0.3444 \nF-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11"
  },
  {
    "objectID": "4-Basics.html#multiple-correlation-r",
    "href": "4-Basics.html#multiple-correlation-r",
    "title": "GLM basics III",
    "section": "Multiple correlation, R",
    "text": "Multiple correlation, R\n\\[\\large \\hat{Y} = b_{0} + b_{1}X_{1} + b_{2}X_{2}\\]\n\n\\(\\hat{Y}\\) is a linear combination of Xs\n\\(r_{Y\\hat{Y}}\\) = multiple correlation = R\n\n–\n\\[\\large R = \\sqrt{b_{1}^*r_{Y1} + b_{2}^*r_{Y2}}\\] \\[\\large R^2 = {b_{1}^*r_{Y1} + b_{2}^*r_{Y2}}\\]"
  },
  {
    "objectID": "4-Basics.html#significance-tests",
    "href": "4-Basics.html#significance-tests",
    "title": "GLM basics III",
    "section": "significance tests",
    "text": "significance tests\n\n\\(R^2\\) (omnibus)\n\nRegression Coefficients\n\nIncrements to \\(R^2\\)"
  },
  {
    "objectID": "4-Basics.html#r-squared-r2",
    "href": "4-Basics.html#r-squared-r2",
    "title": "GLM basics III",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\nSame interpretation as before\nAdding predictors into your model will increase \\(R^2\\) – regardless of whether or not the predictor is significantly correlated with Y.\nAdjusted/Shrunken \\(R^2\\) takes into account the number of predictors in your model"
  },
  {
    "objectID": "4-Basics.html#adjusted-r-squared-textadj-r2",
    "href": "4-Basics.html#adjusted-r-squared-textadj-r2",
    "title": "GLM basics III",
    "section": "Adjusted R-squared, \\(\\text{Adj} R^2\\)",
    "text": "Adjusted R-squared, \\(\\text{Adj} R^2\\)\n\\[\\large R_{A}^2 = 1 - \\frac{Var_{res}}{Var_{total}}\\]\n\\[\\large R_{A}^2 = 1 - \\frac{\\frac{SS_{res}}{n-p-1}}{\\frac{SS_{total}}{n-1}}\\] \\[\\large R_{A}^2 = 1 - (1 -R^2)\\frac{n-1}{n-p-1}\\]"
  },
  {
    "objectID": "4-Basics.html#adjusted-r-squared-textadj-r2-1",
    "href": "4-Basics.html#adjusted-r-squared-textadj-r2-1",
    "title": "GLM basics III",
    "section": "Adjusted R-squared, \\(\\text{Adj} R^2\\)",
    "text": "Adjusted R-squared, \\(\\text{Adj} R^2\\)\n\\[\\large R_{A}^2 = 1 - (1 -R^2)\\frac{n-1}{n-p-1}\\] - What happens if you add many IV’s to your model that are uncorrelated with your DV?\n\nWhat happens as you add more covariates to your model that are highly correlated with your key predictor, X?\n\n\\[b_{1}^* = \\frac{r_{Y1}-r_{Y2}r_{12}}{1-r_{12}^2}\\]"
  },
  {
    "objectID": "4-Basics.html#anova",
    "href": "4-Basics.html#anova",
    "title": "GLM basics III",
    "section": "ANOVA",
    "text": "ANOVA\n\n\nCode\nsummary(mr.model)\n\n\n\nCall:\nlm(formula = Stress ~ Support + Anxiety, data = stress.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1958 -0.8994 -0.1370  0.9990  3.6995 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.31587    0.85596  -0.369 0.712792    \nSupport      0.40618    0.05115   7.941 1.49e-12 ***\nAnxiety      0.25609    0.06740   3.799 0.000234 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.519 on 115 degrees of freedom\nMultiple R-squared:  0.3556,    Adjusted R-squared:  0.3444 \nF-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11"
  },
  {
    "objectID": "4-Basics.html#anova-1",
    "href": "4-Basics.html#anova-1",
    "title": "GLM basics III",
    "section": "ANOVA",
    "text": "ANOVA\n\n\nCode\nanova(mr.model)\n\n\nAnalysis of Variance Table\n\nResponse: Stress\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nSupport     1 113.151 113.151  49.028 1.807e-10 ***\nAnxiety     1  33.314  33.314  14.435 0.0002336 ***\nResiduals 115 265.407   2.308                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#test-of-individual-regression-coefficients",
    "href": "4-Basics.html#test-of-individual-regression-coefficients",
    "title": "GLM basics III",
    "section": "Test of individual regression coefficients",
    "text": "Test of individual regression coefficients\n\\[\\Large H_{0}: \\beta_{X}= 0\\] \\[\\Large H_{1}: \\beta_{X} \\neq 0\\]"
  },
  {
    "objectID": "4-Basics.html#test-of-individual-regression-coefficients-1",
    "href": "4-Basics.html#test-of-individual-regression-coefficients-1",
    "title": "GLM basics III",
    "section": "Test of individual regression coefficients",
    "text": "Test of individual regression coefficients\nIn the case of univariate regression:\n\\[\\Large se_{b} = \\frac{s_{Y}}{s_{X}}{\\sqrt{\\frac {1-r_{xy}^2}{n-2}}}\\]\nIn the case of multiple regression:\n\\[\\Large se_{b} = \\frac{s_{Y}}{s_{X}}{\\sqrt{\\frac {1-R_{Y\\hat{Y}}^2}{n-p-1}}} \\sqrt{\\frac {1}{1-R_{i.jkl...p}^2}}\\]\n\nAs N increases…\nAs variance explained increases…"
  },
  {
    "objectID": "4-Basics.html#can-you",
    "href": "4-Basics.html#can-you",
    "title": "GLM basics III",
    "section": "Can you…",
    "text": "Can you…\n\nwrite out standardized and unstandardized regression equations?\ninterpret the coefficients of a multiple regression?\ndraw comparisons from ANOVA and regression?\ncalculate \\(R^2\\)?"
  },
  {
    "objectID": "4-Basics.html#warm-up",
    "href": "4-Basics.html#warm-up",
    "title": "GLM basics III",
    "section": "Warm Up",
    "text": "Warm Up\n\n\n\nCall:\nlm(formula = Weight ~ Age + Poverty, data = nhanes)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47.54 -19.90   0.82  16.96  65.53 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  58.8926     7.0353   8.371 8.69e-13 ***\nAge           0.3537     0.1310   2.699  0.00835 ** \nPoverty      -0.3501     1.5890  -0.220  0.82612    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.93 on 87 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.07949,   Adjusted R-squared:  0.05833 \nF-statistic: 3.756 on 2 and 87 DF,  p-value: 0.02724\n\n\n\nInterpret all coefficients\nInterpret all significance tests of coefficients\nIs it a good model?"
  },
  {
    "objectID": "4-Basics.html#which-variables-to-include",
    "href": "4-Basics.html#which-variables-to-include",
    "title": "GLM basics III",
    "section": "Which variables to include",
    "text": "Which variables to include\n\nYour goal should be to match the population model (theoretically)\nIncluding many variables will not bias parameter estimates but will potentially increase degrees of freedom and standard errors; in other words, putting too many variables in your model may make it more difficult to find a statistically significant result\nBut that’s only the case if you add variables unrelated to Y or X; there are some cases in which adding the wrong variables can lead to spurious results. [Stay tuned for the lecture on causal models.]"
  },
  {
    "objectID": "4-Basics.html#hierarchical-regression-model-comparison",
    "href": "4-Basics.html#hierarchical-regression-model-comparison",
    "title": "GLM basics III",
    "section": "Hierarchical regression / Model Comparison",
    "text": "Hierarchical regression / Model Comparison\nModel comparison: Comparing how well two (or more) models fit the data in order to determine which model is better.\nIf we’re comparing nested models by incrementally adding or subtracting variables, this is known as hierarchical regression.\n\nMultiple models are calculated\nEach predictor (or set of predictors) is assessed in terms of what it adds (in terms of variance explained) at the time it is entered\nOrder is dependent on an a priori hypothesis"
  },
  {
    "objectID": "4-Basics.html#r-square-change",
    "href": "4-Basics.html#r-square-change",
    "title": "GLM basics III",
    "section": "R-square change",
    "text": "R-square change\n\ndistributed as an F \\[F(p.new, N - 1 - p.all) = \\frac {R_{m.2}^2- R_{m.1}^2} {1-R_{m.2}^2} (\\frac {N-1-p.all}{p.new})\\]\ncan also be written in terms of SSresiduals"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-2",
    "href": "4-Basics.html#model-comparisons-2",
    "title": "GLM basics III",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nm.1 &lt;- lm(Stress ~ Support, data = stress.data)\nm.2 &lt;- lm(Stress ~ Support + Anxiety, data = stress.data)\nanova(m.1, m.2)\n\nAnalysis of Variance Table\n\nModel 1: Stress ~ Support\nModel 2: Stress ~ Support + Anxiety\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    116 298.72                                  \n2    115 265.41  1    33.314 14.435 0.0002336 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-3",
    "href": "4-Basics.html#model-comparisons-3",
    "title": "GLM basics III",
    "section": "model comparisons",
    "text": "model comparisons\n\nanova(m.1)\n\nAnalysis of Variance Table\n\nResponse: Stress\n           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nSupport     1 113.15 113.151  43.939 1.12e-09 ***\nResiduals 116 298.72   2.575                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-4",
    "href": "4-Basics.html#model-comparisons-4",
    "title": "GLM basics III",
    "section": "model comparisons",
    "text": "model comparisons\n\nanova(m.2)\n\nAnalysis of Variance Table\n\nResponse: Stress\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nSupport     1 113.151 113.151  49.028 1.807e-10 ***\nAnxiety     1  33.314  33.314  14.435 0.0002336 ***\nResiduals 115 265.407   2.308                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-5",
    "href": "4-Basics.html#model-comparisons-5",
    "title": "GLM basics III",
    "section": "model comparisons",
    "text": "model comparisons\n\n\nCode\nsummary(m.2)\n\n\n\nCall:\nlm(formula = Stress ~ Support + Anxiety, data = stress.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1958 -0.8994 -0.1370  0.9990  3.6995 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.31587    0.85596  -0.369 0.712792    \nSupport      0.40618    0.05115   7.941 1.49e-12 ***\nAnxiety      0.25609    0.06740   3.799 0.000234 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.519 on 115 degrees of freedom\nMultiple R-squared:  0.3556,    Adjusted R-squared:  0.3444 \nF-statistic: 31.73 on 2 and 115 DF,  p-value: 1.062e-11"
  },
  {
    "objectID": "4-Basics.html#model-comparisons-6",
    "href": "4-Basics.html#model-comparisons-6",
    "title": "GLM basics III",
    "section": "model comparisons",
    "text": "model comparisons\n\n\n\nCall:\nlm(formula = Stress ~ Support, data = stress.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8215 -1.2145 -0.1796  1.0806  3.4326 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.56046    0.42189   6.069 1.66e-08 ***\nSupport      0.30006    0.04527   6.629 1.12e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.605 on 116 degrees of freedom\nMultiple R-squared:  0.2747,    Adjusted R-squared:  0.2685 \nF-statistic: 43.94 on 1 and 116 DF,  p-value: 1.12e-09"
  },
  {
    "objectID": "2-Basics.html",
    "href": "2-Basics.html",
    "title": "GLM basics",
    "section": "",
    "text": "Our DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (here forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "3-Basics.html",
    "href": "3-Basics.html",
    "title": "GLM basics II",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nWhat is regression and why is it useful?\nNuts and bolts\n\nEquation\nOrdinary least squares\nInterpretation"
  },
  {
    "objectID": "3-HW.html",
    "href": "3-HW.html",
    "title": "Homework 3",
    "section": "",
    "text": "Data: You will use the same Human Connectome Project dataset you worked with in the R class and this semester’s R refresher. It is available here\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "3-HW.html#question-1",
    "href": "3-HW.html#question-1",
    "title": "Homework 3",
    "section": "Question 1",
    "text": "Question 1\nRun a univariate regression model where Mean Purpose in Life is predicted by Life Satisfaction. Answer the following for this model:\n\nInterpret the coefficients\nDescribe what \\(R^2\\) means, and interpret this model’s \\(R^2\\)\nDescribe what \\(\\sigma\\) means, and interpret this model’s \\(\\sigma\\). Overall, is this a “good” model?\nIs Life Satisfaction a good predictor of Mean Purpose in Life? How do you know"
  },
  {
    "objectID": "3-HW.html#question-2",
    "href": "3-HW.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\nRepeat the same as above, but now use the NEO Agreeableness subscale as the predictor. Answer the same questions as above."
  },
  {
    "objectID": "3-HW.html#question-3",
    "href": "3-HW.html#question-3",
    "title": "Homework 3",
    "section": "Question 3",
    "text": "Question 3\nRun a third model where Mean Purpose in Life is predicted by both Life Satisfaction and the NEO Agreeableness subscale.\n\nInterpret all the coefficients\nIs this a good model? How do you know?"
  },
  {
    "objectID": "3-HW.html#question-4",
    "href": "3-HW.html#question-4",
    "title": "Homework 3",
    "section": "Question 4",
    "text": "Question 4\nGiven your model from Question 3, please do the following (note, you might need to create some more univariate models)\n\nCreate a measure of Mean Purpose that is independent of Life Satisfaction. Store this as an object. This will be your Y axis.\nCreate a measure of Life Satisfaction that is independent of Agreeableness. This will be your X axis.\nNow plot the above 2. Store this plot as it’s own object. Make sure to include a best fit line in your figure."
  },
  {
    "objectID": "3-HW.html#question-5",
    "href": "3-HW.html#question-5",
    "title": "Homework 3",
    "section": "Question 5",
    "text": "Question 5\nGiven your model from Question 3, please do the following (note, you might need to create some more univariate models)\n\nCreate a measure of Mean Purpose that is independent of Agreeableness. Store this as an object. This will be your Y axis.\nCreate a measure of Agreeableness that is independent of Life Satisfaction This will be your X axis.\nNow plot the above 2. Store this plot as it’s own object. Make sure to include a best fit line in your figure."
  },
  {
    "objectID": "3-HW.html#question-6",
    "href": "3-HW.html#question-6",
    "title": "Homework 3",
    "section": "Question 6",
    "text": "Question 6\n\nCreate a single figure where the plot from Question 4 is the first panel on the left, and the plot from Question 5 is the second panel on the right. You should wind up with a single figure.\nGive this figure the title “Added Variable Plot”. In your own words, try to describe what you did and why you did it. It’s OK to be confused! Just give it your best shot."
  },
  {
    "objectID": "4-HW.html",
    "href": "4-HW.html",
    "title": "Homework 4",
    "section": "",
    "text": "Please complete this assignment by creating a new RMarkdown (or Quarto) file. Be sure to include your name in the preamble. Please submit both your .Rmd (or .qmd) and your .html files to Canvas.\nData: These data were from a study where physical health was predicted by a happiness measure, socioeconomic status, number of close friends, and a 16-week mood intervention with a waitlist control group (coded 0), a positive self affirmation intervention group (coded 1), and a meditation group (coded 2). The data are here: HW4-data.csv"
  },
  {
    "objectID": "4-HW.html#instructions",
    "href": "4-HW.html#instructions",
    "title": "Homework 4",
    "section": "",
    "text": "Please complete this assignment by creating a new RMarkdown (or Quarto) file. Be sure to include your name in the preamble. Please submit both your .Rmd (or .qmd) and your .html files to Canvas.\nData: These data were from a study where physical health was predicted by a happiness measure, socioeconomic status, number of close friends, and a 16-week mood intervention with a waitlist control group (coded 0), a positive self affirmation intervention group (coded 1), and a meditation group (coded 2). The data are here: HW4-data.csv"
  },
  {
    "objectID": "4-HW.html#homework-questions",
    "href": "4-HW.html#homework-questions",
    "title": "Homework 4",
    "section": "Homework Questions",
    "text": "Homework Questions\n\nPerform a multiple regression where health is regressed on (predicted by) friends, SES, and happiness. Interpret each regression coefficient.\nGraph the (2-d) relationship between health and happiness as found in the above regression. Make sure the association controls for the remaining variables in the model. Include data points in your graph and a confidence band.\nDescribe why the confidence band looks like a bow tie in the above graph.\nFor the above regression, calculate the variance explained using the ANOVA table and interpret the residual standard error.\nAdd the group variable mood.group into the regression model from #2. Then answer the following:\n\nInterpret the coefficients\nIs there a significant increase in variance explained compared to the initial regression in #1? If so, what is the estimate?"
  },
  {
    "objectID": "5-interact.html#today",
    "href": "5-interact.html#today",
    "title": "Interactions",
    "section": "Today",
    "text": "Today\nInteractions/moderation"
  },
  {
    "objectID": "5-interact.html#what-are-interactions",
    "href": "5-interact.html#what-are-interactions",
    "title": "Interactions",
    "section": "What are interactions?",
    "text": "What are interactions?\nWhen we have two variables, A and B, in a regression model, we are testing whether these variables have additive effects on our outcome, Y. That is, the effect of A on Y is constant over all values of B.\n\nExample: Drinking coffee and hours of sleep have additive effects on alertness; no matter how any hours I slept the previous night, drinking one cup of coffee will make me .5 SD more awake than not drinking coffee."
  },
  {
    "objectID": "5-interact.html#what-are-interactions-1",
    "href": "5-interact.html#what-are-interactions-1",
    "title": "Interactions",
    "section": "What are interactions?",
    "text": "What are interactions?\nHowever, we may hypothesize that two variables have joint effects, or interact with each other. In this case, the effect of A on Y changes as a function of B.\n\nExample: Chronic stress has a negative impact on health but only for individuals who receive little or no social support; for individuals with high social support, chronic stress has no impact on health.\nThis is also referred to as moderation."
  },
  {
    "objectID": "5-interact.html#univariate-regression",
    "href": "5-interact.html#univariate-regression",
    "title": "Interactions",
    "section": "Univariate regression",
    "text": "Univariate regression\n\n\nCode\nlibrary(tidyverse)\nset.seed(022020)\nx = rnorm(n = 20)\nz = rnorm(n = 20)\ny  = x*z + rnorm(n = 20)\ndata.frame(x,y) %&gt;%\n  ggplot(aes(x,y)) + geom_point() + geom_smooth(method = 'lm', se = F) + theme_bw()"
  },
  {
    "objectID": "5-interact.html#multivariate-regression",
    "href": "5-interact.html#multivariate-regression",
    "title": "Interactions",
    "section": "Multivariate regression",
    "text": "Multivariate regression\n\n\nCode\nlibrary(visreg)\nx1 = x\nx2 = z\nmr.model = lm(y ~ x1 + x2)\nvisreg2d(mr.model,\"x1\", \"x2\", plot.type = \"persp\")"
  },
  {
    "objectID": "5-interact.html#multivariate-regression-with-an-interaction",
    "href": "5-interact.html#multivariate-regression-with-an-interaction",
    "title": "Interactions",
    "section": "Multivariate regression with an interaction",
    "text": "Multivariate regression with an interaction\n\n\nCode\nmr.model2 = lm(y ~ x1*x2)\nvisreg2d(mr.model2,\"x1\", \"x2\", plot.type = \"persp\")"
  },
  {
    "objectID": "5-interact.html#example",
    "href": "5-interact.html#example",
    "title": "Interactions",
    "section": "Example",
    "text": "Example\nWe shall return to the data we used in the multiple regression model with two continuous variables. Here we have an outcome (Stress) that we are interested in predicting from trait Anxiety and levels of Social Support.\n\n\nCode\nlibrary(here)\nstress.data = read.csv(here(\"stress.csv\"))\nlibrary(psych)\ndescribe(stress.data)\n\n\n        vars   n   mean     sd median trimmed    mad  min    max  range  skew\nid         1 118 488.65 295.95 462.50  485.76 372.13 2.00 986.00 984.00  0.10\nAnxiety    2 118   7.61   2.49   7.75    7.67   2.26 0.70  14.64  13.94 -0.18\nStress     3 118   5.18   1.88   5.27    5.17   1.65 0.62  10.32   9.71  0.08\nSupport    4 118   8.73   3.28   8.52    8.66   3.16 0.02  17.34  17.32  0.18\ngroup*     5 118   1.53   0.50   2.00    1.53   0.00 1.00   2.00   1.00 -0.10\n        kurtosis    se\nid         -1.29 27.24\nAnxiety     0.28  0.23\nStress      0.22  0.17\nSupport     0.19  0.30\ngroup*     -2.01  0.05"
  },
  {
    "objectID": "5-interact.html#conceptual-interpretation",
    "href": "5-interact.html#conceptual-interpretation",
    "title": "Interactions",
    "section": "Conceptual interpretation",
    "text": "Conceptual interpretation\n\\[\\hat{Y} = b_{0} + b_{1}X + b_{2}Z + b_{3}XZ\\]\nYou can interpret the interaction term in the same way you normally interpret a slope coefficient – this is the effect of the interaction controlling for other variables in the model.\nYou can also interpret the intercept the same way as before (the expected value of Y when all predictors are 0).\nBut here, \\(b_1\\) is the effect of X on Y when Z is equal to 0."
  },
  {
    "objectID": "5-interact.html#conceptual-interpretation-1",
    "href": "5-interact.html#conceptual-interpretation-1",
    "title": "Interactions",
    "section": "Conceptual interpretation",
    "text": "Conceptual interpretation\n\\[\\hat{Y} = b_{0} + b_{1}X + b_{2}Z + b_{3}XZ\\]\nLower-order terms change depending on the values of the higher-order terms. The value of \\(b_1\\) and \\(b_2\\) will change depending on the value of \\(b_3\\).\n\nThese values represent “conditional effects” (because the value is conditional on the level of the other variable). In many cases, the value and significance test with these terms is either meaningless (if Z is never equal to 0) or unhelpful, as these values and significance change across the data."
  },
  {
    "objectID": "5-interact.html#conceptual-interpretation-2",
    "href": "5-interact.html#conceptual-interpretation-2",
    "title": "Interactions",
    "section": "Conceptual interpretation",
    "text": "Conceptual interpretation\n\\[\\hat{Y} = b_{0} + b_{1}X + b_{2}Z + b_{3}XZ\\]\nHigher-order terms are those terms that represent interactions. \\(b_3\\) is a higher-order term. - This value represents how much the slope of X changes for every 1-unit increase in Z AND how much the slope of Z changes for everyone 1-unit increase in X.\n???\nIs \\(b_0\\) a higher-order or lower-order term?\nAsk what values \\(b_0\\) depends on – both intercept and slope. Maybe be helpful to use paper to represent plane."
  },
  {
    "objectID": "5-interact.html#conceptual-interpretation-3",
    "href": "5-interact.html#conceptual-interpretation-3",
    "title": "Interactions",
    "section": "Conceptual interpretation",
    "text": "Conceptual interpretation\nHigher-order interaction terms represent: - the change in the slope of X as a function of Z - the degree of curvature in the regression plane - the linear effect of the product of independent variables\n\n\nCode\nstress.data$AxS = stress.data$Anxiety*stress.data$Support\nhead(stress.data[,c(\"Anxiety\", \"Support\", \"AxS\")])\n\n\n   Anxiety Support       AxS\n1 10.18520  6.1602  62.74287\n2  5.58873  8.9069  49.77826\n3  6.58500 10.5433  69.42763\n4  8.95430 11.4605 102.62076\n5  7.59910  5.5516  42.18716\n6  8.15600  7.5117  61.26543"
  },
  {
    "objectID": "5-interact.html#conditional-effects-and-simple-slopes",
    "href": "5-interact.html#conditional-effects-and-simple-slopes",
    "title": "Interactions",
    "section": "Conditional effects and simple slopes",
    "text": "Conditional effects and simple slopes\nThe regression line estimated in this model is quite difficult to interpret on its own. A good strategy is to decompose the regression equation into simple slopes, which are determined by calculating the conditional effects at a specific level of the moderating variable.\n\nSimple slope: the equation for Y on X at different levels of Z; but also refers to only the coefficient for X in this equation\nConditional effect: the slope coefficients in the full regression model which can change. These are the lower-order terms associated with a variable. E.g., X has a conditional effect on Y.\n\nWhich variable is the “predictor” (X) and which is the “moderator” (Z)?"
  },
  {
    "objectID": "5-interact.html#interaction-shapes",
    "href": "5-interact.html#interaction-shapes",
    "title": "Interactions",
    "section": "Interaction shapes",
    "text": "Interaction shapes\nOften we graph the simple slopes as a way to understand the interaction. Interpreting the shape of an interaction can be done using the numbers alone, but it requires a lot of calculation and mental rotation. For those reasons, consider it a requirement that you graph interactions in order to interpret them."
  },
  {
    "objectID": "5-interact.html#ordinal-interactions",
    "href": "5-interact.html#ordinal-interactions",
    "title": "Interactions",
    "section": "Ordinal interactions",
    "text": "Ordinal interactions\n\n\nCode\nx = 1:10\ny1 = .7 + .9*x\ny2 = .6 + .2*x\ny3 = .5 - .4*x\n\ndata.frame(x = rep(x, 3), y = c(y1,y2,y3), group = rep(c(5, 3, 1),each = 10)) %&gt;%\n  ggplot(aes(x,y,color = as.factor(group))) +\n  geom_line(size = 2) +\n  labs(x = \"X\", y = \"Y\", color = \"Z\") +\n  scale_x_continuous(labels = NULL) +\n  scale_y_continuous(labels = NULL, limit = c(-5, 15)) + cowplot::theme_cowplot(font_size = 20)"
  },
  {
    "objectID": "5-interact.html#cross-over-disordinal-interactions",
    "href": "5-interact.html#cross-over-disordinal-interactions",
    "title": "Interactions",
    "section": "Cross-over (disordinal) interactions",
    "text": "Cross-over (disordinal) interactions\n\n\nCode\nx = 1:10\ny1 = -2 + .9*x\ny2 = 1.6 - .2*x\n\n\ndata.frame(x = rep(x, 2), y = c(y1,y2), group = rep(c(5, 3),each = 10)) %&gt;%\n  ggplot(aes(x,y,color = as.factor(group))) +\n  geom_line(size = 2) +\n  labs(x = \"X\", y = \"Y\", color = \"Z\") +\n  scale_x_continuous(labels = NULL) +\n  scale_y_continuous(labels = NULL, limit = c(-5, 10)) + cowplot::theme_cowplot(font_size = 20)"
  },
  {
    "objectID": "5-interact.html#pop-quiz",
    "href": "5-interact.html#pop-quiz",
    "title": "Interactions",
    "section": "Pop Quiz",
    "text": "Pop Quiz\nYou are interested in the effects of “brain games” (or cognitive training) and sleep on the development of Alzheimer’s disease. You hypothesize that cognitive training might aid in slowing the progression of AD if participants are well rested.\n\nWrite out the regression equation. Which variable is the moderator?\nInterpret the coefficients.\nSketch out a plot of the simple slopes. There are different ways of doing this reflecting different hypotheses, but make sure the hypothesis stated above is somehow shown. You can do this on a post-it/scratch paper (I don’t have real data for this, so there’s nothing to make in R unless you want to simulate your own data…but that’s a bit much for a 0-point pop quiz)"
  },
  {
    "objectID": "5-interact.html#simple-slopes---significance-tests",
    "href": "5-interact.html#simple-slopes---significance-tests",
    "title": "Interactions",
    "section": "Simple slopes - Significance tests",
    "text": "Simple slopes - Significance tests\nRecall that the slope at any particular value is a combination of both \\(b_1\\) and \\(b_3\\) \\[\\large \\hat{Y} = (b_{0} + b_{2}Z) + (b_{1} + b_{3}Z)X\\] \\[\\large se_{b@z} = \\sqrt{se_{b_1}^2 + (2 * Z * cov_{b_1b_3})+ (Z^2 se_{b_3}^2)}\\] In this formula, \\(cov_{b_1b_3}\\) refers to the covariance of the coefficients, not the covariance of the variables. This may seem a strange concept, as we only ever have one value for \\(b_1\\) and \\(b_3\\) – the covariance of these coefficients refer to idea that if we randomly sample from a population, estimate the coefficients each time, and then examine the covariance of coefficients across random samples, it will not be 0."
  },
  {
    "objectID": "5-interact.html#simple-slopes---significance-tests-1",
    "href": "5-interact.html#simple-slopes---significance-tests-1",
    "title": "Interactions",
    "section": "Simple slopes - Significance tests",
    "text": "Simple slopes - Significance tests\n\nround(vcov(imodel),4)\n\n                (Intercept) Anxiety Support Anxiety:Support\n(Intercept)          1.2567 -0.1330 -0.0966          0.0100\nAnxiety             -0.1330  0.0169  0.0110         -0.0015\nSupport             -0.0966  0.0110  0.0091         -0.0011\nAnxiety:Support      0.0100 -0.0015 -0.0011          0.0002"
  },
  {
    "objectID": "5-interact.html#example-1",
    "href": "5-interact.html#example-1",
    "title": "Interactions",
    "section": "Example",
    "text": "Example\n\\[\\hat{Stress} = -2.74 + 0.62(\\text{Anx})+ 0.67(\\text{Sup})+ -0.04(\\text{Anx} \\times \\text{Sup})\\]\nWe want to know whether anxiety is a significant predictor of stress at different levels of support.\n\n\nCode\nlibrary(reghelper)\nsimple_slopes(imodel, levels = list(Support = c(4,6,8,10,12)))\n\n\n  Anxiety Support Test Estimate Std. Error t value  df  Pr(&gt;|t|) Sig.\n1  sstest       4        0.4486     0.0886  5.0617 114 1.610e-06  ***\n2  sstest       6        0.3652     0.0733  4.9791 114 2.289e-06  ***\n3  sstest       8        0.2817     0.0654  4.3095 114 3.488e-05  ***\n4  sstest      10        0.1982     0.0674  2.9424 114  0.003946   **\n5  sstest      12        0.1147     0.0786  1.4600 114  0.147036     \n\n\nIf you don’t list levels, then this function will test simple slopes at the mean and 1 SD above and below the mean."
  },
  {
    "objectID": "5-interact.html#centering",
    "href": "5-interact.html#centering",
    "title": "Interactions",
    "section": "Centering",
    "text": "Centering\nThe regression equation built using the raw data is not only difficult to interpret, but often the terms displayed are not relevant to the hypotheses we’re interested. - \\(b_0\\) is the expected value when all predictors are 0, but this may never happen in real life - \\(b_1\\) is the slope of X when Z is equal to 0, but this may not ever happen either.\nCentering your variables by subtracting the mean from all values can improve the interpretation of your results. - Remember, a linear transformation does not change associations (correlations) between variables. In this case, it only changes the interpretation for some coefficients"
  },
  {
    "objectID": "5-interact.html#standardized-regression-equation",
    "href": "5-interact.html#standardized-regression-equation",
    "title": "Interactions",
    "section": "Standardized regression equation",
    "text": "Standardized regression equation\nSo far, we’ve only discussed the unstandardized regression equation. If you’re interested in getting the standardized regression equation, you can follow the same procedure of standardizing your variables first and then entering them into your linear model.\nAn important note: You must take the product of the Z-scores, not the Z-score of the products to get the correct regression model."
  },
  {
    "objectID": "5-interact.html#standardized-regression-equation-1",
    "href": "5-interact.html#standardized-regression-equation-1",
    "title": "Interactions",
    "section": "Standardized regression equation",
    "text": "Standardized regression equation\nThis is okay:\n\nY ~ z(X) + z(Z) + z(X)*z(Z) \n\nY ~ z(X)*z(Z) \n\nThis is not OK:\n\nY ~ z(X) + z(Z) + z(X*Z)"
  },
  {
    "objectID": "6-interact.html#mixing-categorical-and-continuous",
    "href": "6-interact.html#mixing-categorical-and-continuous",
    "title": "Interactions II",
    "section": "Mixing categorical and continuous",
    "text": "Mixing categorical and continuous\nConsider the case where D is a variable representing two groups. In a univariate regression, how do we interpret the coefficient for D?\n\\[\\hat{Y} = b_{0} + b_{1}D\\]"
  },
  {
    "objectID": "6-interact.html#interpreting-slopes",
    "href": "6-interact.html#interpreting-slopes",
    "title": "Interactions II",
    "section": "Interpreting slopes",
    "text": "Interpreting slopes\nExtending this to the multivariate case, where X is continuous and D is a dummy code representing two groups.\n\\[\\hat{Y} = b_{0} + b_{1}D + b_2X\\]\nHow do we interpret \\(b_1?\\)"
  },
  {
    "objectID": "6-interact.html#visualizing",
    "href": "6-interact.html#visualizing",
    "title": "Interactions II",
    "section": "Visualizing",
    "text": "Visualizing\n\n\nCode\nset.seed(022520)\nD = rep(c(0,1), each = 10)\nX = rnorm(20) + D\nY = 2*D + X + rnorm(20)\n\ndf = data.frame(X,Y,D)\n\nmeans = df %&gt;%\n  group_by(D) %&gt;%\n  summarize(M = mean(Y))\nmod = lm(Y ~ X + D, data = df)\ndf$pmod = predict(mod)\n\npredict.2 = data.frame(X = rep(mean(X)+.1,2), D = c(0,1))\npredict.2$Y = predict(mod, newdata = predict.2) \npredict.2 = cbind(predict.2[1,], predict.2[2,])\nnames(predict.2) = c(\"x1\", \"d1\", \"y1\", \"x2\", \"d2\", \"y2\")\n\nggplot(df, aes(X,Y, color = as.factor(D))) +\n  geom_point(size = 3) +\n  geom_smooth(aes(y = pmod), method = \"lm\", se = F)+\n  labs(color = \"D\") +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = predict.2, \n               inherit.aes = F, size = 1.5)+\n  cowplot::theme_cowplot()"
  },
  {
    "objectID": "6-interact.html#visualizing-1",
    "href": "6-interact.html#visualizing-1",
    "title": "Interactions II",
    "section": "Visualizing",
    "text": "Visualizing"
  },
  {
    "objectID": "6-interact.html#interactions",
    "href": "6-interact.html#interactions",
    "title": "Interactions II",
    "section": "Interactions",
    "text": "Interactions\nNow extend this example to include joint effects, not just additive effects:\n\\[\\hat{Y} = b_{0} + b_{1}D + b_2X + b_3DX\\]\nHow do we interpret \\(b_1?\\), \\(b_2\\), \\(b_3?\\)?"
  },
  {
    "objectID": "6-interact.html#visualizing-2",
    "href": "6-interact.html#visualizing-2",
    "title": "Interactions II",
    "section": "Visualizing",
    "text": "Visualizing\n\n\nCode\nggplot(df, aes(X,Y, color = as.factor(D))) +\n  geom_point(size = 3) +\n  geom_smooth(method = \"lm\", se = F)+\n  labs(color = \"D\") +\n  cowplot::theme_cowplot()"
  },
  {
    "objectID": "6-interact.html#example",
    "href": "6-interact.html#example",
    "title": "Interactions II",
    "section": "Example",
    "text": "Example\nWash U is interested in understanding how undergraduates’ academic performance and choice of major impacts their career success. They contact 150 alumni between the ages of 25 and 35 and collect their current salary (in thousands of dollars), their primary undergraduate major, and their GPA upon graduating.\n\n\nCode\nlibrary(psych)\ntable(inc_data$major)\n\n\n\n   Econ English   Psych \n     50      50      50 \n\n\nCode\ndescribe(inc_data[,c(\"gpa\", \"income\")], fast = T)\n\n\n       vars   n  mean   sd   min    max  range   se\ngpa       1 150  3.36  0.4  2.44   4.19   1.74 0.03\nincome    2 150 84.35 34.0 24.67 160.27 135.60 2.78"
  },
  {
    "objectID": "6-interact.html#pop-quiz",
    "href": "6-interact.html#pop-quiz",
    "title": "Interactions II",
    "section": "Pop Quiz",
    "text": "Pop Quiz\nFor the model just defined in the previous slide…\n\nWrite out the regression equation\nInterpret each term\nWhere is Econ?"
  },
  {
    "objectID": "6-interact.html#model-summary-centering-predictors",
    "href": "6-interact.html#model-summary-centering-predictors",
    "title": "Interactions II",
    "section": "Model summary: centering predictors",
    "text": "Model summary: centering predictors\n\ninc_data$gpa_c = inc_data$gpa - mean(inc_data$gpa)\ncareer.mod_c = lm(income ~ gpa_c*major, data = inc_data)\nsummary(career.mod_c)\n\n\nCall:\nlm(formula = income ~ gpa_c * major, data = inc_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.625 -11.869   0.376   9.301  40.942 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         141.428      3.752  37.691  &lt; 2e-16 ***\ngpa_c                59.660      7.705   7.743 1.58e-12 ***\nmajorEnglish        -97.086      4.907 -19.783  &lt; 2e-16 ***\nmajorPsych          -75.965      4.384 -17.327  &lt; 2e-16 ***\ngpa_c:majorEnglish   -4.562     11.089  -0.411   0.6814    \ngpa_c:majorPsych     29.545     10.949   2.698   0.0078 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.91 on 144 degrees of freedom\nMultiple R-squared:  0.8142,    Adjusted R-squared:  0.8077 \nF-statistic: 126.2 on 5 and 144 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6-interact.html#plotting-results",
    "href": "6-interact.html#plotting-results",
    "title": "Interactions II",
    "section": "Plotting results",
    "text": "Plotting results\n\n\nCode\nlibrary(ggeffects)\npredictedvals = ggpredict(model = career.mod_c, terms = c(\"gpa_c\", \"major\"))\n\nggplot(data = predictedvals, aes(x = x, y = predicted, group = group)) +\n  geom_smooth(aes(ymin = conf.low,\n                  ymax = conf.high,\n                  color = group,\n                  fill = group),\n              stat = \"identity\",\n              alpha = .2) +\n  labs(x = \"GPA\",\n       y = \"Income (in thousands of dollars)\",\n       title = \"I am in the wrong profession\",\n       color = \"Undergraduate Major\",\n       fill = \"Undergraduate Major\",\n       group = \"Undergraduate Major\")"
  },
  {
    "objectID": "6-interact.html#two-categorical-predictors",
    "href": "6-interact.html#two-categorical-predictors",
    "title": "Interactions Two",
    "section": "Two categorical predictors",
    "text": "Two categorical predictors\nIf both X and M are categorical variables, the interpretation of coefficients is no longer the value of means and slopes, but means and differences in means.\n\n\nCode\nlibrary(here)\nsolomon = read.csv(here(\"solomon.csv\"))\n\n\n\n\nCode\nhead(solomon[,c(\"PERSPECTIVE\", \"DISTANCE\", \"WISDOM\")])\n\n\n  PERSPECTIVE  DISTANCE      WISDOM\n1       other  immersed -0.27589395\n2       other distanced  0.42949213\n3       other distanced -0.02785874\n4       other distanced  0.53271500\n5        self distanced  0.62299793\n6        self distanced -1.99578129\n\n\nCode\nsolomon$PERSPECTIVE = factor(solomon$PERSPECTIVE)\nsolomon$DISTANCE = factor(solomon$DISTANCE)"
  },
  {
    "objectID": "6-interact.html#model-means",
    "href": "6-interact.html#model-means",
    "title": "Interactions Two",
    "section": "Model Means",
    "text": "Model Means\n\nsolomon %&gt;% \n  group_by(DISTANCE, PERSPECTIVE) %&gt;% \n  summarize(meanWISDOM = mean(WISDOM, na.rm = TRUE))\n\n# A tibble: 4 × 3\n# Groups:   DISTANCE [2]\n  DISTANCE  PERSPECTIVE meanWISDOM\n  &lt;fct&gt;     &lt;fct&gt;            &lt;dbl&gt;\n1 distanced other            0.334\n2 distanced self             0.122\n3 immersed  other            0.195\n4 immersed  self            -0.559"
  },
  {
    "objectID": "6-interact.html#plotting-results-1",
    "href": "6-interact.html#plotting-results-1",
    "title": "Interactions Two",
    "section": "Plotting results",
    "text": "Plotting results\n\n\nCode\nsolomon.mod = lm(WISDOM ~ PERSPECTIVE*DISTANCE, data = solomon)\nplot_model(solomon.mod, type = \"int\")"
  },
  {
    "objectID": "6-interact.html#factorial-anova",
    "href": "6-interact.html#factorial-anova",
    "title": "Interactions II",
    "section": "Factorial ANOVA",
    "text": "Factorial ANOVA\nThe interaction of two or more categorical variables in a general linear model is formally known as Factorial ANOVA.\nA factorial design is used when there is an interest in how two or more variables (or factors) affect the outcome.\n\nRather than conduct separate one-way ANOVAs for each factor, they are all included in one analysis.\nThe unique and important advantage to a factorial ANOVA over separate one-way ANOVAs is the ability to examine interactions."
  },
  {
    "objectID": "6-interact.html#terminology",
    "href": "6-interact.html#terminology",
    "title": "Interactions II",
    "section": "Terminology",
    "text": "Terminology\nIn a completely crossed factorial design, each level of one factor occurs in combination with each level of the other factor.\nIf equal numbers of participants occur in each combination, the design is balanced. This has some distinct advantages (described later).\n\n\n\n\nSlow\nMedium\nFast\n\n\n\n\nNo Noise\nX\nX\nX\n\n\nControllable Noise\nX\nX\nX\n\n\nUncontrollable Noise\nX\nX\nX"
  },
  {
    "objectID": "6-interact.html#terminology-1",
    "href": "6-interact.html#terminology-1",
    "title": "Interactions II",
    "section": "Terminology",
    "text": "Terminology\nWe describe the factorial ANOVA design by the number of levels of each factor.\n\nFactor: a variable that is being manipulated or in which there are two or more groups\nLevel: the different groups within a factor\n\nIn this case, we have a 3 x 3 ANOVA (“three by three”), because our first factor (speed) has three levels (slow, medium, and fast) and our second factor (noise) also has three levels (none, controllable, and uncontrollable)"
  },
  {
    "objectID": "6-interact.html#questions",
    "href": "6-interact.html#questions",
    "title": "Interactions II",
    "section": "Questions",
    "text": "Questions\n\n\nCode\nmean.summary = Data %&gt;%\n  group_by(Noise, Speed) %&gt;%\n  summarize(Time = mean(Time)) %&gt;%\n  spread(\"Speed\", \"Time\")\nmean.summary$Noise = as.character(mean.summary$Noise)\nmean.summary$Marginal = rowMeans(mean.summary[2:4])\nmean.summary[4,1] = \"Marginal\"\nmean.summary[4,2] = colMeans(mean.summary[2:5], na.rm=T)[1]\nmean.summary[4,3] = colMeans(mean.summary[2:5], na.rm=T)[2]\nmean.summary[4,4] = colMeans(mean.summary[2:5], na.rm=T)[3]\nmean.summary[4,5] = colMeans(mean.summary[2:5], na.rm=T)[4]\nlibrary(knitr)\nlibrary(kableExtra)\nkable(mean.summary, digits = 2) %&gt;% kable_styling() %&gt;% group_rows(start_row = 1, end_row = 3)\n\n\n\n\n\nNoise\nSlow\nMedium\nFast\nMarginal\n\n\n\n\n\n\n\nNone\n630.72\n525.29\n329.28\n495.10\n\n\nControllable\n576.67\n492.72\n287.23\n452.21\n\n\nUncontrollable\n594.44\n304.62\n268.16\n389.08\n\n\nMarginal\n600.61\n440.88\n294.89\n445.46"
  },
  {
    "objectID": "6-interact.html#marginal-means",
    "href": "6-interact.html#marginal-means",
    "title": "Interactions II",
    "section": "Marginal means",
    "text": "Marginal means\n\n\n\n\nNoise\nSlow\nMedium\nFast\nMarginal\n\n\n\n\n\n\n\nNone\n630.72\n525.29\n329.28\n495.10\n\n\nControllable\n576.67\n492.72\n287.23\n452.21\n\n\nUncontrollable\n594.44\n304.62\n268.16\n389.08\n\n\nMarginal\n600.61\n440.88\n294.89\n445.46\n\n\n\n\n\n\nRegardless of dot speed, does noise condition affect performance? Performance declines in the presence of noise, especially if the noise is uncontrollable.\nHere, too adding information about variability allows us a sense of whether these are significant and meaningful differences…"
  },
  {
    "objectID": "6-interact.html#marginal-means-1",
    "href": "6-interact.html#marginal-means-1",
    "title": "Interactions II",
    "section": "Marginal means",
    "text": "Marginal means\n\n\n\n\nNoise\nSlow\nMedium\nFast\nMarginal\n\n\n\n\n\n\n\nNone\n630.72\n525.29\n329.28\n495.10\n\n\nControllable\n576.67\n492.72\n287.23\n452.21\n\n\nUncontrollable\n594.44\n304.62\n268.16\n389.08\n\n\nMarginal\n600.61\n440.88\n294.89\n445.46\n\n\n\n\n\n\nThe marginal mean differences correspond to main effects. They tell us what impact a particular factor has, ignoring the impact of the other factor.\nThe remaining effect in a factorial design, and it primary advantage over separate one-way ANOVAs, is the ability to examine conditional mean differences."
  },
  {
    "objectID": "6-interact.html#the-linear-model-way",
    "href": "6-interact.html#the-linear-model-way",
    "title": "Interactions II",
    "section": "The Linear Model Way",
    "text": "The Linear Model Way\n\nsummary(lm(Time ~ Noise*Speed, data = Data))\n\n\nCall:\nlm(formula = Time ~ Noise * Speed, data = Data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-316.23  -70.82    4.99   79.87  244.40 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       630.72      25.32  24.908  &lt; 2e-16 ***\nNoiseControllable                 -54.05      35.81  -1.509  0.13305    \nNoiseUncontrollable               -36.28      35.81  -1.013  0.31243    \nSpeedMedium                      -105.44      35.81  -2.944  0.00369 ** \nSpeedFast                        -301.45      35.81  -8.418 1.49e-14 ***\nNoiseControllable:SpeedMedium      21.48      50.64   0.424  0.67201    \nNoiseUncontrollable:SpeedMedium  -184.39      50.64  -3.641  0.00036 ***\nNoiseControllable:SpeedFast        12.01      50.64   0.237  0.81287    \nNoiseUncontrollable:SpeedFast     -24.84      50.64  -0.490  0.62448    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 113.2 on 171 degrees of freedom\nMultiple R-squared:  0.6109,    Adjusted R-squared:  0.5927 \nF-statistic: 33.56 on 8 and 171 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6-interact.html#mean-differences",
    "href": "6-interact.html#mean-differences",
    "title": "Interactions II",
    "section": "Mean differences",
    "text": "Mean differences\n\n\n\n\nNoise\nSlow\nMedium\nFast\nMarginal\n\n\n\n\n\n\n\nNone\n630.72\n525.29\n329.28\n495.10\n\n\nControllable\n576.67\n492.72\n287.23\n452.21\n\n\nUncontrollable\n594.44\n304.62\n268.16\n389.08\n\n\nMarginal\n600.61\n440.88\n294.89\n445.46\n\n\n\n\n\n\nAre the marginal mean differences for noise condition a good representation of what is happening within each of the dot speed conditions?\nIf not, then we would need to say that the noise condition effect depends upon (is conditional on) dot speed. We would have an interaction between noise condition and dot speed condition."
  },
  {
    "objectID": "6-interact.html#interpretation-of-significance-tests",
    "href": "6-interact.html#interpretation-of-significance-tests",
    "title": "Interactions II",
    "section": "Interpretation of significance tests",
    "text": "Interpretation of significance tests\n\n\nCode\nfit = lm(Time ~ Speed*Noise, data = Data)\nanova(fit)\n\n\nAnalysis of Variance Table\n\nResponse: Time\n             Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nSpeed         2 2805871 1402936 109.3975 &lt; 2.2e-16 ***\nNoise         2  341315  170658  13.3075 4.252e-06 ***\nSpeed:Noise   4  295720   73930   5.7649 0.0002241 ***\nResiduals   171 2192939   12824                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAll three null hypotheses are rejected. This only tells us that systemic differences among the means are present; follow-up comparisons are necessary to determine the nature of the differences."
  },
  {
    "objectID": "6-interact.html#cell-contrasts",
    "href": "6-interact.html#cell-contrasts",
    "title": "Interactions II",
    "section": "Cell contrasts",
    "text": "Cell contrasts\n\npairs(em)\n\n contrast                                    estimate   SE  df t.ratio p.value\n Slow None - Medium None                        105.4 35.8 171   2.944  0.0856\n Slow None - Fast None                          301.4 35.8 171   8.418  &lt;.0001\n Slow None - Slow Controllable                   54.1 35.8 171   1.509  0.8498\n Slow None - Medium Controllable                138.0 35.8 171   3.854  0.0051\n Slow None - Fast Controllable                  343.5 35.8 171   9.592  &lt;.0001\n Slow None - Slow Uncontrollable                 36.3 35.8 171   1.013  0.9842\n Slow None - Medium Uncontrollable              326.1 35.8 171   9.106  &lt;.0001\n Slow None - Fast Uncontrollable                362.6 35.8 171  10.124  &lt;.0001\n Medium None - Fast None                        196.0 35.8 171   5.473  &lt;.0001\n Medium None - Slow Controllable                -51.4 35.8 171  -1.435  0.8828\n Medium None - Medium Controllable               32.6 35.8 171   0.910  0.9922\n Medium None - Fast Controllable                238.1 35.8 171   6.648  &lt;.0001\n Medium None - Slow Uncontrollable              -69.2 35.8 171  -1.931  0.5933\n Medium None - Medium Uncontrollable            220.7 35.8 171   6.162  &lt;.0001\n Medium None - Fast Uncontrollable              257.1 35.8 171   7.180  &lt;.0001\n Fast None - Slow Controllable                 -247.4 35.8 171  -6.908  &lt;.0001\n Fast None - Medium Controllable               -163.4 35.8 171  -4.564  0.0003\n Fast None - Fast Controllable                   42.0 35.8 171   1.174  0.9609\n Fast None - Slow Uncontrollable               -265.2 35.8 171  -7.405  &lt;.0001\n Fast None - Medium Uncontrollable               24.7 35.8 171   0.689  0.9989\n Fast None - Fast Uncontrollable                 61.1 35.8 171   1.707  0.7417\n Slow Controllable - Medium Controllable         84.0 35.8 171   2.344  0.3219\n Slow Controllable - Fast Controllable          289.4 35.8 171   8.082  &lt;.0001\n Slow Controllable - Slow Uncontrollable        -17.8 35.8 171  -0.496  0.9999\n Slow Controllable - Medium Uncontrollable      272.1 35.8 171   7.597  &lt;.0001\n Slow Controllable - Fast Uncontrollable        308.5 35.8 171   8.615  &lt;.0001\n Medium Controllable - Fast Controllable        205.5 35.8 171   5.738  &lt;.0001\n Medium Controllable - Slow Uncontrollable     -101.7 35.8 171  -2.841  0.1114\n Medium Controllable - Medium Uncontrollable    188.1 35.8 171   5.252  &lt;.0001\n Medium Controllable - Fast Uncontrollable      224.6 35.8 171   6.271  &lt;.0001\n Fast Controllable - Slow Uncontrollable       -307.2 35.8 171  -8.579  &lt;.0001\n Fast Controllable - Medium Uncontrollable      -17.4 35.8 171  -0.486  0.9999\n Fast Controllable - Fast Uncontrollable         19.1 35.8 171   0.533  0.9998\n Slow Uncontrollable - Medium Uncontrollable    289.8 35.8 171   8.093  &lt;.0001\n Slow Uncontrollable - Fast Uncontrollable      326.3 35.8 171   9.111  &lt;.0001\n Medium Uncontrollable - Fast Uncontrollable     36.5 35.8 171   1.018  0.9837\n\nP value adjustment: tukey method for comparing a family of 9 estimates"
  },
  {
    "objectID": "6-interact.html#marginal-contrasts",
    "href": "6-interact.html#marginal-contrasts",
    "title": "Interactions II",
    "section": "Marginal contrasts",
    "text": "Marginal contrasts\n\nemmeans(fit, pairwise ~ Speed)\n\n$emmeans\n Speed  emmean   SE  df lower.CL upper.CL\n Slow      601 14.6 171      572      629\n Medium    441 14.6 171      412      470\n Fast      295 14.6 171      266      324\n\nResults are averaged over the levels of: Noise \nConfidence level used: 0.95 \n\n$contrasts\n contrast      estimate   SE  df t.ratio p.value\n Slow - Medium      160 20.7 171   7.726  &lt;.0001\n Slow - Fast        306 20.7 171  14.787  &lt;.0001\n Medium - Fast      146 20.7 171   7.061  &lt;.0001\n\nResults are averaged over the levels of: Noise \nP value adjustment: tukey method for comparing a family of 3 estimates"
  },
  {
    "objectID": "6-interact.html#uncentered-plot",
    "href": "6-interact.html#uncentered-plot",
    "title": "Interactions II",
    "section": "Uncentered Plot",
    "text": "Uncentered Plot\n\n\nCode\nlibrary(ggeffects)\npredictedvals = ggpredict(model = career.mod, terms = c(\"gpa\", \"major\"))\n\nggplot(data = predictedvals, aes(x = x, y = predicted, group = group)) +\n  geom_smooth(aes(ymin = conf.low,\n                  ymax = conf.high,\n                  color = group,\n                  fill = group),\n              stat = \"identity\",\n              alpha = .2) +\n  labs(x = \"GPA\",\n       y = \"Income (in thousands of dollars)\",\n       title = \"I am in the wrong profession\",\n       color = \"Undergraduate Major\",\n       fill = \"Undergraduate Major\",\n       group = \"Undergraduate Major\")"
  },
  {
    "objectID": "hw-5.html",
    "href": "hw-5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Due 2/26 at 9:59am\nData: The dataset looks at happiness in college as a relationship with school success, friendship quality, SES, and an intervention group (1 = control, 2 = study skills training, 3 = social skills training). The data are here: hw-5-data.csv\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "hw-5.html#question-1",
    "href": "hw-5.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\nRun a two-predictor regression predicting happiness by friendship and school success and the interaction between the two.\n\nDescribe your hypothesis for the interaction using the study variables\nDescribe in words what exactly the b1, b2, and b3 coefficients are telling us in this model."
  },
  {
    "objectID": "hw-5.html#question-2",
    "href": "hw-5.html#question-2",
    "title": "Homework 5",
    "section": "Question 2",
    "text": "Question 2\nCompute the following 3 simple regression equations based on the model you ran in Question 1. Your choice of moderating variable depends on your hypothesis. Write out and interpret each of these simple slope equations.\n\nThe regression of Y on X at the mean of X2\nAt one standard deviation above the mean of X2 (X2hi)\nAt one standard deviation below the mean of X2 (X2low)."
  },
  {
    "objectID": "hw-5.html#question-3",
    "href": "hw-5.html#question-3",
    "title": "Homework 5",
    "section": "Question 3",
    "text": "Question 3\nGraph the three simple slopes above within a single figure (no multiple panels)."
  },
  {
    "objectID": "hw-5.html#question-4",
    "href": "hw-5.html#question-4",
    "title": "Homework 5",
    "section": "Question 4",
    "text": "Question 4\nAre any of these simple slopes significantly different from 0? How do you know? (i.e., we want a little more info than just “yes” or “no”, but you could do this in 1-2 sentences.)"
  },
  {
    "objectID": "hw-5.html#question-5",
    "href": "hw-5.html#question-5",
    "title": "Homework 5",
    "section": "Question 5",
    "text": "Question 5\nCreate a new model that examines if happiness can be explained by school success and your intervention. Do not transform any variables.\n\nInterpret the intercept and each of the lower order coefficients. Be precise.\nConceptually interpret the interaction(s)\nCreate a single figure illustrating the interaction relationships"
  },
  {
    "objectID": "hw-5.html#question-6",
    "href": "hw-5.html#question-6",
    "title": "Homework 5",
    "section": "Question 6",
    "text": "Question 6\nRepeat the same regression you ran in Question 5, however before running it, please mean-center your continuous predictor. Re-interpret the intercept, the lower order coefficients, and the interaction(s)."
  },
  {
    "objectID": "6-HW.html",
    "href": "6-HW.html",
    "title": "hw 6",
    "section": "",
    "text": "Due: 3/4 at 9:59am\nData: The data consist of a 2x3 anova design examining comics. The two factors are: 1) Publisher, which has two levels (DC or Marvel comics). 2) is Alignment, which has three levels (good, bad, neutral). The DV is Total, which is a combination of strength, intelligence, speed, durability, power, and combat. Higher scores are better.\nThe research question is whether there are differences in total across publisher, across type of superhereo alignment and whether there is an interaction between these two factors. Data can be found here: hw-6-data.csv\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd/.qmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!\n\nWrite the equation that you will use to test the hypothesis. Run this in R and interpret what each of the coefficients mean.\nAre there any significant differences between?\n\n\n\nGood and bad?\nDC vs Marvel\nDC bad superheros vs Marvel bad superheros\nDC neutral vs DC bad\n\n\nWhat are the standardized (cohen’s d) differences between the groups in #2?\nCreate two graphs. 1. that show cell differences between groups. 2. that show differences in one of the marginal means."
  },
  {
    "objectID": "6-interact.html#effect-sizes",
    "href": "6-interact.html#effect-sizes",
    "title": "Interactions II",
    "section": "effect sizes",
    "text": "effect sizes\n\nem.ef &lt;- emmeans(fit, pairwise ~ Speed)\neff_size(em.ef, sigma = sigma(fit), edf = 171)\n\n contrast        effect.size    SE  df lower.CL upper.CL\n (Slow - Medium)        1.41 0.198 171    1.020     1.80\n (Slow - Fast)          2.70 0.234 171    2.238     3.16\n (Medium - Fast)        1.29 0.195 171    0.903     1.67\n\nResults are averaged over the levels of: Noise \nsigma used for effect sizes: 113.2 \nConfidence level used: 0.95"
  },
  {
    "objectID": "7-mlm.html#mlm",
    "href": "7-mlm.html#mlm",
    "title": "MLM",
    "section": "MLM",
    "text": "MLM\n\nWe will use MLM anytime we have more than one DV per grouping variable where the grouping variable has dependency.\nObervations nested within people; dyads within a relationship; children in classrooms; members within family; …\nWhen we have such designs we violate standard assumptions in regression"
  },
  {
    "objectID": "7-mlm.html#building-intuition",
    "href": "7-mlm.html#building-intuition",
    "title": "MLM",
    "section": "Building intuition",
    "text": "Building intuition"
  },
  {
    "objectID": "7-mlm.html#handling-multiple-dvs",
    "href": "7-mlm.html#handling-multiple-dvs",
    "title": "MLM",
    "section": "Handling multiple DVs?",
    "text": "Handling multiple DVs?\nWhat if people had more than 1 DV, like we do with time? What do you do with multiple items, multiple trials, multiple _____?\nTwo options: 1. Collapse and average across."
  },
  {
    "objectID": "7-mlm.html#example",
    "href": "7-mlm.html#example",
    "title": "MLM",
    "section": "Example",
    "text": "Example\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\n\nsimp&lt;- tribble(\n  ~ID, ~group,  ~test.score, ~study,\n1,1,5,1,\n2,1,7,3,\n3,2,4,2,\n4,2,6,4,\n5,3,3,3,\n6,3,5,5,\n7,4,2,4,\n8,4,4,6,\n9,5,1,5,\n10,5,3,7)"
  },
  {
    "objectID": "7-mlm.html#aggregation-obscures-hypotheses",
    "href": "7-mlm.html#aggregation-obscures-hypotheses",
    "title": "MLM",
    "section": "1. Aggregation obscures hypotheses",
    "text": "1. Aggregation obscures hypotheses\n\nBetween person H1: Do students who study more get better grades?\nWithin person H2: When a student studies, do they get better grades?\nH1 and H2 are independent from one another! Aggregation collapses the two. When you have nested data with many DVs it is important to not aggregate."
  },
  {
    "objectID": "7-mlm.html#regressions-within-regressions",
    "href": "7-mlm.html#regressions-within-regressions",
    "title": "MLM",
    "section": "2. Regressions within regressions",
    "text": "2. Regressions within regressions\nHelps to take multilevel and split it into the different levels.\nLevel 1 is the smallest unit of analysis (students, waves, trials, family members)\nLevel 2 variables are what level 1 variables are “nested” in (people, schools, counties, families, dyads)\nWe are going to use level one components to run a regression, and use the results to run a second regression (Coefficents as outcomes)"
  },
  {
    "objectID": "7-mlm.html#stroop-example",
    "href": "7-mlm.html#stroop-example",
    "title": "MLM",
    "section": "Stroop example",
    "text": "Stroop example\nLevel 1 can be thought of as regressions for each person. DV is reaction time, our dummy is congrudent (0) vs incongruent (1) trials\n\\[Y_\\text{josh} = \\beta_{0} + \\beta_{1}D_\\text{josh} + \\varepsilon_\\text{josh}\\]\n\\(\\beta_{0}\\) is my average for congruent trials, whereas \\(\\beta_{1}\\) is my difference between congruent and incongruent."
  },
  {
    "objectID": "7-mlm.html#level-1-equation",
    "href": "7-mlm.html#level-1-equation",
    "title": "MLM",
    "section": "Level 1 equation",
    "text": "Level 1 equation\n\\[Y_{i,j} = \\beta_{0j} + \\beta_{1j}X_{i,j} + \\varepsilon_{i,j}\\]"
  },
  {
    "objectID": "7-mlm.html#level-2-equation",
    "href": "7-mlm.html#level-2-equation",
    "title": "MLM",
    "section": "Level 2 equation",
    "text": "Level 2 equation\n\\[\\beta_{0j} = \\gamma_{00} + U_{0j}\\] \\[\\beta_{1j} = \\gamma_{10} + U_{1j}\\]\nOur B1 coefficient indexes people’s slope, and can be thought of as a simple vector of scores. This vector can be a DV and can be decomposed further by a simple regression. There is some average effect (fixed effect) that serves as the intercept. The \\(U_{j}\\) are like residuals."
  },
  {
    "objectID": "7-mlm.html#diferent-levels",
    "href": "7-mlm.html#diferent-levels",
    "title": "MLM",
    "section": "3. Diferent levels",
    "text": "3. Diferent levels\nThe third way is to think of questions at different levels. Level 1 is the smallest unit of analysis (students, waves, trials, family members). We can ask questions about those levels\nLevel 2 variables are what level 1 variables are “nested” in (people, schools, counties, families, dyads). We can ask separate questions about those questions."
  },
  {
    "objectID": "7-mlm.html#level-2-equation-1",
    "href": "7-mlm.html#level-2-equation-1",
    "title": "MLM",
    "section": "Level 2 equation",
    "text": "Level 2 equation\n\\[\\beta_{0j} = \\gamma_{00} + \\gamma_{01}\\bar{S}_{j} + U_{0j}\\] \\[\\beta_{1j} = \\gamma_{10}\\]\nWhat direction will the \\(\\gamma_{01}\\) coefficient be? \\(\\gamma_{10}\\)?"
  },
  {
    "objectID": "7-mlm.html#variance-decomposition",
    "href": "7-mlm.html#variance-decomposition",
    "title": "MLM",
    "section": "4. Variance decomposition",
    "text": "4. Variance decomposition\nFor standard regression, we think of error as existing in one big bucket called \\(\\varepsilon\\) . Everything that we do not know goes into that bucket, from measurement error to unmeasured important factors.\nFor MLMs we will be breaking up ( \\(\\varepsilon\\) ) into multiple buckets. These useful “buckets” (Us) are what we refer to as random/varying effects.\nThinking about MLM as two separate regressions, these can be conceptualized as residuals. Thinking about variance decomposition, these can be thought of as similar to ANVOA models."
  },
  {
    "objectID": "7-mlm.html#random-and-fixed-effects",
    "href": "7-mlm.html#random-and-fixed-effects",
    "title": "MLM",
    "section": "Random and fixed effects",
    "text": "Random and fixed effects\n\n\nLevel 1: \\[{Y}_{ij} = \\beta_{0j}  + \\beta_{1j}  +\\varepsilon_{ij}\\]\nLevel 2: \\[{\\beta}_{0j} = \\gamma_{00} + U_{0j}\\] \\[\\beta_{1j} = \\gamma_{10} + U_{1j}\\] Combined: \\[{Y}_{ij} = \\gamma_{00} + \\gamma_{10} +  U_{1j}+ U_{0j}  + \\varepsilon_{ij}\\]\n\n\\(U_{0j}\\) is considered a random effect, as it is varies across our grouping\n\\(\\gamma_{00}\\) is considered a fixed effect, as it is what is fixed (average) across our grouping"
  },
  {
    "objectID": "7-mlm.html#shrinkagepartial-pooling",
    "href": "7-mlm.html#shrinkagepartial-pooling",
    "title": "MLM",
    "section": "5. Shrinkage/partial pooling",
    "text": "5. Shrinkage/partial pooling\n\nWe treat our group variable as coming from a population. All groups are alike (because they are from the same population), but different in their own way. Because of this it is helpful to use information from other groups to help make predictions.\nWe do this in standard regression where we make predictions based on values from the whole dataset (not just binned Xs). A similar analogy for MLM is running a regression for each group. We want to pool as this leads to better predictions as we are not over fitting our data!"
  },
  {
    "objectID": "7-mlm.html#complete-partial-and-no-pooling",
    "href": "7-mlm.html#complete-partial-and-no-pooling",
    "title": "MLM",
    "section": "Complete, partial and no pooling",
    "text": "Complete, partial and no pooling\n\nComplete assumes everyone is the same, with \\(U_{0i}\\) being zero for everyone.\nNo pooling is if we calculate every person’s effect with a regression, subtracting out he grand mean average.\nPartial pooling is in the middle, a weighted average between the two. For those with fewer trials there is less information for a particular individual, thus the complete pooling estimate will be given more weight. If someone has a lot of data, there weighted average is closer to no pooling.\nPartial pooling prevents both over and under fitting of your data, leading to increased out of sample predictions."
  },
  {
    "objectID": "7-mlm.html#complete-pooling",
    "href": "7-mlm.html#complete-pooling",
    "title": "MLM",
    "section": "Complete pooling",
    "text": "Complete pooling\nIgnores any dependency. Doesn’t learn from others, assumes everyone is the same. Underfits the model.\n\n\nCode\nggplot(mlm, aes(x = week, y = SMN7)) + geom_point() + stat_smooth(method=\"lm\") +ylab(\"test score\") + xlab(\"study\") +  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank())"
  },
  {
    "objectID": "7-mlm.html#no-pooling",
    "href": "7-mlm.html#no-pooling",
    "title": "MLM",
    "section": "No Pooling",
    "text": "No Pooling\nEveryone is unique and we cannot learn from others. Leads to overfitting\n\n\nCode\nggplot(mlm, aes(x = week, y = SMN7, group = ID)) + geom_line() +  geom_point() + facet_wrap( ~ ID) +ylab(\"test score\") + xlab(\"study\")"
  },
  {
    "objectID": "7-mlm.html#partial-pooling-aka-shrinkage-aka-regularization",
    "href": "7-mlm.html#partial-pooling-aka-shrinkage-aka-regularization",
    "title": "MLM",
    "section": "Partial pooling aka shrinkage aka regularization",
    "text": "Partial pooling aka shrinkage aka regularization\n\n\nCode\nlibrary(viridis)\nggplot(mlm, aes(x = week, y = SMN7, group = ID, colour = ID)) + stat_smooth(method = \"lm\", se = FALSE, alpha = .5) +scale_color_viridis()+  ylab(\"test score\") + xlab(\"study amount\") + geom_point()"
  },
  {
    "objectID": "7-mlm.html#example-1",
    "href": "7-mlm.html#example-1",
    "title": "MLM",
    "section": "Example",
    "text": "Example\nDay to day negative affect\n\n\nCode\nmelsm %&gt;% \ndistinct(record_id) %&gt;% \n  count()\n\n\n    n\n1 193\n\n\n\n\nCode\nmelsm %&gt;% \n    count(record_id) %&gt;% \n  ggplot(aes(x = n)) +\n  geom_bar() +\n  scale_x_continuous(\"number of days\", limits = c(0, NA))"
  },
  {
    "objectID": "7-hw.html",
    "href": "7-hw.html",
    "title": "HW 7",
    "section": "",
    "text": "Due: 3/18 at 9:59am\nData: Data are the same as presented in class. They are repeated measures data where daily observations of negative and positive affect are nested within participant. Participants could have up to 100 observations. Data can be found here: data_hw_7.csv\n\nWhat situations would you need to use MLM? What are two benefits to MLMs?\nWhat types of variables can be considered “level 1” and what types of variables can be considered “level 2”?\nA researcher is interested in how exercise (operationalized as number of steps) is related to a person’s level of positive affect, with positive affect serving as the DV and exercise as the IV. However the researcher knows that she could think of exercise both as a between subjects variable (some people exercise more than others), as well as a within person variable (some days you exercise, sometimes you don’t). Write out a level 1 equation and level 2 equation that tests the hypothesis that these two forms of exercise are related to positive affect, describing how to interpret each coefficient.\nUsing the data, run a model where daily levels of positive affect (P_A.std) are predicted by both person mean centered daily steps (steps.pmd) and person mean average steps (steps.pm).\n\n\nInterpret the coefficient for the level 1 effect of steps\nWhat coefficient tests whether people who take more steps on average have higher levels of positive affect?\nInterpret the random effect for intercept.\n\n\nHow would you change the model to test the hypothesis that those who take more steps on average (lets call them athletes) don’t benefit from daily steps in the same way as those who take less steps on average do (lets call them couch potatoes)? In other words, how could we test whether couch potatoes feel higher levels of positive affect after taking more steps than they typically do whereas if an athlete has a day of higher than typical steps they do not have higher levels of positive affect."
  },
  {
    "objectID": "9-logistic.html#this-whole-semester-has-been-linear-regression-on-steroids",
    "href": "9-logistic.html#this-whole-semester-has-been-linear-regression-on-steroids",
    "title": "Logistic",
    "section": "This whole semester has been linear regression (on steroids)",
    "text": "This whole semester has been linear regression (on steroids)\n\nWe’ve had continuous IVs (regression)\nWe’ve had categorical IVs (factorial ANOVA)\nWe’ve had a mixture of continuous/categorical IVs (more regression)\nBut we’ve never had a DV variable that is categorical… in comes Logistic Regression"
  },
  {
    "objectID": "9-logistic.html#linear-regression",
    "href": "9-logistic.html#linear-regression",
    "title": "Logistic",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhen we have a continuous DV, we can… - Calculate the \\(R^2\\) and determine if our IVs & DV are correlated (large value implying = large effect)\n\nCalculate a \\(p\\)-value to determine if \\(R^2\\) (or our model) is statistically significant\nUse the line/slope of a linear regression to make a calculated prediction of y given x\nCompare models\nAdd in predictors & look at interactions"
  },
  {
    "objectID": "9-logistic.html#logistic-regression",
    "href": "9-logistic.html#logistic-regression",
    "title": "Logistic",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nWhat is the goal of prediction when you have a categorial (dichotomous) outcome?\nWe are trying to see if something is TRUE or FALSE"
  },
  {
    "objectID": "9-logistic.html#learning-goals",
    "href": "9-logistic.html#learning-goals",
    "title": "Logistic",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nWTF is logistic regression. When to use it?\nWTF is the generalized linear model. When to use it?\nWTF is maximum likelihood estimation\nHTF do I do this in R"
  },
  {
    "objectID": "9-logistic.html#learning-goals-1",
    "href": "9-logistic.html#learning-goals-1",
    "title": "Logistic",
    "section": "Learning Goals",
    "text": "Learning Goals\nLearning goals today are not: - know every little thing about odds/probabilities/weird things I’m going to introduce - know every element of the output - memorize anything"
  },
  {
    "objectID": "9-logistic.html#logistic-regression-1",
    "href": "9-logistic.html#logistic-regression-1",
    "title": "Logistic",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nUsed when your DV is binary (0,1)\n- Clinical diagnosis\n- Disease prevalence\n- Experiences (Yes/No) - Correct/Incorrect"
  },
  {
    "objectID": "9-logistic.html#where-is-the-line",
    "href": "9-logistic.html#where-is-the-line",
    "title": "Logistic",
    "section": "Where is the line??",
    "text": "Where is the line??\nLogistic Regression, rather than a straight, fitted line like linear regression, logistic regression fits an S-shaped logistic function.\nThis curved line can tell us the probability that something will be “1” given X"
  },
  {
    "objectID": "9-logistic.html#model-complexity",
    "href": "9-logistic.html#model-complexity",
    "title": "Logistic",
    "section": "Model Complexity",
    "text": "Model Complexity\n…and just like linear regression, we can make simple models (a relationship between x and y, with y being a binary variable) or more complicated models (with covariates).\nUnlike linear regression, it isn’t as easy to just compare a more complex model to a simple model"
  },
  {
    "objectID": "9-logistic.html#assumption-violations",
    "href": "9-logistic.html#assumption-violations",
    "title": "Logistic",
    "section": "Assumption violations",
    "text": "Assumption violations\nWhen our outcome is binary, we violate OLS regression assumptions\nViolates:\n\nCorrectly specified form (not linear)\nHomoscedasticity (as probability approaches 1 or 0, variance approaches 0)\nNormality of the errors (lol not close)"
  },
  {
    "objectID": "9-logistic.html#need-to-think-in-terms-of-probabilities",
    "href": "9-logistic.html#need-to-think-in-terms-of-probabilities",
    "title": "Logistic",
    "section": "Need to think in terms of probabilities",
    "text": "Need to think in terms of probabilities\n\nIf we use OLS, we violate assumptions and have predicted values that go outside 0 & 1\nHow does the predicted probability of getting a 0 or a 1 relate to our predictors?\n\n\\[\\hat{p}_{i} \\leftrightsquigarrow b_{0} + b_{1}X_{1} + b_{2}X_{2}... b_{3}X_{p}\\]"
  },
  {
    "objectID": "9-logistic.html#generalized-linear-models",
    "href": "9-logistic.html#generalized-linear-models",
    "title": "Logistic",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nExtends the general linear model framework\nUsed to describe different Data Generating Processes (DGPs) other than Gaussian normal\nNeed to use if we cannot use the Gaussian normal e.g. the range of Y is restricted (e.g. binary, count) and/or the variance of Y depends on the mean, etc…\nIn other words, what is your DGP?"
  },
  {
    "objectID": "9-logistic.html#generalized-linear-models-1",
    "href": "9-logistic.html#generalized-linear-models-1",
    "title": "Logistic",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nGaussian model is \\[y_i \\sim  N(\\mu_i, \\sigma)\\]\n\n\\[\\mu_i = \\alpha + \\beta x_i\\] - The parameters of the normal distribution are the mean μ and the standard deviation σ (or the variance \\(\\sigma^2\\))]\n\nBinomial outcome is \\[y_i \\sim  Binomial(n, p_i)\\]\n\n\\[f(p_i) = \\alpha + \\beta x_i\\] - n stands for the number of times the experiment runs. - p represents the probability of one specific outcome.]"
  },
  {
    "objectID": "9-logistic.html#glm-components",
    "href": "9-logistic.html#glm-components",
    "title": "Logistic",
    "section": "GLM components",
    "text": "GLM components\nIt is not common to have an average outcome \\(\\mu\\) (other than in a normal distribution)\nIt is not common to have parameters range from negative to positive infinity.\nWe need something that translates our model into the parameters that describe the distribution."
  },
  {
    "objectID": "9-logistic.html#logistic-regression-2",
    "href": "9-logistic.html#logistic-regression-2",
    "title": "Logistic",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[y_i \\sim  Binomial(n, p_i)\\]\n\\[f(p_i) = \\alpha + \\beta x_i\\]\nOur number of trials is \\(N\\), so we are predicting the probability of \\(y\\). Probabilities are bounded between zero and one.\nBecause our variables are not in probability units we need to “link” them via a function. The linear right side of the equation is not in the same units as the non-linear left side.\nTwo most popular are logit and log. Others are available too, such as probit."
  },
  {
    "objectID": "9-logistic.html#link-function-for-logistic",
    "href": "9-logistic.html#link-function-for-logistic",
    "title": "Logistic",
    "section": "Link function for logistic",
    "text": "Link function for logistic\n\nWe need to map (0,1) to \\((-\\infty, \\infty)\\)\nLogistic regression uses the logistic function to link the predicted probabilities to the predictors\nThink of it as a transformation of \\(\\hat{Y}\\)s"
  },
  {
    "objectID": "9-logistic.html#sigmoid-function",
    "href": "9-logistic.html#sigmoid-function",
    "title": "Logistic",
    "section": "Sigmoid function",
    "text": "Sigmoid function\n\\[f(x) = \\frac{1}{1+e^{-X}}\\]\n… where \\(e\\) is Euler’s number\n\nexp(1)\n\n[1] 2.718282"
  },
  {
    "objectID": "9-logistic.html#sigmoid-function-cont.",
    "href": "9-logistic.html#sigmoid-function-cont.",
    "title": "Logistic",
    "section": "Sigmoid function (cont.)",
    "text": "Sigmoid function (cont.)\nFor the Sigmoid function, as x approaches \\(\\infty\\), it reaches a natural limit at 1.\nAs x approaches \\(-\\infty\\), it reaches a natural limit of 0\nThis all keeps the limits within the 0, 1 range."
  },
  {
    "objectID": "9-logistic.html#odds",
    "href": "9-logistic.html#odds",
    "title": "Logistic",
    "section": "Odds",
    "text": "Odds\n\nOdds are defined as the probability of being a case divided by the probability of being a noncase\nNot bound between 0 and 1\nRange from 0 to infinity\nLess than one is less than 50% probability \\[odds = \\frac {\\hat{p}}{1-\\hat{p}}\\]\n\n\\[probability= \\frac{\\hat{odds}}{1+\\hat{odds}}\\]"
  },
  {
    "objectID": "9-logistic.html#odds-versus-probability",
    "href": "9-logistic.html#odds-versus-probability",
    "title": "Logistic",
    "section": "Odds versus Probability",
    "text": "Odds versus Probability\nPut simply:\n\nOdds are the ratio of something happening over something not happening.\nProbability is the ratio of something happening over everything that could happen.\n\nThe odds being so asymmetrical (unlike probability) make it difficult to compare the odds of being 0 versus the odds of being 1. So we use something called log odds."
  },
  {
    "objectID": "9-logistic.html#log-odds",
    "href": "9-logistic.html#log-odds",
    "title": "Logistic",
    "section": "Log Odds",
    "text": "Log Odds\nTaking the log of the odds solves this problem by making everything symetrical!\nOdds are 1 to 6:\n\nlog(1/6) = log(0.17) = -1.79\n\nOdds are 6 to 1:\n\nlog(6/1) = log(6) = 1.79"
  },
  {
    "objectID": "9-logistic.html#linear-probability-model",
    "href": "9-logistic.html#linear-probability-model",
    "title": "Logistic",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\n\\[f(x) = \\frac{1}{1+e^{-X}}\\] \\[\\hat{p} = \\frac{1}{1+e^{-b_{0}+b_{1}X}}\\]\n\\[odds = \\frac {\\hat{p}}{1-\\hat{p}}=e^{b_{0}+b_{1}X}\\] \\[logit= Log(odds)=ln(\\frac{\\hat{p}}{1-\\hat{p}}) = b_{0}+b_{1}X\\]"
  },
  {
    "objectID": "9-logistic.html#logit",
    "href": "9-logistic.html#logit",
    "title": "Logistic",
    "section": "Logit",
    "text": "Logit\n\\[logit= Log(odds)=ln(\\frac{\\hat{p}}{1-\\hat{p}}) = b_{0}+b_{1}X\\] - DV is a logit, the natural log of odds\n\nPredicted scores are not dichotomous\nInstead of predicting probabilities directly, we are instead predicting the log of the odds.\nThe regression we are used to is not predicting \\(\\hat{Y}\\), because we’re predicting (or relating our IV) logits"
  },
  {
    "objectID": "9-logistic.html#now-what",
    "href": "9-logistic.html#now-what",
    "title": "Logistic",
    "section": "Now what?",
    "text": "Now what?\n\nIn order to get the probability of a value of X being 1 or 0 based on some model parameters ( \\(b_0\\), \\(b_1\\) etc. ), we rearrange our equation so that we actually get the predicted logit (log of odds) given our model parameters.\nHow the hell do you interpret a logit? Not easily…\nConvert back into odds\nConvert your odds back into probabilities"
  },
  {
    "objectID": "9-logistic.html#uhhhhhh",
    "href": "9-logistic.html#uhhhhhh",
    "title": "Logistic",
    "section": "Uhhhhhh",
    "text": "Uhhhhhh\nDoes this seem convoluted?\nWe are predicting logits, which happen to be the log of odds. It’s bananas.\nHow do we get these numbers in the first place? Can we use OLS?"
  },
  {
    "objectID": "9-logistic.html#estimation-with-maximum-likelihood",
    "href": "9-logistic.html#estimation-with-maximum-likelihood",
    "title": "Logistic",
    "section": "Estimation with Maximum Likelihood",
    "text": "Estimation with Maximum Likelihood\n\nOLS minimizes the errors ( \\(SS_{res}\\)), which maximizes ( \\(SS_{reg}\\))\n\nIn logistic regression we are not so lucky\nNeed to rely on iterative procedure, Maximum Likelihood (ML) Estimation:\n\nPick parameters of your model ( \\(b_0\\), \\(b_1\\) etc. ), and calculate the likelihood of the data, given those parameters. We do this iteratively until we find the best parameters – the ones that maximize the likelihood of your data."
  },
  {
    "objectID": "9-logistic.html#estimation-with-maximium-likelihood",
    "href": "9-logistic.html#estimation-with-maximium-likelihood",
    "title": "Logistic",
    "section": "Estimation with Maximium Likelihood",
    "text": "Estimation with Maximium Likelihood\nDo any of you hike? Know how to read a topographic map?\nML Estimation is sort of like going hiking with your data to find the highest point. And most of the time you can!\nBut if if you take a wrong turn, you might get screwed. It’s the risk we take when using ML Estimation!"
  },
  {
    "objectID": "9-logistic.html#estimation-with-maximum-likelihood-1",
    "href": "9-logistic.html#estimation-with-maximum-likelihood-1",
    "title": "Logistic",
    "section": "Estimation with Maximum Likelihood",
    "text": "Estimation with Maximum Likelihood\nWe are using the distribution of the data to find the location that maximizes the likelihood of observing the variable that we measured.\nWe are essentially trying to find the optimal value for the mean (or standard deviation) for a distribution given our observed data.\nWe are talking here about the mean of the distribution, not the mean of the data. (However, in a normal distribution, these are the same thing.)"
  },
  {
    "objectID": "9-logistic.html#estimation-with-maximum-likelihood-2",
    "href": "9-logistic.html#estimation-with-maximum-likelihood-2",
    "title": "Logistic",
    "section": "Estimation with Maximum Likelihood",
    "text": "Estimation with Maximum Likelihood\n\nSo we get model parameters, estimated via MLE (instead of OLS)\nSame but different:\n\nAsymptotic standard errors (an approximation to the standard error)\nInterpret test statistics as \\(z\\)’s, not \\(t\\)’s\nNo \\(t\\)-tests; instead a Wald test = \\(\\chi^2\\) test with 1 df = \\((\\frac{coef}{se})^2\\)\n\n\nasymptotic se’s are just approximations since we finite-sample dist of the estimator isn’t known"
  },
  {
    "objectID": "9-logistic.html#all-together-now",
    "href": "9-logistic.html#all-together-now",
    "title": "Logistic",
    "section": "All Together Now",
    "text": "All Together Now\n\nWe try to predict binary outcomes. Our DV is dichotomous.\nWe can use categorical or continuous predictors (IVs) to do so\nWe can write the equation in roughly the same way\nWe cannot use OLS because it will give us impossible values and just logically it doesn’t work. We use ML Estimation to determine our logistic regression coefficients instead. This gives us a S-shaped curve\nWe can ask if our coefficients are significantly different from 0\nWe convert to odds and/or probabilities"
  },
  {
    "objectID": "9-logistic.html#glm-in-r",
    "href": "9-logistic.html#glm-in-r",
    "title": "Logistic",
    "section": "GLM in R",
    "text": "GLM in R\n\nglm(formula,\n    family = gaussian(link=\"identity\"), #&lt;&lt;\n    data,\n    weights,\n    subset,\n    na.action,\n    start = NULL,\n    etastart,\n    mustart,\n    offset, \n    control = glm.control(...),\n    model = TRUE,\n    method = ”glm.fit”,\n    x = FALSE, \n    y = TRUE,\n    contrasts = NULL, ...)"
  },
  {
    "objectID": "9-logistic.html#glm-in-r-1",
    "href": "9-logistic.html#glm-in-r-1",
    "title": "Logistic",
    "section": "GLM in R",
    "text": "GLM in R\n\nglm(y ~ X1+ X2 + X3 , #&lt;&lt;\n    family = binomial,\n    data = dataset)\n\nSpecify the model like you would with lm"
  },
  {
    "objectID": "9-logistic.html#glm-in-r-2",
    "href": "9-logistic.html#glm-in-r-2",
    "title": "Logistic",
    "section": "GLM in R",
    "text": "GLM in R\n\nglm(y ~ X1+ X2 + X3 , \n    family = binomial, #&lt;&lt;\n    data = dataset)\n\nSpecify the distribution you’re working with. When binary outcomes, we’ll use the binomial."
  },
  {
    "objectID": "9-logistic.html#glm-in-r-3",
    "href": "9-logistic.html#glm-in-r-3",
    "title": "Logistic",
    "section": "GLM in R",
    "text": "GLM in R\n\nglm(y ~ X1+ X2 + X3 , \n    family = binomial, \n    data = dataset) #&lt;&lt;\n\nSpecify your dataset."
  },
  {
    "objectID": "9-logistic.html#how-to-interpret",
    "href": "9-logistic.html#how-to-interpret",
    "title": "Logistic",
    "section": "How to interpret",
    "text": "How to interpret\n\n\\(b_1\\) is the predicted change in the logit for a 1-unit change in X, holding the other predictors constant\nFor a 1-unit change in X, holding other predictors constant, the odds that Y = 1 changes by \\(e^{b_{1}}\\)\n\ne.g,. \\(b_{1}\\) = .4, \\(e^{.4}\\) = 1.49\n\nFor fitted values, need to use entire equation \\(\\hat{Y} = e^{b_{0}+b_{1}X_{1}}\\)\nTurn to probabilities by: \\(\\frac{\\text{odds}}{(1 + \\text{odds})}\\)"
  },
  {
    "objectID": "9-logistic.html#example",
    "href": "9-logistic.html#example",
    "title": "Logistic",
    "section": "Example",
    "text": "Example\n\n# 1 = not premature\nmortality\n\n# A tibble: 300 × 4\n   Intelligence_Self Intelligence_Mate premature.d NOT.premature\n               &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;\n 1                22                19 normal                  1\n 2                22                18 normal                  1\n 3                21                21 normal                  1\n 4                22                17 normal                  1\n 5                19                18 normal                  1\n 6                19                20 premature               0\n 7                16                18 normal                  1\n 8                15                11 premature               0\n 9                16                21 normal                  1\n10                19                22 normal                  1\n# ℹ 290 more rows"
  },
  {
    "objectID": "9-logistic.html#interpretation",
    "href": "9-logistic.html#interpretation",
    "title": "Logistic",
    "section": "Interpretation",
    "text": "Interpretation\nFor a 1-unit change in X, holding other predictors constant, the odds that Y = 1 changes by \\(e^{b_{1}}\\)\n\nexp(1)^.08012\n\n[1] 1.083417\n\n\nFor every 1-unit increase in Intelligence, the odds of not having a premature death 8%"
  },
  {
    "objectID": "9-logistic.html#specific-values",
    "href": "9-logistic.html#specific-values",
    "title": "Logistic",
    "section": "Specific Values?",
    "text": "Specific Values?\nWhat if you want the probability of being a premature death for a given level of Intelligence? (Now that we’ve run our model and have parameters…)\nFor fitted values, need to use entire equation \\(\\hat{Y} = e^{b_{0}+b_{1}X_{1}}\\)\n\n# get fitted value with a given value of X (here 20)\nexp(1)^(0.28695 + (.08012*20))\n\n[1] 6.615067\n\n# now get odds\n6.615067 / (1+6.615067)\n\n[1] 0.8686814"
  },
  {
    "objectID": "9-logistic.html#probit",
    "href": "9-logistic.html#probit",
    "title": "Logistic",
    "section": "Probit",
    "text": "Probit\nWe can have different link functions. When your response variable (DV) is truly binary – the data generating process generates legit binary data – logit is your pick."
  },
  {
    "objectID": "9-logistic.html#probit-1",
    "href": "9-logistic.html#probit-1",
    "title": "Logistic",
    "section": "Probit",
    "text": "Probit\n\ndeath.4 &lt;- glm(NOT.premature ~ Intelligence_Self,\n        family = binomial(link = \"probit\"), data = mortality)\nsummary(death.4)\n\n\nCall:\nglm(formula = NOT.premature ~ Intelligence_Self, family = binomial(link = \"probit\"), \n    data = mortality)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)        0.21961    0.38376   0.572   0.5671  \nIntelligence_Self  0.04513    0.02319   1.946   0.0516 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.53  on 299  degrees of freedom\nResidual deviance: 269.72  on 298  degrees of freedom\nAIC: 273.72\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "8-hw.html",
    "href": "8-hw.html",
    "title": "homework 8",
    "section": "",
    "text": "Please complete this assignment by creating a new RMarkdown (or Quarto) file. Be sure to include your name in the preamble. Please submit both your .Rmd (or .qmd) and your .html files to Canvas.\nData: hw-8-data.csv"
  },
  {
    "objectID": "8-hw.html#instructions",
    "href": "8-hw.html#instructions",
    "title": "homework 8",
    "section": "",
    "text": "Please complete this assignment by creating a new RMarkdown (or Quarto) file. Be sure to include your name in the preamble. Please submit both your .Rmd (or .qmd) and your .html files to Canvas.\nData: hw-8-data.csv"
  },
  {
    "objectID": "10-bayes.html#reassurance-before-we-get-to-the-three-steps",
    "href": "10-bayes.html#reassurance-before-we-get-to-the-three-steps",
    "title": "Bayes",
    "section": "Reassurance, before we get to the three steps",
    "text": "Reassurance, before we get to the three steps\n\nNot drastically different!\nYou get to keep everything you like\nYour models stay the same!!"
  },
  {
    "objectID": "10-bayes.html#glm",
    "href": "10-bayes.html#glm",
    "title": "Bayes",
    "section": "GLM",
    "text": "GLM\n\nOur good friend that gives us 99% of the models psychologists use (general(ized) linear model), is exactly the same\n\n\\[\\Large Y = b_{0} + b_{1}X +e\\] - No need to think about setting up new t-test, ANOVAS, regressions, etc. ALL THE SAME."
  },
  {
    "objectID": "10-bayes.html#a-working-mental-model",
    "href": "10-bayes.html#a-working-mental-model",
    "title": "Bayes",
    "section": "A working mental model",
    "text": "A working mental model\nWhat are Bayesian models?\n\n“Normal” regression with a different algorithm.\nResults that represent a distribution rather than a point estimate and some uncertainty.\nPriors that incorporate existing knowledge."
  },
  {
    "objectID": "10-bayes.html#be-comfortable-with-a-different-estimation-algorithm",
    "href": "10-bayes.html#be-comfortable-with-a-different-estimation-algorithm",
    "title": "Bayes",
    "section": "1. Be comfortable with a different estimation algorithm",
    "text": "1. Be comfortable with a different estimation algorithm\n\nWhat do you mean by estimation algorithm?\n\n\n\nOLS i.e. \\(min\\sum(Y_{i}-\\hat{Y})^{2}\\)\nFun fact, R uses QR decomposition, Newton Raphson, Fisher Scoring, SVR, etc – not this equation.\nAnother fun fact, more advanced stats use an even different algorithm (e.g., maximum likelihood)"
  },
  {
    "objectID": "10-bayes.html#standard-way",
    "href": "10-bayes.html#standard-way",
    "title": "Bayes",
    "section": "Standard way",
    "text": "Standard way\n\n\nCode\nlibrary(tidyverse)\ngalton.data &lt;- psychTools::galton"
  },
  {
    "objectID": "10-bayes.html#fisher-scoring",
    "href": "10-bayes.html#fisher-scoring",
    "title": "Bayes",
    "section": "Fisher Scoring",
    "text": "Fisher Scoring\n\n\nCode\nfit.1.g &lt;- glm(child ~ parent, family = gaussian, data = galton.data)\nsummary(fit.1.g)\n\n\n\nCall:\nglm(formula = child ~ parent, family = gaussian, data = galton.data)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5.011094)\n\n    Null deviance: 5877.2  on 927  degrees of freedom\nResidual deviance: 4640.3  on 926  degrees of freedom\nAIC: 4133.2\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "10-bayes.html#maximum-likelihood",
    "href": "10-bayes.html#maximum-likelihood",
    "title": "Bayes",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\n\n\nlavaan 0.6.16 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         3\n\n  Number of observations                           928\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  child ~                                             \n    parent            0.646    0.041   15.728    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .child            23.942    2.808    8.527    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .child             5.000    0.232   21.541    0.000"
  },
  {
    "objectID": "10-bayes.html#bayesian-way",
    "href": "10-bayes.html#bayesian-way",
    "title": "Bayes",
    "section": "Bayesian way",
    "text": "Bayesian way\n\n\nCode\nlibrary(brms)\nfit.1.bayesian &lt;- brm(child ~ parent, data = galton.data, backend = \"cmdstanr\", file = \"fit.1.b\")\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.7 seconds.\n\n\n\nsummary(fit.1.bayesian)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: child ~ parent \n   Data: galton.data (Number of observations: 928) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    24.00      2.79    18.54    29.38 1.00     3554     3112\nparent        0.65      0.04     0.57     0.73 1.00     3545     3112\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.24      0.05     2.14     2.34 1.00     3880     3058\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "10-bayes.html#step-1-is-easy",
    "href": "10-bayes.html#step-1-is-easy",
    "title": "Bayes",
    "section": "Step 1 is easy",
    "text": "Step 1 is easy\n\nBayes gives you basically the same results\n\n\n\nSo why use it? Many reasons, but the most direct is manipulating, visualizing, and extrapolating from results"
  },
  {
    "objectID": "10-bayes.html#think-of-results-in-terms-of-distributions",
    "href": "10-bayes.html#think-of-results-in-terms-of-distributions",
    "title": "Bayes",
    "section": "2. Think of results in terms of distributions",
    "text": "2. Think of results in terms of distributions\n\nWhat are results?\n\n\n\nEstimate and an SE\nIndicates a “best guess” ie mean/median/mode and the imprecision related to it\nIf this guess is far away from zero (and imprecision not large), then it is significant\nWe know that if we repeated this again we won’t get the same answer (estimate), but likely in between our CIs\nHow do we convey the “best guess?”"
  },
  {
    "objectID": "10-bayes.html#posterior-distribution-ie-results",
    "href": "10-bayes.html#posterior-distribution-ie-results",
    "title": "Bayes",
    "section": "Posterior distribution (ie results)",
    "text": "Posterior distribution (ie results)\n\nIs made up of a series of educated guesses (via our algorithm), each of which is consistent with the data.\nIn aggregate, these guesses provide us not with a best guess and an SD (as with Maximum Likelihood), but a more complete sense of each parameter we are trying to estimate.\nWe can assume this distribution (typically normal) with standard estimation, but with bayes it can be flexible!"
  },
  {
    "objectID": "10-bayes.html#posterior-distribution-ie-results-1",
    "href": "10-bayes.html#posterior-distribution-ie-results-1",
    "title": "Bayes",
    "section": "Posterior distribution (ie results)",
    "text": "Posterior distribution (ie results)\nIs made of up of a series of educated guesses. Each dot represents a particular guess. Guesses that occur more often are considered more likely.\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nggplot(aes(x = b_parent)) +\n  stat_dotsinterval()"
  },
  {
    "objectID": "10-bayes.html#how-does-the-algorithm-work",
    "href": "10-bayes.html#how-does-the-algorithm-work",
    "title": "Bayes",
    "section": "How does the algorithm work?",
    "text": "How does the algorithm work?\n\nPlayed a role in developing the thermonuclear bomb with one of the earliest computers. Published in 1953 but ignored within stats b/c it was published within a physics/chemistry journal. Took about until 1990 for desktop computers to run fast enough to do at home.\nMany variants, but the general idea is a) propose an estimate value + noise N(0, \\(\\sigma\\) ) then b) see how “likely” the data is given the estimate, c) based on some criteria (better than worse that some value) either accept or reject the estimate and d) repeat"
  },
  {
    "objectID": "10-bayes.html#what-do-you-mean-by-likely",
    "href": "10-bayes.html#what-do-you-mean-by-likely",
    "title": "Bayes",
    "section": "What do you mean by likely?",
    "text": "What do you mean by likely?\nYou’ve done this before last semester. Three parameters in a binomial distribution (# successes, # of trials, probability of success). Often you would fix #trials and probability of success to see what # successes are most/least likely.\n\n\nCode\ndata.frame(heads = 0:10, prob = dbinom(x = 0:10, size = 10, prob = .5)) %&gt;% \n  ggplot(aes(x = factor(heads), y = prob)) +\n  geom_col(fill = \"#562457\") +\n  geom_text(aes(label = round(prob, 2), y = prob + .01),\n            position = position_dodge(.9),\n            size = 5, \n            vjust = 0) +\n  labs(title = \"Binomial Distribution of Coin Flips\",\n       subtitle = \"n = 10, p = .5\",\n       x = \"Number of Successes (Heads)\",\n       y = \"Density\") +\n  theme_classic(base_size = 16)"
  },
  {
    "objectID": "10-bayes.html#more-intuituion",
    "href": "10-bayes.html#more-intuituion",
    "title": "Bayes",
    "section": "More intuituion",
    "text": "More intuituion\n\nThink of the algorithm as picking out marbles from a sack, with replacement, to figure out the distribution of colors.\nOr us doing rnorm with me hiding what the mean and SD are, but then figuring out what the mean and SD are through counting the samples."
  },
  {
    "objectID": "10-bayes.html#bayesian-analysis-is-just-counting",
    "href": "10-bayes.html#bayesian-analysis-is-just-counting",
    "title": "Bayes",
    "section": "Bayesian analysis is just counting",
    "text": "Bayesian analysis is just counting\n\nBayesian analysis counts all ways that something can happen (according to assumptions/model). Assumptions with more ways that are consistent with data are more plausible.\nThis method is not demonstrably different than standard approaches. Standard likelihood approaches use the values that are most consistent with the data as an estimate. Try out all possible numbers and then tells you which one is most likely.\nWhere Bayes differs, is we will focus beyond just a “best estimate”"
  },
  {
    "objectID": "10-bayes.html#visualizing-uncertainty",
    "href": "10-bayes.html#visualizing-uncertainty",
    "title": "Bayes",
    "section": "Visualizing uncertainty",
    "text": "Visualizing uncertainty\nOur posterior (ie different educated guesses at a the correct parameters; distribution of plausible values) is highlighting: that there is no ONE result, that there are many possible results that are consistent with the data.\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nggplot(aes(x = b_parent)) +\n  stat_dotsinterval()"
  },
  {
    "objectID": "10-bayes.html#some-positives-of-focusing-on-uncertainty",
    "href": "10-bayes.html#some-positives-of-focusing-on-uncertainty",
    "title": "Bayes",
    "section": "Some positives of focusing on uncertainty",
    "text": "Some positives of focusing on uncertainty\n\nDo not need to assume normal or multivariate normal. Uncertainty does not need to be even tailed.\nDifferences (say across groups) in uncertainty is allowed. Do not need to assume groups have same standard errors. One can better account for and/or probe situations where a certain group has a lot or little variability.\nEasy to calculate uncertainty"
  },
  {
    "objectID": "10-bayes.html#cis-around-a-particular-value",
    "href": "10-bayes.html#cis-around-a-particular-value",
    "title": "Bayes",
    "section": "CIs around a particular value",
    "text": "CIs around a particular value\nWith your current knowledge, calculate a 95% CI around parent = 72 inches, to tell you what is possible for the sample mean at that hight.\n\\[  \\hat{Y}\\pm t_{critical} * se_{residual}*\\sqrt{\\frac {1}{n}+\\frac{(X-\\bar{X})^2}{(n-1)s_{X}^2}}  \\]"
  },
  {
    "objectID": "10-bayes.html#be-comfortable-integrating-prior-knowledge",
    "href": "10-bayes.html#be-comfortable-integrating-prior-knowledge",
    "title": "Bayes",
    "section": "3. Be comfortable integrating prior knowledge",
    "text": "3. Be comfortable integrating prior knowledge\n\nPriors insert knowledge you have outside of your data into your model\nThis can seem “subjective” as opposed to the more “objective” way of letting the data speak.\n\n\n\nWe will mostly not “tip the scales” towards an outcome we want.\nMost of the time the prior knowledge constrains plausible or implausible range of values e.g. we know an effect size of a million is very unlikely.\nOften priors don’t matter…"
  },
  {
    "objectID": "10-bayes.html#okay-so-what-does-this-mean",
    "href": "10-bayes.html#okay-so-what-does-this-mean",
    "title": "Bayes",
    "section": "Okay so what does this mean?",
    "text": "Okay so what does this mean?\nIt means, BEFORE WE SEE THE DATA we are comfortable with different regression lines."
  },
  {
    "objectID": "10-bayes.html#okay-so-why-is-this-important",
    "href": "10-bayes.html#okay-so-why-is-this-important",
    "title": "Bayes",
    "section": "Okay so why is this important?",
    "text": "Okay so why is this important?\n\nA model that makes impossible predictions prior to seeing the data isn’t too useful. Why waste the effort? We often know what values are likely, given what we know about effect sizes\nThis is exactly what we do with standard “frequentist” methods. They have implicit priors such that all values, from negative infinity to positive infinity are equally likely.\nIf we use priors from a uniform distribution we will get the EXACT same results as a frequentist method."
  },
  {
    "objectID": "10-bayes.html#tying-it-together",
    "href": "10-bayes.html#tying-it-together",
    "title": "Bayes",
    "section": "Tying it together",
    "text": "Tying it together\n\nBe comfortable with a different estimation algorithm\nThink of results in terms of distributions\nBe comfortable integrating prior knowledge\n\n\\[p(\\theta | data) \\propto \\frac{p(data | \\theta) \\times p(\\theta )}{p(data)}\\] P(θ|data) is the posterior probability.\nP(θ) is the prior probability.\np(data| \\(\\theta\\) ) is the likelihood.\np(data) can be ignored, it is just a normalized coefficient"
  },
  {
    "objectID": "10-bayes.html#combining-the-three-components",
    "href": "10-bayes.html#combining-the-three-components",
    "title": "Bayes",
    "section": "Combining the three components",
    "text": "Combining the three components\nPriors influencing Posterior"
  },
  {
    "objectID": "10-bayes.html#sample-size-influence",
    "href": "10-bayes.html#sample-size-influence",
    "title": "Bayes",
    "section": "sample size influence",
    "text": "sample size influence"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior",
    "href": "10-bayes.html#going-from-prior-to-posterior",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\nWhat is our regression estimate again?\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nselect(b_parent) %&gt;% \n  mode_hdi(.width = c(.95))\n\n\n# A tibble: 1 × 6\n  b_parent .lower .upper .width .point .interval\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    0.640  0.564  0.722   0.95 mode   hdi"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior-1",
    "href": "10-bayes.html#going-from-prior-to-posterior-1",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\nWith a prior for b0 of N(0, .5)\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \n  ggplot(aes(x = b_parent)) +\n  stat_slab() +\n  stat_function(data = data.frame(x = c(-2, 2)), aes(x), fun = dnorm, n = 100, args = list(0, .5))"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior-2",
    "href": "10-bayes.html#going-from-prior-to-posterior-2",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\n\nplusible lines prior to data –&gt; plausible lines after data"
  },
  {
    "objectID": "10-bayes.html#what-is-confusing",
    "href": "10-bayes.html#what-is-confusing",
    "title": "Bayes",
    "section": "What is confusing:",
    "text": "What is confusing:\n\n\nIs it a philosophical different frame work? We can talk about how it is p(H0|d) vs p(d|H0) but it really doesn’t matter. Become a Bayesian just means using the algorithm, and again, most of us don’t have strong algo preferences\nTechnically we don’t have P-values, but Bayesian has analogues. Technically there isn’t NHST (because no null distribution to create sampling distribution) but you can easily do it.\n\nWhy don’t we do this already? Isn’t frequentist better? Historical accident due to computation limitations\nBayes Factors. Mostly garbage (imho) as they can be easily manipulated. But they have their place. BFs =/= Bayesian."
  },
  {
    "objectID": "11-ml.html#today",
    "href": "11-ml.html#today",
    "title": "bootstrapping",
    "section": "Today",
    "text": "Today\n\nStart looking at topics related to machine learning\nWe are going to start off with Resampling methods"
  },
  {
    "objectID": "11-ml.html#first.-sampling",
    "href": "11-ml.html#first.-sampling",
    "title": "bootstrapping",
    "section": "First. Sampling",
    "text": "First. Sampling\nIt is impossible to survey every person in the population we are interested in, so we often take a “random sample” from the population and calculate a sample statistic (e.g., mean, median).\nA lot of our statistics follow well-defined distributions (e.g., normal distribution), and we use the properties of these distributions to estimate the population parameter."
  },
  {
    "objectID": "11-ml.html#problems-with-sampling",
    "href": "11-ml.html#problems-with-sampling",
    "title": "bootstrapping",
    "section": "Problems with Sampling",
    "text": "Problems with Sampling\nSingle Estimate of the Population Parameter\n\nEstimate the population mean\nSampling distribution of mean derived from sample statistics\nBuilt a 95% confidence interval around the sample mean\nWe’ve been relying on a single estimate of the population parameter (except for with Bayesian methods)."
  },
  {
    "objectID": "11-ml.html#introduction-to-resampling",
    "href": "11-ml.html#introduction-to-resampling",
    "title": "bootstrapping",
    "section": "Introduction to Resampling",
    "text": "Introduction to Resampling\nA statistical technique that involves re-estimation of the population parameter by repeatedly drawing samples from the original sample"
  },
  {
    "objectID": "11-ml.html#reasons-for-resampling",
    "href": "11-ml.html#reasons-for-resampling",
    "title": "bootstrapping",
    "section": "Reasons for Resampling",
    "text": "Reasons for Resampling\n\nReduce bias of the estimate by using multiple samples instead of one\nBetter sense of precision of the estimated population parameter\nWe do not need to make assumptions about the population distribution (e.g., when we perform two samples t-test, for example, we make the assumption that the populations from which the samples are drawn are normally distributed)\nSample does not have to be large"
  },
  {
    "objectID": "11-ml.html#types-of-resampling",
    "href": "11-ml.html#types-of-resampling",
    "title": "bootstrapping",
    "section": "Types of Resampling",
    "text": "Types of Resampling\n\nBootstrapping\nJackknife method (just a glimpse)\nPermutation testing (just a glimpse)"
  },
  {
    "objectID": "11-ml.html#bootstrapping",
    "href": "11-ml.html#bootstrapping",
    "title": "bootstrapping",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrapping is a method where we rely entirely on the sample that we have at hand.\n\nWe randomly sample within the sample (with replacement)\nCompute the estimator of interest in order to…\nBuild an empirical distribution of that test statistic."
  },
  {
    "objectID": "11-ml.html#illustration-of-bootstrapping",
    "href": "11-ml.html#illustration-of-bootstrapping",
    "title": "bootstrapping",
    "section": "Illustration of Bootstrapping",
    "text": "Illustration of Bootstrapping\nImagine we are trying to estimate the height of a cohort of friends. But you only have 6. Specifically, these 6 friends."
  },
  {
    "objectID": "11-ml.html#repeat",
    "href": "11-ml.html#repeat",
    "title": "bootstrapping",
    "section": "Repeat",
    "text": "Repeat\n\n\nCode\n(sample1 = sample(friends, size = 6, replace = T)); paste('Mean height of this sample:', mean(heights[sample1]))\n\n\n[1] \"Monica\"   \"Monica\"   \"Joey\"     \"Ross\"     \"Ross\"     \"Chandler\"\n\n\n[1] \"Mean height of this sample: 171.5\"\n\n\n\n\nCode\n(sample1 = sample(friends, size = 6, replace = T)); paste('Mean height of this sample:', mean(heights[sample1]))\n\n\n[1] \"Ross\"     \"Rachel\"   \"Rachel\"   \"Joey\"     \"Chandler\" \"Monica\"  \n\n\n[1] \"Mean height of this sample: 169.333333333333\"\n\n\n\n\nCode\n(sample1 = sample(friends, size = 6, replace = T)); paste('Mean height of this sample:', mean(heights[sample1]))\n\n\n[1] \"Monica\"   \"Chandler\" \"Chandler\" \"Ross\"     \"Ross\"     \"Rachel\"  \n\n\n[1] \"Mean height of this sample: 172\""
  },
  {
    "objectID": "11-ml.html#bootstrap-10000-times",
    "href": "11-ml.html#bootstrap-10000-times",
    "title": "bootstrapping",
    "section": "Bootstrap 10,000 Times",
    "text": "Bootstrap 10,000 Times\n\n\nCode\n# When resampling, it is generally a good practice to set random seed\n# for full reproducibility of the resampling process\nset.seed(5067)\n\nboot &lt;- 10000 # Set number of bootstrap samples\n\nfriends = c('Monica', 'Rachel', 'Ross', 'Joey', 'Phoebe', 'Chandler')\nheights &lt;- c(165, 165, 178, 170, 172, 173)\n\nsample_means &lt;- NULL # Initialize list to store sample means"
  },
  {
    "objectID": "11-ml.html#bootstrap-10000-times-1",
    "href": "11-ml.html#bootstrap-10000-times-1",
    "title": "bootstrapping",
    "section": "Bootstrap 10,000 Times",
    "text": "Bootstrap 10,000 Times\n\n\nCode\n# When resampling, it is generally a good practice to set random seed\n# for full reproducibility of the resampling process\nset.seed(5067)\n\nboot &lt;- 10000 # Set number of bootstrap samples\n\nfriends = c('Monica', 'Rachel', 'Ross', 'Joey', 'Phoebe', 'Chandler')\nheights &lt;- c(165, 165, 178, 170, 172, 173)\n\nsample_means &lt;- NULL # Initialize list to store sample means\n\n# Append the mean of bootstrap sample heights to *sample_means*\nfor(i in 1:boot){ #&lt;&lt;\n  this_sample &lt;- sample(heights, size = length(heights), replace = T) #&lt;&lt;\n  sample_means &lt;- c(sample_means, mean(this_sample)) #&lt;&lt;\n} #&lt;&lt;"
  },
  {
    "objectID": "11-ml.html#comparison",
    "href": "11-ml.html#comparison",
    "title": "bootstrapping",
    "section": "Comparison",
    "text": "Comparison\n\n\nCode\nlibrary(ggpubr)\nmu = mean(heights)\nsem = sd(heights)/sqrt(length(heights))\ncv_t = qt(p = .975, df = length(heights)-1)\n\nbootstrapped = data.frame(means = sample_means) %&gt;%\n  ggplot(aes(x = means)) + \n  geom_histogram(color = \"white\") +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(sample_means), color = \"mean\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(sample_means), color = \"median\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(sample_means, probs = .025), color = \"Lower 2.5%\"), \n             linewidth = 2) +\n    geom_vline(aes(xintercept = quantile(sample_means, probs = .975), color = \"Upper 2.5%\"), \n               linewidth = 2) +\n  scale_x_continuous(limits = c(mu-3*sem, mu+3*sem))+\n  ggtitle(\"Bootstrapped distribution\") +\n  cowplot::theme_cowplot()\n\nfrom_prob = data.frame(means = seq(from = min(sample_means), to = max(sample_means))) %&gt;%\n  ggplot(aes(x = means)) +\n  stat_function(fun = function(x) dnorm(x, m = mu, sd = sem)) + \n  geom_vline(aes(xintercept = mean(heights), color = \"mean\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(heights), color = \"median\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = mu-cv_t*sem, color = \"Lower 2.5%\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = mu+cv_t*sem, color = \"Upper 2.5%\"), \n             linewidth = 2) + \n  scale_x_continuous(limits = c(mu-3*sem, mu+3*sem))+  \n  ggtitle(\"Distribution from probability theory\") +\n  cowplot::theme_cowplot()\n\nggarrange(bootstrapped, from_prob, ncol = 1)"
  },
  {
    "objectID": "11-ml.html#example-2",
    "href": "11-ml.html#example-2",
    "title": "bootstrapping",
    "section": "Example 2",
    "text": "Example 2\nCentral tendency and variability of 216 reaction times\n\n\nCode\n# Set random seed before generating data\nset.seed(5067)\n\n# The observations generally follow the F Distribution + random noise\nresponse = rf(n = 216, 3, 50) \nresponse = response * 500 + rnorm(n = 216, mean = 200, sd = 100)"
  },
  {
    "objectID": "11-ml.html#visualize-data",
    "href": "11-ml.html#visualize-data",
    "title": "bootstrapping",
    "section": "Visualize Data",
    "text": "Visualize Data\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(1048596)\nresponse = rf(n = 216, 3, 50) \nresponse = response * 500 + rnorm(n = 216, mean = 200, sd = 100)\n\nvalues = quantile(response, \n                  probs = c(.025, .5, .975))\nmean_res = mean(response)\n\ndata.frame(x = response) %&gt;%\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = ..density..), \n                 binwidth = 150, \n                 fill = \"lightgrey\",\n                 color = \"black\")+\n  geom_density()+\n  geom_vline(aes(xintercept = values[1], \n                 color = \"Lower 2.5%\"), linewidth = 2)+\n  geom_vline(aes(xintercept = values[2], color = \"Median\"), \n             linewidth = 2)+\n  geom_vline(aes(xintercept = values[3], color = \"Upper 2.5%\"),\n             linewidth = 2)+\n  geom_vline(aes(xintercept = mean_res, color = \"Mean\"), \n             linewidth = 2)+\n  labs(x = \"Reponse time (ms)\", title = \"Response Time Distribution\") + cowplot::theme_cowplot(font_size = 20)"
  },
  {
    "objectID": "11-ml.html#distribution-of-means",
    "href": "11-ml.html#distribution-of-means",
    "title": "bootstrapping",
    "section": "Distribution of Means",
    "text": "Distribution of Means\n\n\nCode\ndata.frame(means = response_means) %&gt;%\n  ggplot(aes(x = means)) + \n  geom_histogram(color = \"white\") +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(response_means), color = \"mean\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(response_means), color = \"median\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(response_means, probs = .025), \n                 color = \"Lower 2.5%\"), linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(response_means, probs = .975), \n                 color = \"Upper 2.5%\"), linewidth = 2) +\n  cowplot::theme_cowplot()"
  },
  {
    "objectID": "11-ml.html#other-estimators",
    "href": "11-ml.html#other-estimators",
    "title": "bootstrapping",
    "section": "Other Estimators?",
    "text": "Other Estimators?\nYou can bootstrap estimates and 95% confidence intervals for any statistics you’ll need to estimate.\nThings you should learn how to do in R:\n\nlearn to read a for loop.\nlearn to write your own function."
  },
  {
    "objectID": "11-ml.html#another-example",
    "href": "11-ml.html#another-example",
    "title": "bootstrapping",
    "section": "Another Example",
    "text": "Another Example\nIn this district, Verizon provides line service to both Verizon and non-Verizon customers. Here, we are going to look at a dataset containing service waiting times for Verizon customers (ILEC) and non-Verizon customers (CLEC). We are interested in whether waiting time of non-Verizon customers is longer than that of Verizon customers.\n\n\nCode\nVerizon = read.csv(\"https://raw.githubusercontent.com/shellyc26/psy5067/master/data/Verizon.csv\")\n\n\n\nhead(Verizon, 3)\n\n  Time Group\n1 17.5  ILEC\n2  2.4  ILEC\n3  0.0  ILEC"
  },
  {
    "objectID": "11-ml.html#inspect-verizon-data-1",
    "href": "11-ml.html#inspect-verizon-data-1",
    "title": "bootstrapping",
    "section": "Inspect Verizon Data 1",
    "text": "Inspect Verizon Data 1\n\n\nCode\nVerizon %&gt;%\n  ggplot(aes(x = Time, fill = Group)) + \n  geom_histogram(bins = 30) + \n  guides(fill = \"none\") +\n  facet_wrap(~Group, scales = \"free_y\")\n\n\n\n\nLeft is the distribution of waiting times of Non-Verizon (CLEC) customers and right is the distribution of waiting times of Verizon (ILEC) customers"
  },
  {
    "objectID": "11-ml.html#inspect-verizon-data-2",
    "href": "11-ml.html#inspect-verizon-data-2",
    "title": "bootstrapping",
    "section": "Inspect Verizon Data 2",
    "text": "Inspect Verizon Data 2\n\n\nCode\nVerizon %&gt;%\n  ggplot(aes(x = Time, fill = Group)) + \n  geom_histogram(bins = 50, position = \"dodge\") + \n  guides(fill = \"none\") +\n  theme_bw()\n\n\n\n\n\nCode\ntable(Verizon$Group)\n\n\n\nCLEC ILEC \n  23 1664"
  },
  {
    "objectID": "11-ml.html#analysis-plan-and-justification",
    "href": "11-ml.html#analysis-plan-and-justification",
    "title": "bootstrapping",
    "section": "Analysis Plan and Justification",
    "text": "Analysis Plan and Justification\nIt seems that the data do not meet the typical assumptions of an independent samples \\(t\\)-test.\nIn this case, to estimate mean differences we can use bootstrapping.\nHere, we’ll resample with replacement separately from the two samples and calculate their difference in means."
  },
  {
    "objectID": "11-ml.html#one-solution",
    "href": "11-ml.html#one-solution",
    "title": "bootstrapping",
    "section": "One Solution",
    "text": "One Solution\n\nset.seed(5067)\nboot &lt;- 10000\ndifference &lt;- NULL\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\") #&lt;&lt;\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\") #&lt;&lt;"
  },
  {
    "objectID": "11-ml.html#one-solution-1",
    "href": "11-ml.html#one-solution-1",
    "title": "bootstrapping",
    "section": "One Solution",
    "text": "One Solution\n\nset.seed(5067)\nboot &lt;- 10000\ndifference &lt;- NULL\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\n\nfor(i in 1:boot){\n  # Sample (with replacement) Verizon group (ILEC) customers\n  sample_CLEC = sample(subsample_CLEC$Time, #&lt;&lt;\n                       size = nrow(subsample_CLEC), #&lt;&lt;\n                       replace = T) #&lt;&lt;\n  # Sample (with replacement) Non-Verizon group (CLEC) customers\n  sample_ILEC = sample(subsample_ILEC$Time, #&lt;&lt;\n                       size = nrow(subsample_ILEC), #&lt;&lt;\n                       replace = T) #&lt;&lt;\n}"
  },
  {
    "objectID": "11-ml.html#one-solution-2",
    "href": "11-ml.html#one-solution-2",
    "title": "bootstrapping",
    "section": "One Solution",
    "text": "One Solution\n\nset.seed(5067)\nboot &lt;- 10000\ndifference &lt;- NULL\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\n\nfor(i in 1:boot){\n  # Sample (with replacement) Verizon group (ILEC) customers\n  sample_CLEC = sample(subsample_CLEC$Time, \n                       size = nrow(subsample_CLEC), \n                       replace = T)\n  # Sample (with replacement) Non-Verizon group (CLEC) customers\n  sample_ILEC = sample(subsample_ILEC$Time, \n                       size = nrow(subsample_ILEC), \n                       replace = T)\n  \n  # Calculate the difference in means between the two groups\n  # Append the difference value to a list\n  difference &lt;- c(difference, mean(sample_CLEC) - mean(sample_ILEC)) #&lt;&lt;\n}"
  },
  {
    "objectID": "11-ml.html#bootstrap-distribution-of-differences",
    "href": "11-ml.html#bootstrap-distribution-of-differences",
    "title": "bootstrapping",
    "section": "Bootstrap Distribution of Differences",
    "text": "Bootstrap Distribution of Differences\n\n\nCode\ndata.frame(differences = difference) %&gt;%\n  ggplot(aes(x = differences)) + \n  geom_histogram(aes(y = ..density..),color = \"white\", fill = \"grey\") +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(differences), color = \"mean\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(differences), color = \"median\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(differences, probs = .025), color = \"Lower 2.5%\"), \n             linewidth = 2) +\n    geom_vline(aes(xintercept = quantile(differences, probs = .975), color = \"Upper 2.5%\"), \n               linewidth = 2) +\n  cowplot::theme_cowplot()\n\n\n\nThe difference in means is 7.64 \\([1.66,16.98]\\). What would this mean?"
  },
  {
    "objectID": "11-ml.html#jackknife-resampling",
    "href": "11-ml.html#jackknife-resampling",
    "title": "bootstrapping",
    "section": "Jackknife Resampling",
    "text": "Jackknife Resampling\nJackknife Resampling is a method where researchers generate n sub-samples, each leaving out one observation. The method is very similar to bootstrapping except the way that we create the sub-samples."
  },
  {
    "objectID": "11-ml.html#jack-of-all-trades-master-of-none---jackknife",
    "href": "11-ml.html#jack-of-all-trades-master-of-none---jackknife",
    "title": "bootstrapping",
    "section": "Jack of all trades, master of none - Jackknife",
    "text": "Jack of all trades, master of none - Jackknife\n\nCan assess the accuracy of a statistical estimator without making assumptions about the underlying distribution\nComputationally efficient bc only n sub-samples are generated (compared to 10,000 for example)\nNot used as much these days\nSensitive to sample size: If sample size is small, it can result in inaccurate estimates of the bias (e.g., sample size of 6 means six jackknife samples)."
  },
  {
    "objectID": "11-ml.html#permutation-testing",
    "href": "11-ml.html#permutation-testing",
    "title": "bootstrapping",
    "section": "Permutation Testing",
    "text": "Permutation Testing\nA resampling method that involves randomly shuffling the labels (e.g., conditions) across the data and recomputing the test statistic of interest, thereby deriving a null distribution of the test statistic."
  },
  {
    "objectID": "11-ml.html#permutation-example-restaurants",
    "href": "11-ml.html#permutation-example-restaurants",
    "title": "bootstrapping",
    "section": "Permutation Example: Restaurants",
    "text": "Permutation Example: Restaurants\nWe are interested in the difference in the rating of group A and group B restaurants.\n\n\nCode\nnames &lt;- c(\"Pappy's Smokehouse\", 'Mai Lee', \"Adriana's on the Hill\", 'Salt & Smoke', 'Chilli Spot', 'BLK MKT')\nlabels &lt;- c('A', 'A', 'A', 'B', 'B', 'B')\nratings &lt;- c(7, 8, 8, 4, 6, 7)\n\nperm_df &lt;- data.frame(name = names, \n                      group = labels, \n                      rating = ratings)\n\nhead(perm_df, 6)\n\n\n                   name group rating\n1    Pappy's Smokehouse     A      7\n2               Mai Lee     A      8\n3 Adriana's on the Hill     A      8\n4          Salt & Smoke     B      4\n5           Chilli Spot     B      6\n6               BLK MKT     B      7"
  },
  {
    "objectID": "11-ml.html#permutation-example-restaurants-1",
    "href": "11-ml.html#permutation-example-restaurants-1",
    "title": "bootstrapping",
    "section": "Permutation Example: Restaurants",
    "text": "Permutation Example: Restaurants\nThe observed difference is…\n\n\n                   name group rating\n1    Pappy's Smokehouse     A      7\n2               Mai Lee     A      8\n3 Adriana's on the Hill     A      8\n\n\n\n\n[1] 7.666667\n\n\n\n\n          name group rating\n1 Salt & Smoke     B      4\n2  Chilli Spot     B      6\n3      BLK MKT     B      7\n\n\n\n\n[1] 5.666667\n\n\n\nperm_df %&gt;% filter(group == 'A') %&gt;% pull(rating) %&gt;% mean() - perm_df %&gt;% filter(group == 'B') %&gt;% pull(rating) %&gt;% mean()\n\n[1] 2"
  },
  {
    "objectID": "11-ml.html#lets-try",
    "href": "11-ml.html#lets-try",
    "title": "bootstrapping",
    "section": "Let’s Try",
    "text": "Let’s Try\nInitially, we had 23 CLEC and 1,664 ILEC customers. First, we are going to calculate the mean difference in waiting time between the two groups using the observed data.\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\n\n(mean(subsample_CLEC$Time) - mean(subsample_ILEC$Time))\n\n[1] 8.09752"
  },
  {
    "objectID": "11-ml.html#second-step",
    "href": "11-ml.html#second-step",
    "title": "bootstrapping",
    "section": "Second Step",
    "text": "Second Step\nWe are going to randomly shuffle the labels of groups (previous CLEC and ILEC labels don’t matter!). Label 23 random customers as CLEC and 1,664 random customers as ILEC 1,000 times, and store the mean difference in waiting time between the two groups inside a list."
  },
  {
    "objectID": "11-ml.html#repeat-1000-times-to-derive-the-p-value",
    "href": "11-ml.html#repeat-1000-times-to-derive-the-p-value",
    "title": "bootstrapping",
    "section": "Repeat 1,000 times to derive the p-value",
    "text": "Repeat 1,000 times to derive the p-value\nOne iteration would look something like this:\n\nrandom_indices &lt;- sample.int(nrow(Verizon), 23)\nrandom_CLEC &lt;- Verizon[random_indices,]\nrandom_ILEC &lt;- Verizon[-random_indices,]\n\n(mean(random_CLEC$Time) - mean(random_ILEC$Time))\n\n[1] -0.3775922"
  },
  {
    "objectID": "11-ml.html#analysis-plan",
    "href": "11-ml.html#analysis-plan",
    "title": "bootstrapping",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nPut this in a for loop\n\n–\n\nConstruct a list of mean differences\n\n–\n\nDetermine the number of (random) mean differences greater than the observed difference in means (8.10)."
  },
  {
    "objectID": "11-ml.html#one-way-to-do-this",
    "href": "11-ml.html#one-way-to-do-this",
    "title": "bootstrapping",
    "section": "One Way to do this",
    "text": "One Way to do this\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\nobserved_diff &lt;- mean(subsample_CLEC$Time) - mean(subsample_ILEC$Time)\n\nset.seed(1048596) # Set random seed\nperm = 1000 # Set number of permutations\n\ndifferences &lt;- NULL #&lt;&lt;"
  },
  {
    "objectID": "11-ml.html#one-way-to-do-this-1",
    "href": "11-ml.html#one-way-to-do-this-1",
    "title": "bootstrapping",
    "section": "One Way to do this",
    "text": "One Way to do this\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\nobserved_diff &lt;- mean(subsample_CLEC$Time) - mean(subsample_ILEC$Time)\n\nset.seed(1048596) # Set random seed\nperm = 1000 # Set number of permutations\n\ndifferences &lt;- NULL\n\nfor (i in 1:perm){\n  random_indices &lt;- sample.int(nrow(Verizon), 23)\n  random_CLEC &lt;- Verizon[random_indices,]\n  random_ILEC &lt;- Verizon[-random_indices,]\n  \n  differences &lt;- c(differences, #&lt;&lt;\n                   mean(random_CLEC$Time) - mean(random_ILEC$Time)) #&lt;&lt;\n}"
  },
  {
    "objectID": "11-ml.html#visualization",
    "href": "11-ml.html#visualization",
    "title": "bootstrapping",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\nperm_df = data.frame(diff = differences)\n\nggplot(perm_df, aes(x = diff)) +\n  geom_histogram(color = \"white\") + \n  geom_vline(aes(xintercept = mean(diff), color = \"mean\"), \n             linewidth = 1) +\n  geom_vline(aes(xintercept = median(diff), color = \"median\"), \n             linewidth = 1) +\n  geom_vline(aes(xintercept = observed_diff, color = \"observed\"), \n             linewidth = 1) +\n  ggtitle(\"Empirical Distribution of Differences under the Null\") +\n  cowplot::theme_cowplot()"
  },
  {
    "objectID": "11-ml.html#permutation-testing-summary",
    "href": "11-ml.html#permutation-testing-summary",
    "title": "bootstrapping",
    "section": "Permutation Testing Summary",
    "text": "Permutation Testing Summary\n\nPermutation testing is useful for hypothesis testing because we can easily derive a \\(p\\)-value\nCan be used when data violates common assumptions about the data (i.e., homogeneity of variance and normality)\nAssumption that the observations need to be exchangeable. Some observations may not be exchangeable (e.g., time series data - data collected at different time points)\nSample size needs to be large. No point randomly shuffling 1,000 times when the possible permutation is less than 1000."
  },
  {
    "objectID": "9-hw.html",
    "href": "9-hw.html",
    "title": "HW-9",
    "section": "",
    "text": "Please complete this assignment by creating a new RMarkdown (or Quarto) file. Be sure to include your name in the preamble. Please submit both your .Rmd (or .qmd) and your .html files to Canvas.\nData: You will use the same Human Connectome Project dataset you worked with in the R class and this semester’s R refresher. It is available here"
  },
  {
    "objectID": "9-hw.html#instructions",
    "href": "9-hw.html#instructions",
    "title": "HW-9",
    "section": "",
    "text": "Please complete this assignment by creating a new RMarkdown (or Quarto) file. Be sure to include your name in the preamble. Please submit both your .Rmd (or .qmd) and your .html files to Canvas.\nData: You will use the same Human Connectome Project dataset you worked with in the R class and this semester’s R refresher. It is available here"
  },
  {
    "objectID": "9-hw.html#questions",
    "href": "9-hw.html#questions",
    "title": "HW-9",
    "section": "Questions",
    "text": "Questions\n\nIn a few sentences, describe the difference between a theoretical probability distribution and an empirical distribution. In your answer, make sure to discuss how bootstrapping fits into this.\nYou are interested in the correlation between the Penn Matrix Reasoning Task (PMAT) and the List Sorting task, and you are wondering if the correlation is significantly different from 0. Use bootstrapping to create a more precise estimate of the correlation, as well as answer if it is different from 0. Create a figure illustrating your findings.\nRun a permutation test to see if Neuroticism is different for men and women. Create a figure illustrating your findings.\nReflect on the Yarkoni & Westfall paper, as well as our discussion in class. In 4-6 sentences, what were YOUR takeaways? Did you change your mind on something? Did you start thinking about scientific research in a different way? Were you convinced or unconvinced in either direction? We want to hear what you got out of the paper and subsequent discussion!"
  },
  {
    "objectID": "12-ml.html#today-last-time",
    "href": "12-ml.html#today-last-time",
    "title": "ML-2",
    "section": "Today & Last time…",
    "text": "Today & Last time…\n\nResampling methods via bootstrapping & permutation testing\nLessons from machine learning"
  },
  {
    "objectID": "12-ml.html#explanation-vs.-prediction",
    "href": "12-ml.html#explanation-vs.-prediction",
    "title": "ML-2",
    "section": "Explanation vs. Prediction",
    "text": "Explanation vs. Prediction\nExplanation: describe causal underpinnings of behaviors/outcomes\nPrediction: Accurately forecast behaviors/outcomes\nSimilarities.\n\nBoth are goals of science.\nGood predictions can help us develop theories of explanation and vice versa"
  },
  {
    "objectID": "12-ml.html#explanation-vs.-prediction-1",
    "href": "12-ml.html#explanation-vs.-prediction-1",
    "title": "ML-2",
    "section": "Explanation vs. Prediction",
    "text": "Explanation vs. Prediction\nStatistical Tensions\n\nStatistical models that accurately describe causal truths often have poor prediction and are complex\nPredictive models are often very different from the “true”, underlying data generating processes"
  },
  {
    "objectID": "12-ml.html#explanation-vs.-prediction-2",
    "href": "12-ml.html#explanation-vs.-prediction-2",
    "title": "ML-2",
    "section": "Explanation vs. Prediction",
    "text": "Explanation vs. Prediction\n\nWhat do we do in Psychology and Neuroscience?\nWhat about regression?\n\nPoint of Yarkoni & Westfall (2017) - How far have we really come in the past 10 years? 20 years? 30 years? …not as far as we’d like\n\nWe should spend more time and resources developing predictive models than we currently do"
  },
  {
    "objectID": "12-ml.html#maybe-helpful-definitions",
    "href": "12-ml.html#maybe-helpful-definitions",
    "title": "ML-2",
    "section": "Maybe Helpful Definitions",
    "text": "Maybe Helpful Definitions\nMachine Learning (ML) is the process of feeding previous observations into a computer and using the computer to generate predictions for new observations. AKA:\n\nmachine inference\npattern recognition (Google Photos!)\nstatistical estimation\nprediction modeling\nstatistical learning (ish)"
  },
  {
    "objectID": "12-ml.html#maybe-helpful-definitions-1",
    "href": "12-ml.html#maybe-helpful-definitions-1",
    "title": "ML-2",
    "section": "Maybe Helpful Definitions",
    "text": "Maybe Helpful Definitions\nTraining occurs by extracting patterns from the observed data; think of this as learning\nTesting occurs by verifying predictions on previously unobserved data; think of this as evaluating\nArtificial Intelligence is not the same as machine learning. AI is the simulation of human intelligence by computers; AI systems are generally trained with machine learning approaches."
  },
  {
    "objectID": "12-ml.html#some-more-definitions",
    "href": "12-ml.html#some-more-definitions",
    "title": "ML-2",
    "section": "Some more definitions",
    "text": "Some more definitions\nSupervised Learning: using known patterns between input and output observations to train a mapping between the two\n\nRegression! learning the mapping between a continuous input feature variable and a continuous output target variable\nClassification: learning the mapping between a continuous input feature variable and a categorical output target variable (i.e., a label)"
  },
  {
    "objectID": "12-ml.html#some-more-definitions-1",
    "href": "12-ml.html#some-more-definitions-1",
    "title": "ML-2",
    "section": "Some more definitions",
    "text": "Some more definitions\nUnsupervised Learning: determining patterns in observations without guiding referents\n\nDimensionality Reduction: decreasing the overall number of features considered in a learning procedure (i.e., PCA, ICA etc.)\nClustering: grouping features together that are similar as determined by some metric"
  },
  {
    "objectID": "12-ml.html#wash-u",
    "href": "12-ml.html#wash-u",
    "title": "ML-2",
    "section": "@Wash U",
    "text": "@Wash U\n\nReinforcement Learning: determining a mapping between input and output observations using only a measure of training quality\nML classes in CS department\nWouter Kool for reinforcement learning\nACCSN with Dennis Barbour and myself for high level discussions on these topics\nPoli Sci department for dealing with categorical outcomes"
  },
  {
    "objectID": "12-ml.html#machine-learning-algorithms",
    "href": "12-ml.html#machine-learning-algorithms",
    "title": "ML-2",
    "section": "Machine Learning Algorithms",
    "text": "Machine Learning Algorithms\n\n\n\nordinary least squares linear regression\nlogistic regression\nk-means clustering\nnearest neighbor\nnaive Bayes\nridge regression\nLASSO regression\nsupport vector machine\n\n\n\nrandom forest\nGaussian process estimator\nmultilayer perceptron (deep net)\nconvolutional network\nrecurrent network\ngeneralized adversarial network"
  },
  {
    "objectID": "12-ml.html#some-terminology",
    "href": "12-ml.html#some-terminology",
    "title": "ML-2",
    "section": "Some terminology",
    "text": "Some terminology\nOverfitting is when we mistakenly fit sample-specific noise as if it were actually a signal.\n\nIf our model has a \\(R^2 = .9\\), we do an excellent job of explaining variance in our sample.\nOLS models tend to be overfit because they minimize error for a specific sample"
  },
  {
    "objectID": "12-ml.html#bias-and-variance",
    "href": "12-ml.html#bias-and-variance",
    "title": "ML-2",
    "section": "Bias and Variance",
    "text": "Bias and Variance\nBias refers to systematically over- or under-estimating parameters.\nVariance refers to how much estimates tend to jump around\nBias-Variance Tradeoff we can reduce variance around our estimates but at the expense of increasing bias of estimates and vice versa\nUnderfitting means we can’t capture a relationship at all – not as big of a problem for us"
  },
  {
    "objectID": "12-ml.html#cross-validation",
    "href": "12-ml.html#cross-validation",
    "title": "ML-2",
    "section": "Cross-validation",
    "text": "Cross-validation\nCross-validation is a family of techniques that involve testing and training a model on different samples of data."
  },
  {
    "objectID": "12-ml.html#cross-validation-hold-out-samples",
    "href": "12-ml.html#cross-validation-hold-out-samples",
    "title": "ML-2",
    "section": "Cross-validation: Hold-out Samples",
    "text": "Cross-validation: Hold-out Samples\n\n\n\nSplit into training and testing sets\nFit your model on the training set\nPredict outputs for your testing set\n\n\nPros\n\nStraightforward; computationally easy\n\nCons\n\nWhich data go into which set?\nWhat if the vast majority of group A fall into the training set and the vast majority of group B fall into the testing set?"
  },
  {
    "objectID": "12-ml.html#k-fold-cross-validation",
    "href": "12-ml.html#k-fold-cross-validation",
    "title": "ML-2",
    "section": "K-fold Cross-validation",
    "text": "K-fold Cross-validation\n\nMake k subsets of your data\nRepeat the hold-out method of test/train, but do it k times\nGet the model fit for all k iterations; take the average model fit"
  },
  {
    "objectID": "12-ml.html#k-fold-cross-validation-1",
    "href": "12-ml.html#k-fold-cross-validation-1",
    "title": "ML-2",
    "section": "K-fold Cross-validation",
    "text": "K-fold Cross-validation\nPros\n\nDoesn’t matter much which data points fall into test or train since each subset can be both a test and a training set\nThe more folds you do (larger k), the more you are able to decrease your variance around your averaged model fit\n\nCons\n\nCan take a decent amount of computational power, depending on the dataset"
  },
  {
    "objectID": "12-ml.html#leave-one-out-cross-validation",
    "href": "12-ml.html#leave-one-out-cross-validation",
    "title": "ML-2",
    "section": "Leave-One-Out Cross-validation",
    "text": "Leave-One-Out Cross-validation\n\nSame as k-fold, but now k is equal to your \\(N\\)\n\nPros\n\nGood estimations\n\nCons\n\nEven more computationally expensive\nEspecially if using “big data”"
  },
  {
    "objectID": "12-ml.html#a-brief-aside",
    "href": "12-ml.html#a-brief-aside",
    "title": "ML-2",
    "section": "A brief aside:",
    "text": "A brief aside:\nNewer package called tidymodels is better for machine learning, but requires many more steps. For now, this is the simpler method.\nR vs. Python 🐍 . R is excellent for statistics and visualizing data. Most of what we do in Psychology. Python is better for machine learning and is more of a full suite language.\nIf you’re going hard with ML, use tidymodels or Python."
  },
  {
    "objectID": "12-ml.html#example-10-fold-cross-validation",
    "href": "12-ml.html#example-10-fold-cross-validation",
    "title": "ML-2",
    "section": "Example: 10-fold cross validation",
    "text": "Example: 10-fold cross validation\n\n\nCode\nlibrary(caret)\n# set control parameters\nctrl &lt;- trainControl(method=\"cv\", number=10)\n# use train() instead of lm()\ncv.model &lt;- train(Stress ~ Anxiety*Support*group, \n               data = stress.data, \n               trControl=ctrl, # what are the control parameters\n               method=\"lm\") # what kind of model\ncv.model\n\n\nLinear Regression \n\n118 samples\n  3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 106, 107, 106, 107, 106, 106, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1.575509  0.3851594  1.279616\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "12-ml.html#cross-validation-summary",
    "href": "12-ml.html#cross-validation-summary",
    "title": "ML-2",
    "section": "Cross-Validation Summary",
    "text": "Cross-Validation Summary\n\nTempers your estimates to protect against overfitting\nYou do not need fancy ML algorithms. You can do this with your classic regression!\nIf your whole sample is not representative, this will not save you"
  },
  {
    "objectID": "12-ml.html#regularization",
    "href": "12-ml.html#regularization",
    "title": "ML-2",
    "section": "Regularization",
    "text": "Regularization\nPenalizing a model as it grows more complex.\n\nUsually involves shrinking coefficient estimates – the model will fit less well in-sample but may be more predictive"
  },
  {
    "objectID": "12-ml.html#lasso-regression",
    "href": "12-ml.html#lasso-regression",
    "title": "ML-2",
    "section": "LASSO Regression",
    "text": "LASSO Regression\n\nThe glmnet package has the tools for LASSO regression.\nOne small complication is that the package uses matrix algebra, so you need to feed it a matrix of predictors – specifically, instead of saying “find the interaction between A and B”, you need to create the variable that represents this term.\nLuckily, the function model.matrix() can do this for you."
  },
  {
    "objectID": "12-ml.html#what-value-of-lambda-to-choose",
    "href": "12-ml.html#what-value-of-lambda-to-choose",
    "title": "ML-2",
    "section": "What value of \\(\\lambda\\) to choose?",
    "text": "What value of \\(\\lambda\\) to choose?\nLooks like coefficients 1, 2, and 3 have high values even with shrinkage.\n\n\nCode\nplot(lasso.mod, xvar = \"dev\", label = T, )"
  },
  {
    "objectID": "12-ml.html#what-value-of-lambda-to-choose-1",
    "href": "12-ml.html#what-value-of-lambda-to-choose-1",
    "title": "ML-2",
    "section": "What value of \\(\\lambda\\) to choose?",
    "text": "What value of \\(\\lambda\\) to choose?\nLooking for \\(\\lambda\\) values where those coefficients are still different from 0.\n\n\nCode\nplot(lasso.mod, xvar = \"lambda\", label = TRUE, cex = 8)"
  },
  {
    "objectID": "12-ml.html#summary-yarkoni-and-westfall-2017",
    "href": "12-ml.html#summary-yarkoni-and-westfall-2017",
    "title": "ML-2",
    "section": "Summary: Yarkoni and Westfall (2017)",
    "text": "Summary: Yarkoni and Westfall (2017)\nBig Data * Reduce the likelihood of overfitting – more data means less error\nCross-validation * Is my model overfit?\nRegularization * Constrain the model to be less overfit"
  }
]